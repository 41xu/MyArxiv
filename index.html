<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-08-30T00:00:00Z">2023-08-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> P2M: A Fast Solver for Querying Distance from Point to Mesh Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the existing point-to-mesh distance query solvers, such as Proximity
Query Package (PQP), Embree and Fast Closest Point Query (FCPW), are based on
bounding volume hierarchy (BVH). The hierarchical organizational structure
enables one to eliminate the vast majority of triangles that do not help find
the closest point. In this paper, we develop a totally different algorithmic
paradigm, named P2M, to speed up point-to-mesh distance queries. Our original
intention is to precompute a KD tree (KDT) of mesh vertices to approximately
encode the geometry of a mesh surface containing vertices, edges and faces.
However, it is very likely that the closest primitive to the query point is an
edge e (resp., a face f), but the KDT reports a mesh vertex \u{psion} instead.
We call \u{psion} an interceptor of e (resp., f). The main contribution of this
paper is to invent a simple yet effective interception inspection rule and an
efficient flooding interception inspection algorithm for quickly finding out
all the interception pairs. Once the KDT and the interception table are
precomputed, the query stage proceeds by first searching the KDT and then
looking up the interception table to retrieve the closest geometric primitive.
Statistics show that our query algorithm runs many times faster than the
state-of-the-art solvers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Saving temporary exhibitions in virtual environments: the Digital
  Renaissance of Ulisse Aldrovandi 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>b<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>à<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our goal was to obtain the digital twin of the temporary exhibition "The
Other Renaissance: Ulisse Aldrovandi and the Wonders of the World", to make it
accessible online to users using various devices (from smartphones to VR
headsets). We started with a preliminary assessment of the exhibition,
focussing on possible acquisition constraints - time, space, and materials -
and related solutions. Then, we proceeded with the creation of the digital twin
by acquiring, processing, modelling, optimising, exporting, metadating, and
uploading the exhibition. We adopted a hybrid use of two distinct acquisition
techniques, i.e. structured light projection scanning and photogrammetry, to
create new digital cultural heritage objects and environments, and we used open
technologies, formats and protocols to make available the final digital
product. We described the process to collect and curate bibliographical
(meta)data of the exhibition and digital twin creation process to foster its
findability, accessibility, interoperability and reusability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Human Motion Diffusion as a <span class="highlight-title">Generative</span> Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has demonstrated the significant potential of denoising diffusion
models for generating human motion, including text-to-motion capabilities.
However, these methods are restricted by the paucity of annotated motion data,
a focus on single-person motions, and a lack of detailed control. In this
paper, we introduce three forms of composition based on diffusion priors:
sequential, parallel, and model composition. Using sequential composition, we
tackle the challenge of long sequence generation. We introduce DoubleTake, an
inference-time method with which we generate long animations consisting of
sequences of prompted intervals and their transitions, using a prior trained
only for short clips. Using parallel composition, we show promising steps
toward two-person generation. Beginning with two fixed priors as well as a few
two-person training examples, we learn a slim communication block, ComMDM, to
coordinate interaction between the two resulting motions. Lastly, using model
composition, we first train individual priors to complete motions that realize
a prescribed motion for a given joint. We then introduce DiffusionBlending, an
interpolation mechanism to effectively blend several such models to enable
flexible and efficient fine-grained joint and trajectory-level control and
editing. We evaluate the composition methods using an off-the-shelf motion
diffusion model, and further compare the results to dedicated models trained
for these specific tasks.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">104</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Boosting Detection in Crowd Analysis via Underutilized Output Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detection-based methods have been viewed unfavorably in crowd analysis due to
their poor performance in dense crowds. However, we argue that the potential of
these methods has been underestimated, as they offer crucial information for
crowd analysis that is often ignored. Specifically, the area size and
confidence score of output proposals and bounding boxes provide insight into
the scale and density of the crowd. To leverage these underutilized features,
we propose Crowd Hat, a plug-and-play module that can be easily integrated with
existing detection models. This module uses a mixed 2D-1D compression technique
to refine the output features and obtain the spatial and numerical distribution
of crowd-specific information. Based on these features, we further propose
region-adaptive NMS thresholds and a decouple-then-align paradigm that address
the major limitations of detection-based methods. Our extensive evaluations on
various crowd analysis tasks, including crowd counting, localization, and
detection, demonstrate the effectiveness of utilizing output features and the
potential of detection-based methods in crowd analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://fredfyyang.github.io/Crowd-Hat/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> SAM-Med2D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Segment Anything Model (SAM) represents a state-of-the-art research
advancement in natural image segmentation, achieving impressive results with
input prompts such as points and bounding boxes. However, our evaluation and
recent research indicate that directly applying the pretrained SAM to medical
image segmentation does not yield satisfactory performance. This limitation
primarily arises from significant domain gap between natural images and medical
images. To bridge this gap, we introduce SAM-Med2D, the most comprehensive
studies on applying SAM to medical 2D images. Specifically, we first collect
and curate approximately 4.6M images and 19.7M masks from public and private
datasets, constructing a large-scale medical image segmentation dataset
encompassing various modalities and objects. Then, we comprehensively fine-tune
SAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that
only adopt bounding box or point prompts as interactive segmentation approach,
we adapt SAM to medical image segmentation through more comprehensive prompts
involving bounding boxes, points, and masks. We additionally fine-tune the
encoder and decoder of the original SAM to obtain a well-performed SAM-Med2D,
leading to the most comprehensive fine-tuning strategies to date. Finally, we
conducted a comprehensive evaluation and analysis to investigate the
performance of SAM-Med2D in medical image segmentation across various
modalities, anatomical structures, and organs. Concurrently, we validated the
generalization capability of SAM-Med2D on 9 datasets from MICCAI 2023
challenge. Overall, our approach demonstrated significantly superior
performance and generalization capability compared to SAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> GREC: Generalized Referring Expression Comprehension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of Classic Referring Expression Comprehension (REC) is to
produce a bounding box corresponding to the object mentioned in a given textual
description. Commonly, existing datasets and techniques in classic REC are
tailored for expressions that pertain to a single target, meaning a sole
expression is linked to one specific object. Expressions that refer to multiple
targets or involve no specific target have not been taken into account. This
constraint hinders the practical applicability of REC. This study introduces a
new benchmark termed as Generalized Referring Expression Comprehension (GREC).
This benchmark extends the classic REC by permitting expressions to describe
any number of target objects. To achieve this goal, we have built the first
large-scale GREC dataset named gRefCOCO. This dataset encompasses a range of
expressions: those referring to multiple targets, expressions with no specific
target, and the single-target expressions. The design of GREC and gRefCOCO
ensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a
GREC method implementation code, and GREC evaluation code are available at
https://github.com/henghuiding/gRefCOCO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GREC Technical Report, Project Page:
  https://henghuiding.github.io/GRES</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MMVP: Motion-Matrix-based Video Prediction <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>l<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>U<span class="highlight-author"></span>l<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A central challenge of video prediction lies where the system has to reason
the objects' future motions from image frames while simultaneously maintaining
the consistency of their appearances across frames. This work introduces an
end-to-end trainable two-stream video prediction framework, Motion-Matrix-based
Video Prediction (MMVP), to tackle this challenge. Unlike previous methods that
usually handle motion prediction and appearance maintenance within the same set
of modules, MMVP decouples motion and appearance information by constructing
appearance-agnostic motion matrices. The motion matrices represent the temporal
similarity of each and every pair of feature patches in the input frames, and
are the sole input of the motion prediction module in MMVP. This design
improves video prediction in both accuracy and efficiency, and reduces the
model size. Results of extensive experiments demonstrate that MMVP outperforms
state-of-the-art systems on public data sets by non-negligible large margins
(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the
size or smaller). Please refer to
https://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for the
official code and the datasets used in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Modality Cycles with Masked Conditional Diffusion for Unsupervised
  Anomaly Segmentation in MRI <span class="chip">MICCAI
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised anomaly segmentation aims to detect patterns that are distinct
from any patterns processed during training, commonly called abnormal or
out-of-distribution patterns, without providing any associated manual
segmentations. Since anomalies during deployment can lead to model failure,
detecting the anomaly can enhance the reliability of models, which is valuable
in high-risk domains like medical imaging. This paper introduces Masked
Modality Cycles with Conditional Diffusion (MMCCD), a method that enables
segmentation of anomalies across diverse patterns in multimodal MRI. The method
is based on two fundamental ideas. First, we propose the use of cyclic modality
translation as a mechanism for enabling abnormality detection.
Image-translation models learn tissue-specific modality mappings, which are
characteristic of tissue physiology. Thus, these learned mappings fail to
translate tissues or image patterns that have never been encountered during
training, and the error enables their segmentation. Furthermore, we combine
image translation with a masked conditional diffusion model, which attempts to
`imagine' what tissue exists under a masked area, further exposing unknown
patterns as the generative model fails to recreate them. We evaluate our method
on a proxy task by training on healthy-looking slices of BraTS2021
multi-modality MRIs and testing on slices with tumors. We show that our method
compares favorably to previous unsupervised approaches based on image
reconstruction and denoising with autoencoders and diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Multiscale Multimodal Medical Imaging workshop in MICCAI
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> CircleFormer: Circular Nuclei Detection in Whole Slide Images with
  Circle Queries and Attention <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Both CNN-based and Transformer-based object detection with bounding box
representation have been extensively studied in computer vision and medical
image analysis, but circular object detection in medical images is still
underexplored. Inspired by the recent anchor free CNN-based circular object
detection method (CircleNet) for ball-shape glomeruli detection in renal
pathology, in this paper, we present CircleFormer, a Transformer-based circular
medical object detection with dynamic anchor circles. Specifically, queries
with circle representation in Transformer decoder iteratively refine the
circular object detection results, and a circle cross attention module is
introduced to compute the similarity between circular queries and image
features. A generalized circle IoU (gCIoU) is proposed to serve as a new
regression loss of circular object detection as well. Moreover, our approach is
easy to generalize to the segmentation task by adding a simple segmentation
branch to CircleFormer. We evaluate our method in circular nuclei detection and
segmentation on the public MoNuSeg dataset, and the experimental results show
that our method achieves promising performance compared with the
state-of-the-art approaches. The effectiveness of each component is validated
via ablation studies as well. Our code is released at:
\url{https://github.com/zhanghx-iim-ahu/CircleFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MedShapeNet -- A Large-Scale Dataset of <span class="highlight-title">3D</span> Medical Shapes for Computer
  Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>x<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>n<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>z<span class="highlight-author"></span>y<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>f<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>d<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>-<span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>j<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>z<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>p<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>x<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>U<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>O<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>x<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>j<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>j<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>g<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present MedShapeNet, a large collection of anatomical shapes (e.g., bones,
organs, vessels) and 3D surgical instrument models. Prior to the deep learning
era, the broad application of statistical shape models (SSMs) in medical image
analysis is evidence that shapes have been commonly used to describe medical
data. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in
medical imaging are predominantly voxel-based. In computer vision, on the
contrary, shapes (including, voxel occupancy grids, meshes, point clouds and
implicit surface models) are preferred data representations in 3D, as seen from
the numerous shape-related publications in premier vision conferences, such as
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as
well as the increasing popularity of ShapeNet (about 51,300 models) and
Princeton ModelNet (127,915 models) in computer vision research. MedShapeNet is
created as an alternative to these commonly used shape benchmarks to facilitate
the translation of data-driven vision algorithms to medical applications, and
it extends the opportunities to adapt SOTA vision algorithms to solve critical
medical problems. Besides, the majority of the medical shapes in MedShapeNet
are modeled directly on the imaging data of real patients, and therefore it
complements well existing shape benchmarks comprising of computer-aided design
(CAD) models. MedShapeNet currently includes more than 100,000 medical shapes,
and provides annotations in the form of paired data. It is therefore also a
freely available repository of 3D models for extended reality (virtual reality
- VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This
white paper describes in detail the motivations behind MedShapeNet, the shape
acquisition procedures, the use cases, as well as the usage of the online shape
search portal: https://medshapenet.ikim.nrw/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a
  Novel Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>w<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>s<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>ø<span class="highlight-author"></span>r<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting visually similar images is a particularly useful attribute to look
to when calculating product recommendations. Embedding similarity, which
utilizes pre-trained computer vision models to extract high-level image
features, has demonstrated remarkable efficacy in identifying images with
similar compositions. However, there is a lack of methods for evaluating the
embeddings generated by these models, as conventional loss and performance
metrics do not adequately capture their performance in image similarity search
tasks.
  In this paper, we evaluate the viability of the image embeddings from
numerous pre-trained computer vision models using a novel approach named
CorrEmbed. Our approach computes the correlation between distances in image
embeddings and distances in human-generated tag vectors. We extensively
evaluate numerous pre-trained Torchvision models using this metric, revealing
an intuitive relationship of linear scaling between ImageNet1k accuracy scores
and tag-correlation scores. Importantly, our method also identifies deviations
from this pattern, providing insights into how different models capture
high-level image features.
  By offering a robust performance evaluation of these pre-trained models,
CorrEmbed serves as a valuable tool for researchers and practitioners seeking
to develop effective, data-driven approaches to similar item recommendations in
fashion retail.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AI-2023 Forty-third SGAI International Conference on
  Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Improving Few-shot Image Generation by Structural Discrimination and
  Textural Modulation <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot image generation, which aims to produce plausible and diverse images
for one category given a few images from this category, has drawn extensive
attention. Existing approaches either globally interpolate different images or
fuse local representations with pre-defined coefficients. However, such an
intuitive combination of images/features only exploits the most relevant
information for generation, leading to poor diversity and coarse-grained
semantic fusion. To remedy this, this paper proposes a novel textural
modulation (TexMod) mechanism to inject external semantic signals into internal
local representations. Parameterized by the feedback from the discriminator,
our TexMod enables more fined-grained semantic injection while maintaining the
synthesis fidelity. Moreover, a global structural discriminator (StructD) is
developed to explicitly guide the model to generate images with reasonable
layout and outline. Furthermore, the frequency awareness of the model is
reinforced by encouraging the model to distinguish frequency signals. Together
with these techniques, we build a novel and effective model for few-shot image
generation. The effectiveness of our model is identified by extensive
experiments on three popular datasets and various settings. Besides achieving
state-of-the-art synthesis performance on these datasets, our proposed
techniques could be seamlessly integrated into existing models for a further
performance boost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ACM MM 2023, code is available at
  https://github.com/kobeshegu/SDTM-GAN-ACMMM-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Learned Image Reasoning Prior Penetrates Deep Unfolding Network for
  Panchromatic and Multi-Spectral Image Fusion <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of deep neural networks for pan-sharpening is commonly in a form
of black box, lacking transparency and interpretability. To alleviate this
issue, we propose a novel model-driven deep unfolding framework with image
reasoning prior tailored for the pan-sharpening task. Different from existing
unfolding solutions that deliver the proximal operator networks as the
uncertain and vague priors, our framework is motivated by the content reasoning
ability of masked autoencoders (MAE) with insightful designs. Specifically, the
pre-trained MAE with spatial masking strategy, acting as intrinsic reasoning
prior, is embedded into unfolding architecture. Meanwhile, the pre-trained MAE
with spatial-spectral masking strategy is treated as the regularization term
within loss function to constrain the spatial-spectral consistency. Such
designs penetrate the image reasoning prior into deep unfolding networks while
improving its interpretability and representation capability. The uniqueness of
our framework is that the holistic learning process is explicitly integrated
with the inherent physical mechanism underlying the pan-sharpening task.
Extensive experiments on multiple satellite datasets demonstrate the
superiority of our method over the existing state-of-the-art approaches. Code
will be released at \url{https://manman1995.github.io/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages; Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> SignDiff: Learning Diffusion Models for American Sign Language
  Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Sign Language Production (SLP) lacked a large-scale, pre-trained
model based on deep learning for continuous American Sign Language (ASL)
production in the past decade. This limitation hampers communication for all
individuals with disabilities relying on ASL. To address this issue, we
undertook the secondary development and utilization of How2Sign, one of the
largest publicly available ASL datasets. Despite its significance, prior
researchers in the field of sign language have not effectively employed this
corpus due to the intricacies involved in American Sign Language Production
(ASLP).
  To conduct large-scale ASLP, we propose SignDiff based on the latest work in
related fields, which is a dual-condition diffusion pre-training model that can
generate human sign language speakers from a skeleton pose. SignDiff has a
novel Frame Reinforcement Network called FR-Net, similar to dense human pose
estimation work, which enhances the correspondence between text lexical symbols
and sign language dense pose frames reduce the occurrence of multiple fingers
in the diffusion model. In addition, our ASLP method proposes two new improved
modules and a new loss function to improve the accuracy and quality of sign
language skeletal posture and enhance the ability of the model to train on
large-scale data.
  We propose the first baseline for ASL production and report the scores of
17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We also evaluated our
model on the previous mainstream dataset called PHOENIX14T, and the main
experiments achieved the results of SOTA. In addition, our image quality far
exceeds all previous results by 10 percentage points on the SSIM indicator.
Finally, we conducted ablation studies and qualitative evaluations for
discussion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for
  English to Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>j<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>k<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study investigates the effectiveness of utilizing multimodal information
in Neural Machine Translation (NMT). While prior research focused on using
multimodal data in low-resource scenarios, this study examines how image
features impact translation when added to a large-scale, pre-trained unimodal
NMT system. Surprisingly, the study finds that images might be redundant in
this context. Additionally, the research introduces synthetic noise to assess
whether images help the model deal with textual noise. Multimodal models
slightly outperform text-only models in noisy settings, even with random
images. The study's experiments translate from English to Hindi, Bengali, and
Malayalam, outperforming state-of-the-art benchmarks significantly.
Interestingly, the effect of visual context varies with source text noise: no
visual context works best for non-noisy translations, cropped image features
are optimal for low noise, and full image features work better in high-noise
scenarios. This sheds light on the role of visual context, especially in noisy
settings, opening up a new research direction for Noisy Neural Machine
Translation in multimodal setups. The research emphasizes the importance of
combining visual and textual information for improved translation in various
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Semantic Image Synthesis via Class-Adaptive Cross-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>z<span class="highlight-author"></span>z<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In semantic image synthesis, the state of the art is dominated by methods
that use spatially-adaptive normalization layers, which allow for excellent
visual generation quality and editing versatility. Granted their efficacy,
recent research efforts have focused toward finer-grained local style control
and multi-modal generation. By construction though, such layers tend to
overlook global image statistics leading to unconvincing local style editing
and causing global inconsistencies such as color or illumination distribution
shifts. Also, the semantic layout is required for mapping styles in the
generator, putting a strict alignment constraint over the features. In
response, we designed a novel architecture where cross-attention layers are
used in place of de-normalization ones for conditioning the image generation.
Our model inherits the advantages of both solutions, retaining state-of-the-art
reconstruction quality, as well as improved global and local style transfer.
Code and models available at https://github.com/TFonta/CA2SIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> From Pixels to Portraits: A Comprehensive Survey of Talking Head
  Generation Techniques and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in deep learning and computer vision have led to a surge
of interest in generating realistic talking heads. This paper presents a
comprehensive survey of state-of-the-art methods for talking head generation.
We systematically categorises them into four main approaches: image-driven,
audio-driven, video-driven and others (including neural radiance fields (NeRF),
and 3D-based methods). We provide an in-depth analysis of each method,
highlighting their unique contributions, strengths, and limitations.
Furthermore, we thoroughly compare publicly available models, evaluating them
on key aspects such as inference time and human-rated quality of the generated
outputs. Our aim is to provide a clear and concise overview of the current
landscape in talking head generation, elucidating the relationships between
different approaches and identifying promising directions for future research.
This survey will serve as a valuable reference for researchers and
practitioners interested in this rapidly evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Topology-aware MLP for Skeleton-based Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph convolution networks (GCNs) have achieved remarkable performance in
skeleton-based action recognition. However, existing previous GCN-based methods
have relied excessively on elaborate human body priors and constructed complex
feature aggregation mechanisms, which limits the generalizability of networks.
To solve these problems, we propose a novel Spatial Topology Gating Unit
(STGU), which is an MLP-based variant without extra priors, to capture the
co-occurrence topology features that encode the spatial dependency across all
joints. In STGU, to model the sample-specific and completely independent
point-wise topology attention, a new gate-based feature interaction mechanism
is introduced to activate the features point-to-point by the attention map
generated from the input. Based on the STGU, in this work, we propose the first
topology-aware MLP-based model, Ta-MLP, for skeleton-based action recognition.
In comparison with existing previous methods on three large-scale datasets,
Ta-MLP achieves competitive performance. In addition, Ta-MLP reduces the
parameters by up to 62.5% with favorable results. Compared with previous
state-of-the-art (SOAT) approaches, Ta-MLP pushes the frontier of real-time
action recognition. The code will be available at
https://github.com/BUPTSJZhang/Ta-MLP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DTrOCR: Decoder-only Transformer for Optical Character Recognition <span class="chip">WACV2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typical text recognition methods rely on an encoder-decoder structure, in
which the encoder extracts features from an image, and the decoder produces
recognized text from these features. In this study, we propose a simpler and
more effective method for text recognition, known as the Decoder-only
Transformer for Optical Character Recognition (DTrOCR). This method uses a
decoder-only Transformer to take advantage of a generative language model that
is pre-trained on a large corpus. We examined whether a generative language
model that has been successful in natural language processing can also be
effective for text recognition in computer vision. Our experiments demonstrated
that DTrOCR outperforms current state-of-the-art methods by a large margin in
the recognition of printed, handwritten, and scene text in both English and
Chinese.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DiffuVolume: Diffusion Model for Volume based Stereo Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>-<span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching is a significant part in many computer vision tasks and
driving-based applications. Recently cost volume-based methods have achieved
great success benefiting from the rich geometry information in paired images.
However, the redundancy of cost volume also interferes with the model training
and limits the performance. To construct a more precise cost volume, we
pioneeringly apply the diffusion model to stereo matching. Our method, termed
DiffuVolume, considers the diffusion model as a cost volume filter, which will
recurrently remove the redundant information from the cost volume. Two main
designs make our method not trivial. Firstly, to make the diffusion model more
adaptive to stereo matching, we eschew the traditional manner of directly
adding noise into the image but embed the diffusion model into a task-specific
module. In this way, we outperform the traditional diffusion stereo matching
method by 22% EPE improvement and 240 times inference acceleration. Secondly,
DiffuVolume can be easily embedded into any volume-based stereo matching
network with boost performance but slight parameters rise (only 2%). By adding
the DiffuVolume into well-performed methods, we outperform all the published
methods on Scene Flow, KITTI2012, KITTI2015 benchmarks and zero-shot
generalization setting. It is worth mentioning that the proposed model ranks
1st on KITTI 2012 leader board, 2nd on KITTI 2015 leader board since 15, July
2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Learning Structure-from-Motion with Graph Attention Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>é<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we tackle the problem of learning Structure-from-Motion (SfM)
through the use of graph attention networks. SfM is a classic computer vision
problem that is solved though iterative minimization of reprojection errors,
referred to as Bundle Adjustment (BA), starting from a good initialization. In
order to obtain a good enough initialization to BA, conventional methods rely
on a sequence of sub-problems (such as pairwise pose estimation, pose averaging
or triangulation) which provides an initial solution that can then be refined
using BA. In this work we replace these sub-problems by learning a model that
takes as input the 2D keypoints detected across multiple views, and outputs the
corresponding camera poses and 3D keypoint coordinates. Our model takes
advantage of graph neural networks to learn SfM-specific primitives, and we
show that it can be used for fast inference of the reconstruction for new and
unseen sequences. The experimental results show that the proposed model
outperforms competing learning-based methods, and challenges COLMAP while
having lower runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>f<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>y<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>z<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For robots to be useful outside labs and specialized factories we need a way
to teach them new useful behaviors quickly. Current approaches lack either the
generality to onboard new tasks without task-specific engineering, or else lack
the data-efficiency to do so in an amount of time that enables practical use.
In this work we explore dense tracking as a representational vehicle to allow
faster and more general learning from demonstration. Our approach utilizes
Track-Any-Point (TAP) models to isolate the relevant motion in a demonstration,
and parameterize a low-level controller to reproduce this motion across changes
in the scene configuration. We show this results in robust robot policies that
can solve complex object-arrangement tasks such as shape-matching, stacking,
and even full path-following tasks such as applying glue and sticking objects
together, all from demonstrations that can be collected in minutes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://robotap.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> SHARP Challenge 2023: Solving CAD History and pArameters Recovery from
  Point clouds and <span class="highlight-title">3D</span> scans. Overview, Datasets, Metrics, and Baselines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15966v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15966v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>z<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in geometric Deep Learning (DL) and the availability of
large Computer-Aided Design (CAD) datasets have advanced the research on
learning CAD modeling processes and relating them to real objects. In this
context, 3D reverse engineering of CAD models from 3D scans is considered to be
one of the most sought-after goals for the CAD industry. However, recent
efforts assume multiple simplifications limiting the applications in real-world
settings. The SHARP Challenge 2023 aims at pushing the research a step closer
to the real-world scenario of CAD reverse engineering through dedicated
datasets and tracks. In this paper, we define the proposed SHARP 2023 tracks,
describe the provided datasets, and propose a set of baseline methods along
with suitable evaluation metrics to assess the performance of the track
solutions. All proposed datasets along with useful routines and the evaluation
metrics are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>f<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>'<span class="highlight-author"></span>N<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of radiology reporting comprises describing and interpreting the
medical findings in radiographic images, including description of their
location and appearance. Automated approaches to radiology reporting require
the image to be encoded into a suitable token representation for input to the
language model. Previous methods commonly use convolutional neural networks to
encode an image into a series of image-level feature map representations.
However, the generated reports often exhibit realistic style but imperfect
accuracy. Inspired by recent works for image captioning in the general domain
in which each visual token corresponds to an object detected in an image, we
investigate whether using local tokens corresponding to anatomical structures
can improve the quality of the generated reports. We introduce a novel
adaptation of Faster R-CNN in which finding detection is performed for the
candidate bounding boxes extracted during anatomical structure localisation. We
use the resulting bounding box feature representations as our set of
finding-aware anatomical tokens. This encourages the extracted anatomical
tokens to be informative about the findings they contain (required for the
final task of radiology reporting). Evaluating on the MIMIC-CXR dataset of
chest X-Ray images, we show that task-aware anatomical tokens give
state-of-the-art performance when integrated into an automated reporting
pipeline, yielding generated reports with improved clinical accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Fusing Pseudo Labels with Weak Supervision for Dynamic Traffic Scenarios <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced Driver Assistance Systems (ADAS) have made significant strides,
capitalizing on computer vision to enhance perception and decision-making
capabilities. Nonetheless, the adaptation of these systems to diverse traffic
scenarios poses challenges due to shifts in data distribution stemming from
factors such as location, weather, and road infrastructure. To tackle this, we
introduce a weakly-supervised label unification pipeline that amalgamates
pseudo labels from a multitude of object detection models trained on
heterogeneous datasets. Our pipeline engenders a unified label space through
the amalgamation of labels from disparate datasets, rectifying bias and
enhancing generalization. We fine-tune multiple object detection models on
individual datasets, subsequently crafting a unified dataset featuring pseudo
labels, meticulously validated for precision. Following this, we retrain a
solitary object detection model using the merged label space, culminating in a
resilient model proficient in dynamic traffic scenarios. We put forth a
comprehensive evaluation of our approach, employing diverse datasets
originating from varied Asian countries, effectively demonstrating its efficacy
in challenging road conditions. Notably, our method yields substantial
enhancements in object detection performance, culminating in a model with
heightened resistance against domain shifts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was accepted as an extended abstract at the International
  Conference on Computer Vision (ICCV) 2023 BRAVO Workshop, Paris, France</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Latency-aware Unified Dynamic Networks for Efficient Image Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic computation has emerged as a promising avenue to enhance the
inference efficiency of deep networks. It allows selective activation of
computational units, leading to a reduction in unnecessary computations for
each input sample. However, the actual efficiency of these dynamic models can
deviate from theoretical predictions. This mismatch arises from: 1) the lack of
a unified approach due to fragmented research; 2) the focus on algorithm design
over critical scheduling strategies, especially in CUDA-enabled GPU contexts;
and 3) challenges in measuring practical latency, given that most libraries
cater to static operations. Addressing these issues, we unveil the
Latency-Aware Unified Dynamic Networks (LAUDNet), a framework that integrates
three primary dynamic paradigms-spatially adaptive computation, dynamic layer
skipping, and dynamic channel skipping. To bridge the theoretical and practical
efficiency gap, LAUDNet merges algorithmic design with scheduling optimization,
guided by a latency predictor that accurately gauges dynamic operator latency.
We've tested LAUDNet across multiple vision tasks, demonstrating its capacity
to notably reduce the latency of models like ResNet-101 by over 50% on
platforms such as V100, RTX3090, and TX2 GPUs. Notably, LAUDNet stands out in
balancing accuracy and efficiency. Code is available at:
https://www.github.com/LeapLabTHU/LAUDNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Stage-by-stage Wavelet Optimization Refinement Diffusion Model for
  Sparse-View CT Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as potential tools to tackle the challenge of
sparse-view CT reconstruction, displaying superior performance compared to
conventional methods. Nevertheless, these prevailing diffusion models
predominantly focus on the sinogram or image domains, which can lead to
instability during model training, potentially culminating in convergence
towards local minimal solutions. The wavelet trans-form serves to disentangle
image contents and features into distinct frequency-component bands at varying
scales, adeptly capturing diverse directional structures. Employing the Wavelet
transform as a guiding sparsity prior significantly enhances the robustness of
diffusion models. In this study, we present an innovative approach named the
Stage-by-stage Wavelet Optimization Refinement Diffusion (SWORD) model for
sparse-view CT reconstruction. Specifically, we establish a unified
mathematical model integrating low-frequency and high-frequency generative
models, achieving the solution with optimization procedure. Furthermore, we
perform the low-frequency and high-frequency generative models on wavelet's
decomposed components rather than sinogram or image domains, ensuring the
stability of model training. Our method rooted in established optimization
theory, comprising three distinct stages, including low-frequency generation,
high-frequency refinement and domain transform. Our experimental results
demonstrate that the proposed method outperforms existing state-of-the-art
methods both quantitatively and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Language-Image Pre-training (CLIP) models have shown promising
performance on zero-shot visual recognition tasks by learning visual
representations under natural language supervision. Recent studies attempt the
use of CLIP to tackle zero-shot anomaly detection by matching images with
normal and abnormal state prompts. However, since CLIP focuses on building
correspondence between paired text prompts and global image-level
representations, the lack of patch-level vision to text alignment limits its
capability on precise visual anomaly localization. In this work, we introduce a
training-free adaptation (TFA) framework of CLIP for zero-shot anomaly
localization. In the visual encoder, we innovate a training-free value-wise
attention mechanism to extract intrinsic local tokens of CLIP for patch-level
local description. From the perspective of text supervision, we particularly
design a unified domain-aware contrastive state prompting template. On top of
the proposed TFA, we further introduce a test-time adaptation (TTA) mechanism
to refine anomaly localization results, where a layer of trainable parameters
in the adapter is optimized using TFA's pseudo-labels and synthetic
noise-corrupted tokens. With both TFA and TTA adaptation, we significantly
exploit the potential of CLIP for zero-shot anomaly localization and
demonstrate the effectiveness of our proposed methods on various datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Attention-based CT Scan Interpolation for Lesion Segmentation of
  Colorectal Liver Metastases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>p<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Small liver lesions common to colorectal liver metastases (CRLMs) are
challenging for convolutional neural network (CNN) segmentation models,
especially when we have a wide range of slice thicknesses in the computed
tomography (CT) scans. Slice thickness of CT images may vary by clinical
indication. For example, thinner slices are used for presurgical planning when
fine anatomic details of small vessels are required. While keeping the
effective radiation dose in patients as low as possible, various slice
thicknesses are employed in CRLMs due to their limitations. However,
differences in slice thickness across CTs lead to significant performance
degradation in CT segmentation models based on CNNs. This paper proposes a
novel unsupervised attention-based interpolation model to generate intermediate
slices from consecutive triplet slices in CT scans. We integrate segmentation
loss during the interpolation model's training to leverage segmentation labels
in existing slices to generate middle ones. Unlike common interpolation
techniques in CT volumes, our model highlights the regions of interest (liver
and lesions) inside the abdominal CT scans in the interpolated slice. Moreover,
our model's outputs are consistent with the original input slices while
increasing the segmentation performance in two cutting-edge 3D segmentation
pipelines. We tested the proposed model on the CRLM dataset to upsample
subjects with thick slices and create isotropic volume for our segmentation
model. The produced isotropic dataset increases the Dice score in the
segmentation of lesions and outperforms other interpolation approaches in terms
of interpolation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Physics-Informed DeepMRI: Bridging the Gap from Heat Diffusion to
  k-Space Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>-<span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of parallel imaging (PI), alongside image-domain regularization
methods, substantial research has been dedicated to exploring $k$-space
interpolation. However, the interpretability of these methods remains an
unresolved issue. Furthermore, these approaches currently face acceleration
limitations that are comparable to those experienced by image-domain methods.
In order to enhance interpretability and overcome the acceleration limitations,
this paper introduces an interpretable framework that unifies both $k$-space
interpolation techniques and image-domain methods, grounded in the physical
principles of heat diffusion equations. Building upon this foundational
framework, a novel $k$-space interpolation method is proposed. Specifically, we
model the process of high-frequency information attenuation in $k$-space as a
heat diffusion equation, while the effort to reconstruct high-frequency
information from low-frequency regions can be conceptualized as a reverse heat
equation. However, solving the reverse heat equation poses a challenging
inverse problem. To tackle this challenge, we modify the heat equation to align
with the principles of magnetic resonance PI physics and employ the score-based
generative method to precisely execute the modified reverse heat diffusion.
Finally, experimental validation conducted on publicly available datasets
demonstrates the superiority of the proposed approach over traditional
$k$-space interpolation methods, deep learning-based $k$-space interpolation
methods, and conventional diffusion models in terms of reconstruction accuracy,
particularly in high-frequency regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> On the Potential of CLIP for Compositional Logical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we explore the possibility of using OpenAI's CLIP to perform
logically coherent grounded visual reasoning. To that end, we formalize our
terms and give a geometric analysis of how embeddings in CLIP's latent space
would need to be configured in order for the system to be logically coherent.
Our main conclusion is that, as usually configured, CLIP cannot perform such
reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings ICLP 2023, arXiv:2308.14898</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Interpretability-guided Data Augmentation for Robust Segmentation in
  Multi-centre Colonoscopy Data <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-centre colonoscopy images from various medical centres exhibit distinct
complicating factors and overlays that impact the image content, contingent on
the specific acquisition centre. Existing Deep Segmentation networks struggle
to achieve adequate generalizability in such data sets, and the currently
available data augmentation methods do not effectively address these sources of
data variability. As a solution, we introduce an innovative data augmentation
approach centred on interpretability saliency maps, aimed at enhancing the
generalizability of Deep Learning models within the realm of multi-centre
colonoscopy image segmentation. The proposed augmentation technique
demonstrates increased robustness across different segmentation models and
domains. Thorough testing on a publicly available multi-centre dataset for
polyp detection demonstrates the effectiveness and versatility of our approach,
which is observed both in quantitative and qualitative results. The code is
publicly available at:
https://github.com/nki-radiology/interpretability_augmentation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 1 table, accepted at MICCAI 2023 Workshop on
  Machine Learning in Medical Imaging (MLMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Feature Attention Network (FA-Net): A Deep-Learning Based Approach for
  Underwater Single Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>U<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>f<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater image processing and analysis have been a hotspot of study in
recent years, as more emphasis has been focused to underwater monitoring and
usage of marine resources. Compared with the open environment, underwater image
encountered with more complicated conditions such as light abortion,
scattering, turbulence, nonuniform illumination and color diffusion. Although
considerable advances and enhancement techniques achieved in resolving these
issues, they treat low-frequency information equally across the entire channel,
which results in limiting the network's representativeness. We propose a deep
learning and feature-attention-based end-to-end network (FA-Net) to solve this
problem. In particular, we propose a Residual Feature Attention Block (RFAB),
containing the channel attention, pixel attention, and residual learning
mechanism with long and short skip connections. RFAB allows the network to
focus on learning high-frequency information while skipping low-frequency
information on multi-hop connections. The channel and pixel attention mechanism
considers each channel's different features and the uneven distribution of haze
over different pixels in the image. The experimental results shows that the
FA-Net propose by us provides higher accuracy, quantitatively and qualitatively
and superiority to previous state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fourteenth International Conference on Digital Image Processing
  (ICDIP 2022), 2022, Wuhan, China, May 20-23, 2022.8 pages.5 Figures.doi:
  10.1117/12.2644516</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Semi-supervised Domain Adaptation with Inter and Intra-domain Mixing for
  Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in semantic segmentation, an inevitable challenge is
the performance degradation caused by the domain shift in real application.
Current dominant approach to solve this problem is unsupervised domain
adaptation (UDA). However, the absence of labeled target data in UDA is overly
restrictive and limits performance. To overcome this limitation, a more
practical scenario called semi-supervised domain adaptation (SSDA) has been
proposed. Existing SSDA methods are derived from the UDA paradigm and primarily
focus on leveraging the unlabeled target data and source data. In this paper,
we highlight the significance of exploiting the intra-domain information
between the limited labeled target data and unlabeled target data, as it
greatly benefits domain adaptation. Instead of solely using the scarce labeled
data for supervision, we propose a novel SSDA framework that incorporates both
inter-domain mixing and intra-domain mixing, where inter-domain mixing
mitigates the source-target domain gap and intra-domain mixing enriches the
available target domain information. By simultaneously learning from
inter-domain mixing and intra-domain mixing, the network can capture more
domain-invariant features and promote its performance on the target domain. We
also explore different domain mixing operations to better exploit the target
domain information. Comprehensive experiments conducted on the GTA5toCityscapes
and SYNTHIA2Cityscapes benchmarks demonstrate the effectiveness of our method,
surpassing previous methods by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Zero-shot Inversion Process for Image Attribute Editing with Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising diffusion models have shown outstanding performance in image
editing. Existing works tend to use either image-guided methods, which provide
a visual reference but lack control over semantic coherence, or text-guided
methods, which ensure faithfulness to text guidance but lack visual quality. To
address the problem, we propose the Zero-shot Inversion Process (ZIP), a
framework that injects a fusion of generated visual reference and text guidance
into the semantic latent space of a \textit{frozen} pre-trained diffusion
model. Only using a tiny neural network, the proposed ZIP produces diverse
content and attributes under the intuitive control of the text prompt.
Moreover, ZIP shows remarkable robustness for both in-domain and out-of-domain
attribute manipulation on real images. We perform detailed experiments on
various benchmark datasets. Compared to state-of-the-art methods, ZIP produces
images of equivalent quality while providing a realistic editing effect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Exploring Multi-Modal Contextual Knowledge for Open-Vocabulary Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we for the first time explore helpful multi-modal contextual
knowledge to understand novel categories for open-vocabulary object detection
(OVD). The multi-modal contextual knowledge stands for the joint relationship
across regions and words. However, it is challenging to incorporate such
multi-modal contextual knowledge into OVD. The reason is that previous
detection frameworks fail to jointly model multi-modal contextual knowledge, as
object detectors only support vision inputs and no caption description is
provided at test time. To this end, we propose a multi-modal contextual
knowledge distillation framework, MMC-Det, to transfer the learned contextual
knowledge from a teacher fusion transformer with diverse multi-modal masked
language modeling (D-MLM) to a student detector. The diverse multi-modal masked
language modeling is realized by an object divergence constraint upon
traditional multi-modal masked language modeling (MLM), in order to extract
fine-grained region-level visual contexts, which are vital to object detection.
Extensive experiments performed upon various detection datasets show the
effectiveness of our multi-modal context learning strategy, where our approach
well outperforms the recent state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Reconstructing Groups of People with Hypergraph Relational Reasoning <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>u<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the mutual occlusion, severe scale variation, and complex spatial
distribution, the current multi-person mesh recovery methods cannot produce
accurate absolute body poses and shapes in large-scale crowded scenes. To
address the obstacles, we fully exploit crowd features for reconstructing
groups of people from a monocular image. A novel hypergraph relational
reasoning network is proposed to formulate the complex and high-order relation
correlations among individuals and groups in the crowd. We first extract
compact human features and location information from the original
high-resolution image. By conducting the relational reasoning on the extracted
individual features, the underlying crowd collectiveness and interaction
relationship can provide additional group information for the reconstruction.
Finally, the updated individual features and the localization information are
used to regress human meshes in camera coordinates. To facilitate the network
training, we further build pseudo ground-truth on two crowd datasets, which may
also promote future research on pose estimation and human behavior
understanding in crowded scenes. The experimental results show that our
approach outperforms other baseline methods both in crowded and common
scenarios. The code and datasets are publicly available at
https://github.com/boycehbz/GroupRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Utilizing Task-Generic Motion Prior to Recover Full-Body Motion from
  Very Sparse Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>K<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The most popular type of devices used to track a user's posture in a virtual
reality experience consists of a head-mounted display and two controllers held
in both hands. However, due to the limited number of tracking sensors (three in
total), faithfully recovering the user in full-body is challenging, limiting
the potential for interactions among simulated user avatars within the virtual
world. Therefore, recent studies have attempted to reconstruct full-body poses
using neural networks that utilize previously learned human poses or accept a
series of past poses over a short period. In this paper, we propose a method
that utilizes information from a neural motion prior to improve the accuracy of
reconstructed user's motions. Our approach aims to reconstruct user's full-body
poses by predicting the latent representation of the user's overall motion from
limited input signals and integrating this information with tracking sensor
inputs. This is based on the premise that the ultimate goal of pose
reconstruction is to reconstruct the motion, which is a series of poses. Our
results show that this integration enables more accurate reconstruction of the
user's full-body motion, particularly enhancing the robustness of lower body
motion reconstruction from impoverished signals. Web:
https://https://mjsh34.github.io/mp-sspe/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Early Detection of Red Palm Weevil Infestations using Deep Learning
  Classification of Acoustic Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>y<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Red Palm Weevil (RPW), also known as the palm weevil, is considered among
the world's most damaging insect pests of palms. Current detection techniques
include the detection of symptoms of RPW using visual or sound inspection and
chemical detection of volatile signatures generated by infested palm trees.
However, efficient detection of RPW diseases at an early stage is considered
one of the most challenging issues for cultivating date palms. In this paper,
an efficient approach to the early detection of RPW is proposed. The proposed
approach is based on RPW sound activities being recorded and analyzed. The
first step involves the conversion of sound data into images based on a
selected set of features. The second step involves the combination of images
from the same sound file but computed by different features into a single
image. The third step involves the application of different Deep Learning (DL)
techniques to classify resulting images into two classes: infested and not
infested. Experimental results show good performances of the proposed approach
for RPW detection using different DL techniques, namely MobileNetV2,
ResNet50V2, ResNet152V2, VGG16, VGG19, DenseNet121, DenseNet201, Xception, and
InceptionV3. The proposed approach outperformed existing techniques for public
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Introducing Language Guidance in Prompt-based Continual Learning <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>f<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning aims to learn a single model on a sequence of tasks
without having access to data from previous tasks. The biggest challenge in the
domain still remains catastrophic forgetting: a loss in performance on seen
classes of earlier tasks. Some existing methods rely on an expensive replay
buffer to store a chunk of data from previous tasks. This, while promising,
becomes expensive when the number of tasks becomes large or data can not be
stored for privacy reasons. As an alternative, prompt-based methods have been
proposed that store the task information in a learnable prompt pool. This
prompt pool instructs a frozen image encoder on how to solve each task. While
the model faces a disjoint set of classes in each task in this setting, we
argue that these classes can be encoded to the same embedding space of a
pre-trained language encoder. In this work, we propose Language Guidance for
Prompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.
LGCL is model agnostic and introduces language guidance at the task level in
the prompt pool and at the class level on the output feature of the vision
encoder. We show with extensive experimentation that LGCL consistently improves
the performance of prompt-based continual learning methods to set a new
state-of-the art. LGCL achieves these performance improvements without needing
any additional learnable parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> AMDNet23: A combined deep Contour-based Convolutional Neural Network and
  Long Short Term Memory system to diagnose Age-related Macular Degeneration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>d<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>d<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>d<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>k<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>b<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>s<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In light of the expanding population, an automated framework of disease
detection can assist doctors in the diagnosis of ocular diseases, yields
accurate, stable, rapid outcomes, and improves the success rate of early
detection. The work initially intended the enhancing the quality of fundus
images by employing an adaptive contrast enhancement algorithm (CLAHE) and
Gamma correction. In the preprocessing techniques, CLAHE elevates the local
contrast of the fundus image and gamma correction increases the intensity of
relevant features. This study operates on a AMDNet23 system of deep learning
that combined the neural networks made up of convolutions (CNN) and short-term
and long-term memory (LSTM) to automatically detect aged macular degeneration
(AMD) disease from fundus ophthalmology. In this mechanism, CNN is utilized for
extracting features and LSTM is utilized to detect the extracted features. The
dataset of this research is collected from multiple sources and afterward
applied quality assessment techniques, 2000 experimental fundus images
encompass four distinct classes equitably. The proposed hybrid deep AMDNet23
model demonstrates to detection of AMD ocular disease and the experimental
result achieved an accuracy 96.50%, specificity 99.32%, sensitivity 96.5%, and
F1-score 96.49.0%. The system achieves state-of-the-art findings on fundus
imagery datasets to diagnose AMD ocular disease and findings effectively
potential of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Improving Underwater Visual Tracking With a Large Scale Dataset and
  Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>j<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>U<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new dataset and general tracker enhancement method for
Underwater Visual Object Tracking (UVOT). Despite its significance, underwater
tracking has remained unexplored due to data inaccessibility. It poses distinct
challenges; the underwater environment exhibits non-uniform lighting
conditions, low visibility, lack of sharpness, low contrast, camouflage, and
reflections from suspended particles. Performance of traditional tracking
methods designed primarily for terrestrial or open-air scenarios drops in such
conditions. We address the problem by proposing a novel underwater image
enhancement algorithm designed specifically to boost tracking quality. The
method has resulted in a significant performance improvement, of up to 5.0%
AUC, of state-of-the-art (SOTA) visual trackers. To develop robust and accurate
UVOT methods, large-scale datasets are required. To this end, we introduce a
large-scale UVOT benchmark dataset consisting of 400 video segments and 275,000
manually annotated frames enabling underwater training and evaluation of deep
trackers. The videos are labelled with several underwater-specific tracking
attributes including watercolor variation, target distractors, camouflage,
target relative size, and low visibility conditions. The UVOT400 dataset,
tracking results, and the code are publicly available on:
https://github.com/BasitAlawode/UWVOT400.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ACNPU: A 4.75TOPS/W 1080P@30FPS Super Resolution Accelerator with
  Decoupled Asymmetric Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-driven superresolution (SR) outperforms traditional techniques
but also faces the challenge of high complexity and memory bandwidth. This
challenge leads many accelerators to opt for simpler and shallow models like
FSRCNN, compromising performance for real-time needs, especially for
resource-limited edge devices. This paper proposes an energy-efficient SR
accelerator, ACNPU, to tackle this challenge. The ACNPU enhances image quality
by 0.34dB with a 27-layer model, but needs 36\% less complexity than FSRCNN,
while maintaining a similar model size, with the \textit{decoupled asymmetric
convolution and split-bypass structure}. The hardware-friendly 17K-parameter
model enables \textit{holistic model fusion} instead of localized layer fusion
to remove external DRAM access of intermediate feature maps. The on-chip memory
bandwidth is further reduced with the \textit{input stationary flow} and
\textit{parallel-layer execution} to reduce power consumption. Hardware is
regular and easy to control to support different layers by \textit{processing
elements (PEs) clusters with reconfigurable input and uniform data flow}. The
implementation in the 40 nm CMOS process consumes 2333 K gate counts and 198KB
SRAMs. The ACNPU achieves 31.7 FPS and 124.4 FPS for x2 and x4 scales Full-HD
generation, respectively, which attains 4.75 TOPS/W energy efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Occlusion-Aware Detection and Re-ID Calibrated Network for Multi-Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Object Tracking (MOT) is a crucial computer vision task that aims to
predict the bounding boxes and identities of objects simultaneously. While
state-of-the-art methods have made remarkable progress by jointly optimizing
the multi-task problems of detection and Re-ID feature learning, yet, few
approaches explore to tackle the occlusion issue, which is a long-standing
challenge in the MOT field. Generally, occluded objects may hinder the detector
from estimating the bounding boxes, resulting in fragmented trajectories. And
the learned occluded Re-ID embeddings are less distinct since they contain
interferer. To this end, we propose an occlusion-aware detection and Re-ID
calibrated network for multi-object tracking, termed as ORCTrack. Specifically,
we propose an Occlusion-Aware Attention (OAA) module in the detector that
highlights the object features while suppressing the occluded background
regions. OAA can serve as a modulator that enhances the detector for some
potentially occluded objects. Furthermore, we design a Re-ID embedding matching
block based on the optimal transport problem, which focuses on enhancing and
calibrating the Re-ID representations through different adjacent frames
complementarily. To validate the effectiveness of the proposed method,
extensive experiments are conducted on two challenging VisDrone2021-MOT and
KITTI benchmarks. Experimental evaluations demonstrate the superiority of our
approach, which can achieve new state-of-the-art performance and enjoy high
run-time efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Neural Video Compression with Temporal Layer-Adaptive Hierarchical
  B-frame Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural video compression (NVC) is a rapidly evolving video coding research
area, with some models achieving superior coding efficiency compared to the
latest video coding standard Versatile Video Coding (VVC). In conventional
video coding standards, the hierarchical B-frame coding, which utilizes a
bidirectional prediction structure for higher compression, had been
well-studied and exploited. In NVC, however, limited research has investigated
the hierarchical B scheme. In this paper, we propose an NVC model exploiting
hierarchical B-frame coding with temporal layer-adaptive optimization. We first
extend an existing unidirectional NVC model to a bidirectional model, which
achieves -21.13% BD-rate gain over the unidirectional baseline model. However,
this model faces challenges when applied to sequences with complex or large
motions, leading to performance degradation. To address this, we introduce
temporal layer-adaptive optimization, incorporating methods such as temporal
layer-adaptive quality scaling (TAQS) and temporal layer-adaptive latent
scaling (TALS). The final model with the proposed methods achieves an
impressive BD-rate gain of -39.86% against the baseline. It also resolves the
challenges in sequences with large or complex motions with up to -49.13% more
BD-rate gains than the simple bidirectional extension. This improvement is
attributed to the allocation of more bits to lower temporal layers, thereby
enhancing overall reconstruction quality with smaller bits. Since our method
has little dependency on a specific NVC model architecture, it can serve as a
general tool for extending unidirectional NVC models to the ones with
hierarchical B-frame coding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Large-scale data extraction from the UNOS or<span class="highlight-title">gan</span> donor documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>y<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scope of our study is all UNOS data of the USA organ donors since 2008.
The data is not analyzable in a large scale in the past because it was captured
in PDF documents known as "Attachments", whereby every donor is represented by
dozens of PDF documents in heterogenous formats. To make the data analyzable,
one needs to convert the content inside these PDFs to an analyzable data
format, such as a standard SQL database. In this paper we will focus on 2022
UNOS data comprised of $\approx 400,000$ PDF documents spanning millions of
pages. The totality of UNOS data covers 15 years (2008--20022) and our results
will be quickly extended to the entire data. Our method captures a portion of
the data in DCD flowsheets, kidney perfusion data, and data captured during
patient hospital stay (e.g. vital signs, ventilator settings, etc.). The
current paper assumes that the reader is familiar with the content of the UNOS
data. The overview of the types of data and challenges they present is a
subject of another paper. Here we focus on demonstrating that the goal of
building a comprehensive, analyzable database from UNOS documents is an
attainable task, and we provide an overview of our methodology. The project
resulted in datasets by far larger than previously available even in this
preliminary phase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Beard Segmentation and Recognition Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>z<span class="highlight-author"></span>t<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A person's facial hairstyle, such as presence and size of beard, can
significantly impact face recognition accuracy. There are publicly-available
deep networks that achieve reasonable accuracy at binary attribute
classification, such as beard / no beard, but few if any that segment the
facial hair region. To investigate the effect of facial hair in a rigorous
manner, we first created a set of fine-grained facial hair annotations to train
a segmentation model and evaluate its accuracy across African-American and
Caucasian face images. We then use our facial hair segmentations to categorize
image pairs according to the degree of difference or similarity in the facial
hairstyle. We find that the False Match Rate (FMR) for image pairs with
different categories of facial hairstyle varies by a factor of over 10 for
African-American males and over 25 for Caucasian males. To reduce the bias
across image pairs with different facial hairstyles, we propose a scheme for
adaptive thresholding based on facial hairstyle similarity. Evaluation on a
subject-disjoint set of images shows that adaptive similarity thresholding
based on facial hairstyles of the image pair reduces the ratio between the
highest and lowest FMR across facial hairstyle categories for African-American
from 10.7 to 1.8 and for Caucasians from 25.9 to 1.3. Facial hair annotations
and facial hair segmentation model will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Drone-<span class="highlight-title">NeRF</span>: Efficient <span class="highlight-title">NeRF</span> Based <span class="highlight-title">3D</span> Scene Reconstruction for Large-Scale
  Drone Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural rendering has garnered substantial attention owing to its capacity for
creating realistic 3D scenes. However, its applicability to extensive scenes
remains challenging, with limitations in effectiveness. In this work, we
propose the Drone-NeRF framework to enhance the efficient reconstruction of
unbounded large-scale scenes suited for drone oblique photography using Neural
Radiance Fields (NeRF). Our approach involves dividing the scene into uniform
sub-blocks based on camera position and depth visibility. Sub-scenes are
trained in parallel using NeRF, then merged for a complete scene. We refine the
model by optimizing camera poses and guiding NeRF with a uniform sampler.
Integrating chosen samples enhances accuracy. A hash-coded fusion MLP
accelerates density representation, yielding RGB and Depth outputs. Our
framework accounts for sub-scene constraints, reduces parallel-training noise,
handles shadow occlusion, and merges sub-regions for a polished rendering
result. This Drone-NeRF framework demonstrates promising capabilities in
addressing challenges related to scene complexity, rendering efficiency, and
accuracy in drone-obtained imagery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, in submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Background Debiased SAR Target Recognition via Causal Interventional
  Regularizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have utilized deep learning (DL) techniques to automatically
extract features from synthetic aperture radar (SAR) images, which shows great
promise for enhancing the performance of SAR automatic target recognition
(ATR). However, our research reveals a previously overlooked issue: SAR images
to be recognized include not only the foreground (i.e., the target), but also a
certain size of the background area. When a DL-model is trained exclusively on
foreground data, its recognition performance is significantly superior to a
model trained on original data that includes both foreground and background.
This suggests that the presence of background impedes the ability of the
DL-model to learn additional semantic information about the target. To address
this issue, we construct a structural causal model (SCM) that incorporates the
background as a confounder. Based on the constructed SCM, we propose a causal
intervention based regularization method to eliminate the negative impact of
background on feature semantic learning and achieve background debiased
SAR-ATR. The proposed causal interventional regularizer can be integrated into
any existing DL-based SAR-ATR models to mitigate the impact of background
interference on the feature extraction and recognition accuracy. Experimental
results on the Moving and Stationary Target Acquisition and Recognition (MSTAR)
dataset indicate that the proposed method can enhance the efficiency of
existing DL-based methods in a plug-and-play manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Towards Earlier Detection of Oral Diseases On Smartphones Using Oral and
  Dental RGB Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15705v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15705v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oral diseases such as periodontal (gum) diseases and dental caries (cavities)
affect billions of people across the world today. However, previous
state-of-the-art models have relied on X-ray images to detect oral diseases,
making them inaccessible to remote monitoring, developing countries, and
telemedicine. To combat this overuse of X-ray imagery, we propose a lightweight
machine learning model capable of detecting calculus (also known as hardened
plaque or tartar) in RGB images while running efficiently on low-end devices.
The model, a modified MobileNetV3-Small neural network transfer learned from
ImageNet, achieved an accuracy of 72.73% (which is comparable to
state-of-the-art solutions) while still being able to run on mobile devices due
to its reduced memory requirements and processing times. A ResNet34-based model
was also constructed and achieved an accuracy of 81.82%. Both of these models
were tested on a mobile app, demonstrating their potential to limit the number
of serious oral disease cases as their predictions can help patients schedule
appointments earlier without the need to go to the clinic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 1 formula. This research was conducted as a
  mentored project performed for a college course and research program at the
  University of California Santa Barbara's Summer Research Academies program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Intriguing Properties of Diffusion Models: A Large-Scale Dataset for
  Evaluating Natural Attack Capability in Text-to-Image <span class="highlight-title">Generative</span> Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>f<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising probabilistic diffusion models have shown breakthrough performance
that can generate more photo-realistic images or human-level illustrations than
the prior models such as GANs. This high image-generation capability has
stimulated the creation of many downstream applications in various areas.
However, we find that this technology is indeed a double-edged sword: We
identify a new type of attack, called the Natural Denoising Diffusion (NDD)
attack based on the finding that state-of-the-art deep neural network (DNN)
models still hold their prediction even if we intentionally remove their robust
features, which are essential to the human visual system (HVS), by text
prompts. The NDD attack can generate low-cost, model-agnostic, and
transferrable adversarial attacks by exploiting the natural attack capability
in diffusion models. Motivated by the finding, we construct a large-scale
dataset, Natural Denoising Diffusion Attack (NDDA) dataset, to systematically
evaluate the risk of the natural attack capability of diffusion models with
state-of-the-art text-to-image diffusion models. We evaluate the natural attack
capability by answering 6 research questions. Through a user study to confirm
the validity of the NDD attack, we find that the NDD attack can achieve an 88%
detection rate while being stealthy to 93% of human subjects. We also find that
the non-robust features embedded by diffusion models contribute to the natural
attack capability. To confirm the model-agnostic and transferrable attack
capability, we perform the NDD attack against an AD vehicle and find that 73%
of the physically printed attacks can be detected as a stop sign. We hope that
our study and dataset can help our community to be aware of the risk of
diffusion models and facilitate further research toward robust DNN models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present 'CongNaMul', a comprehensive dataset designed for various tasks in
soybean sprouts image analysis. The CongNaMul dataset is curated to facilitate
tasks such as image classification, semantic segmentation, decomposition, and
measurement of length and weight. The classification task provides four classes
to determine the quality of soybean sprouts: normal, broken, spotted, and
broken and spotted, for the development of AI-aided automatic quality
inspection technology. For semantic segmentation, images with varying
complexity, from single sprout images to images with multiple sprouts, along
with human-labelled mask images, are included. The label has 4 different
classes: background, head, body, tail. The dataset also provides images and
masks for the image decomposition task, including two separate sprout images
and their combined form. Lastly, 5 physical features of sprouts (head length,
body length, body thickness, tail length, weight) are provided for image-based
measurement tasks. This dataset is expected to be a valuable resource for a
wide range of research and applications in the advanced analysis of images of
soybean sprouts. Also, we hope that this dataset can assist researchers
studying classification, semantic segmentation, decomposition, and physical
feature measurement in other industrial fields, in evaluating their models. The
dataset is available at the authors' repository. (https://bhban.kr/data)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to International Conference on ICT Convergence 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Going Beyond Nouns With Vision & Language Models Using Synthetic Data <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>ü<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>k<span class="highlight-author"></span>y<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale pre-trained Vision & Language (VL) models have shown remarkable
performance in many applications, enabling replacing a fixed set of supported
classes with zero-shot open vocabulary reasoning over (almost arbitrary)
natural language prompts. However, recent works have uncovered a fundamental
weakness of these models. For example, their difficulty to understand Visual
Language Concepts (VLC) that go 'beyond nouns' such as the meaning of
non-object words (e.g., attributes, actions, relations, states, etc.), or
difficulty in performing compositional reasoning such as understanding the
significance of the order of the words in a sentence. In this work, we
investigate to which extent purely synthetic data could be leveraged to teach
these models to overcome such shortcomings without compromising their zero-shot
capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale
synthetic dataset and data generation codebase allowing to generate additional
suitable data to improve VLC understanding and compositional reasoning of VL
models. Additionally, we propose a general VL finetuning strategy for
effectively leveraging SyViC towards achieving these improvements. Our
extensive experiments and ablations on VL-Checklist, Winoground, and ARO
benchmarks demonstrate that it is possible to adapt strong pre-trained VL
models with synthetic data significantly enhancing their VLC understanding
(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their
zero-shot accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023. Project page: https://synthetic-vic.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CartiMorph: a framework for automated knee articular cartilage
  morphometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.01981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.01981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CartiMorph, a framework for automated knee articular cartilage
morphometrics. It takes an image as input and generates quantitative metrics
for cartilage subregions, including the percentage of full-thickness cartilage
loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the
power of deep learning models for hierarchical image feature representation.
Deep learning models were trained and validated for tissue segmentation,
template construction, and template-to-image registration. We established
methods for surface-normal-based cartilage thickness mapping, FCL estimation,
and rule-based cartilage parcellation. Our cartilage thickness map showed less
error in thin and peripheral regions. We evaluated the effectiveness of the
adopted segmentation model by comparing the quantitative metrics obtained from
model segmentation and those from manual segmentation. The root-mean-squared
deviation of the FCL measurements was less than 8%, and strong correlations
were observed for the mean thickness (Pearson's correlation coefficient $\rho
\in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in
[0.89,0.98]$) measurements. We compared our FCL measurements with those from a
previous study and found that our measurements deviated less from the ground
truths. We observed superior performance of the proposed rule-based cartilage
parcellation method compared with the atlas-based approach. CartiMorph has the
potential to promote imaging biomarkers discovery for knee osteoarthritis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with
  Wavelet Augmentation Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17783v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17783v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Domain Adaptation (UDA) can effectively address domain gap
issues in real-world image Super-Resolution (SR) by accessing both the source
and target data. Considering privacy policies or transmission restrictions of
source data in practical scenarios, we propose a SOurce-free Domain Adaptation
framework for image SR (SODA-SR) to address this issue, i.e., adapt a
source-trained model to a target domain with only unlabeled target data.
SODA-SR leverages the source-trained model to generate refined pseudo-labels
for teacher-student learning. To better utilize pseudo-labels, we propose a
novel wavelet-based augmentation method, named Wavelet Augmentation Transformer
(WAT), which can be flexibly incorporated with existing networks, to implicitly
produce useful augmented data. WAT learns low-frequency information of varying
levels across diverse samples, which is aggregated efficiently via deformable
attention. Furthermore, an uncertainty-aware self-training mechanism is
proposed to improve the accuracy of pseudo-labels, with inaccurate predictions
being rectified by uncertainty estimation. To acquire better SR results and
avoid overfitting pseudo-labels, several regularization losses are proposed to
constrain target LR and SR images in the frequency domain. Experiments show
that without accessing source data, SODA-SR outperforms state-of-the-art UDA
methods in both synthetic$\rightarrow$real and real$\rightarrow$real adaptation
settings, and is not constrained by specific network architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Conditioning Diffusion Models via Attributes and Semantic Masks for Face
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00914v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00914v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models have shown impressive results in generating realistic
images of faces. GANs managed to generate high-quality, high-fidelity images
when conditioned on semantic masks, but they still lack the ability to
diversify their output. Diffusion models partially solve this problem and are
able to generate diverse samples given the same condition. In this paper, we
propose a multi-conditioning approach for diffusion models via cross-attention
exploiting both attributes and semantic masks to generate high-quality and
controllable face images. We also studied the impact of applying
perceptual-focused loss weighting into the latent space instead of the pixel
space. Our method extends the previous approaches by introducing conditioning
on more than one set of features, guaranteeing a more fine-grained control over
the generated face images. We evaluate our approach on the CelebA-HQ dataset,
and we show that it can generate realistic and diverse samples while allowing
for fine-grained control over multiple attributes and semantic regions.
Additionally, we perform an ablation study to evaluate the impact of different
conditioning strategies on the quality and diversity of the generated images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> What You Hear Is What You See: Audio Quality Metrics From Image Quality
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span>b<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>-<span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we investigate the feasibility of utilizing state-of-the-art
image perceptual metrics for evaluating audio signals by representing them as
spectrograms. The encouraging outcome of the proposed approach is based on the
similarity between the neural mechanisms in the auditory and visual pathways.
Furthermore, we customise one of the metrics which has a psychoacoustically
plausible architecture to account for the peculiarities of sound signals. We
evaluate the effectiveness of our proposed metric and several baseline metrics
using a music dataset, with promising results in terms of the correlation
between the metrics and the perceived quality of audio as rated by human
evaluators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Context-VQA: Towards Context-Aware and Purposeful Visual Question
  Answering <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual question answering (VQA) has the potential to make the Internet more
accessible in an interactive way, allowing people who cannot see images to ask
questions about them. However, multiple studies have shown that people who are
blind or have low-vision prefer image explanations that incorporate the context
in which an image appears, yet current VQA datasets focus on images in
isolation. We argue that VQA models will not fully succeed at meeting people's
needs unless they take context into account. To further motivate and analyze
the distinction between different contexts, we introduce Context-VQA, a VQA
dataset that pairs images with contexts, specifically types of websites (e.g.,
a shopping website). We find that the types of questions vary systematically
across contexts. For example, images presented in a travel context garner 2
times more "Where?" questions, and images on social media and news garner 2.8
and 1.8 times more "Who?" questions than the average. We also find that context
effects are especially important when participants can't see the image. These
results demonstrate that context affects the types of questions asked and that
VQA models should be context-sensitive to better meet people's needs,
especially in accessibility settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of ICCV 2023 Workshop on Closing the Loop Between Vision
  and Language</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Priority-Centric Human Motion Generation in Discrete Latent Space <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14480v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14480v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-motion generation is a formidable task, aiming to produce human
motions that align with the input text while also adhering to human
capabilities and physical laws. While there have been advancements in diffusion
models, their application in discrete spaces remains underexplored. Current
methods often overlook the varying significance of different motions, treating
them uniformly. It is essential to recognize that not all motions hold the same
relevance to a particular textual description. Some motions, being more salient
and informative, should be given precedence during generation. In response, we
introduce a Priority-Centric Motion Discrete Diffusion Model (M2DM), which
utilizes a Transformer-based VQ-VAE to derive a concise, discrete motion
representation, incorporating a global self-attention mechanism and a
regularization term to counteract code collapse. We also present a motion
discrete diffusion model that employs an innovative noise schedule, determined
by the significance of each motion token within the entire motion sequence.
This approach retains the most salient motions during the reverse diffusion
process, leading to more semantically rich and varied motions. Additionally, we
formulate two strategies to gauge the importance of motion tokens, drawing from
both textual and visual indicators. Comprehensive experiments on the HumanML3D
and KIT-ML datasets confirm that our model surpasses existing techniques in
fidelity and diversity, particularly for intricate textual descriptions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> NeXtQSM -- A complete deep learning pipeline for data-consistent
  quantitative susceptibility mapping trained with hybrid data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.07752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.07752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>g<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>'<span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great
potential in recent years, obtaining similar results to established
non-learning approaches. Many current deep learning approaches are not data
consistent, require in vivo training data or solve the QSM problem in
consecutive steps resulting in the propagation of errors. Here we aim to
overcome these limitations and developed a framework to solve the QSM
processing steps jointly. We developed a new hybrid training data generation
method that enables the end-to-end training for solving background field
correction and dipole inversion in a data-consistent fashion using a
variational network that combines the QSM model term and a learned regularizer.
We demonstrate that NeXtQSM overcomes the limitations of previous deep learning
methods. NeXtQSM offers a new deep learning based pipeline for computing
quantitative susceptibility maps that integrates each processing step into the
training and provides results that are robust and fast.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> TAPIR: Tracking Any Point with per-frame Initialization and temporal
  Refinement <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>f<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>y<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel model for Tracking Any Point (TAP) that effectively tracks
any queried point on any physical surface throughout a video sequence. Our
approach employs two stages: (1) a matching stage, which independently locates
a suitable candidate point match for the query point on every other frame, and
(2) a refinement stage, which updates both the trajectory and query features
based on local correlations. The resulting model surpasses all baseline methods
by a significant margin on the TAP-Vid benchmark, as demonstrated by an
approximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our model
facilitates fast inference on long and high-resolution video sequences. On a
modern GPU, our implementation has the capacity to track points faster than
real-time, and can be flexibly extended to higher-resolution videos. Given the
high-quality trajectories extracted from a large dataset, we demonstrate a
proof-of-concept diffusion model which generates trajectories from static
images, enabling plausible animations. Visualizations, source code, and
pretrained models can be found on our project webpage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> TriangleNet: Edge Prior Augmented Network for Semantic Segmentation
  through Cross-Task Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05152v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05152v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the task of semantic segmentation in computer vision,
aiming to achieve precise pixel-wise classification. We investigate the joint
training of models for semantic edge detection and semantic segmentation, which
has shown promise. However, implicit cross-task consistency learning in
multi-task networks is limited. To address this, we propose a novel "decoupled
cross-task consistency loss" that explicitly enhances cross-task consistency.
Our semantic segmentation network, TriangleNet, achieves a substantial 2.88\%
improvement over the Baseline in mean Intersection over Union (mIoU) on the
Cityscapes test set. Notably, TriangleNet operates at 77.4\% mIoU/46.2 FPS on
Cityscapes, showcasing real-time inference capabilities at full resolution.
With multi-scale inference, performance is further enhanced to 77.8\%.
Furthermore, TriangleNet consistently outperforms the Baseline on the FloodNet
dataset, demonstrating its robust generalization capabilities. The proposed
method underscores the significance of multi-task learning and explicit
cross-task consistency enhancement for advancing semantic segmentation and
highlights the potential of multitasking in real-time semantic segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the journal "International Journal of
  Intelligent Systems"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DREAM: Efficient Dataset Distillation by Representative Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14416v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14416v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation aims to synthesize small datasets with little
information loss from original large-scale ones for reducing storage and
training costs. Recent state-of-the-art methods mainly constrain the sample
synthesis process by matching synthetic images and the original ones regarding
gradients, embedding distributions, or training trajectories. Although there
are various matching objectives, currently the strategy for selecting original
images is limited to naive random sampling.
  We argue that random sampling overlooks the evenness of the selected sample
distribution, which may result in noisy or biased matching targets.
  Besides, the sample diversity is also not constrained by random sampling.
These factors together lead to optimization instability in the distilling
process and degrade the training efficiency. Accordingly, we propose a novel
matching strategy named as \textbf{D}ataset distillation by
\textbf{RE}present\textbf{A}tive \textbf{M}atching (DREAM), where only
representative original images are selected for matching. DREAM is able to be
easily plugged into popular dataset distillation frameworks and reduce the
distilling iterations by more than 8 times without performance drop. Given
sufficient training time, DREAM further provides significant improvements and
achieves state-of-the-art performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Efficient matching for dataset distillation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> LAC -- Latent Action Composition for Skeleton-based Action Segmentation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skeleton-based action segmentation requires recognizing composable actions in
untrimmed videos. Current approaches decouple this problem by first extracting
local visual features from skeleton sequences and then processing them by a
temporal model to classify frame-wise actions. However, their performances
remain limited as the visual features cannot sufficiently express composable
actions. In this context, we propose Latent Action Composition (LAC), a novel
self-supervised framework aiming at learning from synthesized composable
motions for skeleton-based action segmentation. LAC is composed of a novel
generation module towards synthesizing new sequences. Specifically, we design a
linear latent space in the generator to represent primitive motion. New
composed motions can be synthesized by simply performing arithmetic operations
on latent representations of multiple input skeleton sequences. LAC leverages
such synthesized sequences, which have large diversity and complexity, for
learning visual representations of skeletons in both sequence and frame spaces
via contrastive learning. The resulting visual encoder has a high expressive
power and can be effectively transferred onto action segmentation tasks by
end-to-end fine-tuning without the need for additional temporal models. We
conduct a study focusing on transfer-learning and we show that representations
learned from pre-trained LAC outperform the state-of-the-art by a large margin
on TSU, Charades, PKU-MMD datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Exploring the Benefits of Visual Prompting in Differential Privacy <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.12247v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.12247v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompting (VP) is an emerging and powerful technique that allows
sample-efficient adaptation to downstream tasks by engineering a well-trained
frozen source model. In this work, we explore the benefits of VP in
constructing compelling neural network classifiers with differential privacy
(DP). We explore and integrate VP into canonical DP training methods and
demonstrate its simplicity and efficiency. In particular, we discover that VP
in tandem with PATE, a state-of-the-art DP training method that leverages the
knowledge transfer from an ensemble of teachers, achieves the state-of-the-art
privacy-utility trade-off with minimum expenditure of privacy budget. Moreover,
we conduct additional experiments on cross-domain image classification with a
sufficient domain gap to further unveil the advantage of VP in DP. Lastly, we
also conduct extensive ablation studies to validate the effectiveness and
contribution of VP under DP consideration. Our code is available at
(https://github.com/EzzzLi/Prompt-PATE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> DeltaNN: Assessing the Impact of Computational Environment Parameters on
  the Performance of Image Recognition Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06208v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06208v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>é<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image recognition tasks typically use deep learning and require enormous
processing power, thus relying on hardware accelerators like GPUs and TPUs for
fast, timely processing. Failure in real-time image recognition tasks can occur
due to sub-optimal mapping on hardware accelerators during model deployment,
which may lead to timing uncertainty and erroneous behavior. Mapping on
hardware accelerators is done using multiple software components like deep
learning frameworks, compilers, and device libraries, that we refer to as the
computational environment. Owing to the increased use of image recognition
tasks in safety-critical applications like autonomous driving and medical
imaging, it is imperative to assess their robustness to changes in the
computational environment, as the impact of parameters like deep learning
frameworks, compiler optimizations, and hardware devices on model performance
and correctness is not yet well understood.
  In this paper we present a differential testing framework, DeltaNN, that
allows us to assess the impact of different computational environment
parameters on the performance of image recognition models during deployment,
post training. DeltaNN generates different implementations of a given image
recognition model for variations in environment parameters, namely, deep
learning frameworks, compiler optimizations and hardware devices and analyzes
differences in model performance as a result. Using DeltaNN, we conduct an
empirical study of robustness analysis of three popular image recognition
models using the ImageNet dataset. We report the impact in terms of
misclassifications and inference time differences across different settings. In
total, we observed up to 72% output label differences across deep learning
frameworks, and up to 81% unexpected performance degradation in terms of
inference time, when applying compiler optimizations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Laughing Matters: Introducing Laughing-Face Generation using Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>y<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven animation has gained significant traction in recent years, with
current methods achieving near-photorealistic results. However, the field
remains underexplored regarding non-verbal communication despite evidence
demonstrating its importance in human interaction. In particular, generating
laughter sequences presents a unique challenge due to the intricacy and nuances
of this behaviour. This paper aims to bridge this gap by proposing a novel
model capable of generating realistic laughter sequences, given a still
portrait and an audio clip containing laughter. We highlight the failure cases
of traditional facial animation methods and leverage recent advances in
diffusion models to produce convincing laughter videos. We train our model on a
diverse set of laughter datasets and introduce an evaluation metric
specifically designed for laughter. When compared with previous speech-driven
approaches, our model achieves state-of-the-art performance across all metrics,
even when these are re-trained for laughter generation. Our code and project
are publicly available
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Discriminator-free Unsupervised Domain Adaptation for Multi-label Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>I<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a discriminator-free adversarial-based Unsupervised Domain
Adaptation (UDA) for Multi-Label Image Classification (MLIC) referred to as
DDA-MLIC is proposed. Recently, some attempts have been made for introducing
adversarial-based UDA methods in the context of MLIC. However, these methods
which rely on an additional discriminator subnet present one major shortcoming.
The learning of domain-invariant features may harm their task-specific
discriminative power, since the classification and discrimination tasks are
decoupled. Herein, we propose to overcome this issue by introducing a novel
adversarial critic that is directly deduced from the task-specific classifier.
Specifically, a two-component Gaussian Mixture Model (GMM) is fitted on the
source and target predictions in order to distinguish between two clusters.
This allows extracting a Gaussian distribution for each component. The
resulting Gaussian distributions are then used for formulating an adversarial
loss based on a Frechet distance. The proposed method is evaluated on several
multi-label image datasets covering three different types of domain shift. The
obtained results demonstrate that DDA-MLIC outperforms existing
state-of-the-art methods in terms of precision while requiring a lower number
of parameters. The code will be made publicly available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Fault Localization for Buggy Deep Learning Framework Conversions in
  Image Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06157v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06157v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>é<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deploying Deep Neural Networks (DNNs), developers often convert models
from one deep learning framework to another (e.g., TensorFlow to PyTorch).
However, this process is error-prone and can impact target model accuracy. To
identify the extent of such impact, we perform and briefly present a
differential analysis against three DNNs widely used for image recognition
(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep
learning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which
revealed numerous model crashes and output label discrepancies of up to 72%. To
mitigate such errors, we present a novel approach towards fault localization
and repair of buggy deep learning framework conversions, focusing on
pre-trained image recognition models. Our technique consists of four stages of
analysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,
and 4) graph representation. In addition, we propose various strategies towards
fault repair of the faults detected. We implement our technique on top of the
Apache TVM deep learning compiler, and we test it by conducting a preliminary
fault localization analysis for the conversion of InceptionV3 from TF to
TFLite. Our approach detected a fault in a common DNN converter tool, which
introduced precision errors in weights, reducing model accuracy. After our
fault localization, we repaired the issue, reducing our conversion error to
zero.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Nonrigid Object Contact Estimation With Regional Unwrapping Transformer <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring contact patterns between hands and nonrigid objects is a common
concern in the vision and robotics community. However, existing learning-based
methods focus more on contact with rigid ones from monocular images. When
adopting them for nonrigid contact, a major problem is that the existing
contact representation is restricted by the geometry of the object.
Consequently, contact neighborhoods are stored in an unordered manner and
contact features are difficult to align with image cues. At the core of our
approach lies a novel hand-object contact representation called RUPs (Region
Unwrapping Profiles), which unwrap the roughly estimated hand-object surfaces
as multiple high-resolution 2D regional profiles. The region grouping strategy
is consistent with the hand kinematic bone division because they are the
primitive initiators for a composite contact pattern. Based on this
representation, our Regional Unwrapping Transformer (RUFormer) learns the
correlation priors across regions from monocular inputs and predicts
corresponding contact and deformed transformations. Our experiments demonstrate
that the proposed framework can robustly estimate the deformed degrees and
deformed transformations, which makes it suitable for both nonrigid and rigid
contact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> How Good is Google Bard's Visual Understanding? An Empirical Study on
  Open Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in
the field of conversational AI. Notably, Bard has recently been updated to
handle visual inputs alongside text prompts during conversations. Given Bard's
impressive track record in handling textual inputs, we explore its capabilities
in understanding and interpreting visual data (images) conditioned by text
questions. This exploration holds the potential to unveil new insights and
challenges for Bard and other forthcoming multi-modal Generative models,
especially in addressing complex computer vision problems that demand accurate
visual and language understanding. Specifically, in this study, we focus on 15
diverse task scenarios encompassing regular, camouflaged, medical, under-water
and remote sensing data to comprehensively evaluate Bard's performance. Our
primary finding indicates that Bard still struggles in these vision scenarios,
highlighting the significant gap in vision-based understanding that needs to be
bridged in future developments. We expect that this empirical study will prove
valuable in advancing future models, leading to enhanced capabilities in
comprehending and interpreting fine-grained visual data. Our project is
released on https://github.com/htqin/GoogleBard-VisUnderstand
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor
  Formula for Image Dehazing <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Transformer networks are beginning to replace pure
convolutional neural networks (CNNs) in the field of computer vision due to
their global receptive field and adaptability to input. However, the quadratic
computational complexity of softmax-attention limits the wide application in
image dehazing task, especially for high-resolution images. To address this
issue, we propose a new Transformer variant, which applies the Taylor expansion
to approximate the softmax-attention and achieves linear computational
complexity. A multi-scale attention refinement module is proposed as a
complement to correct the error of the Taylor expansion. Furthermore, we
introduce a multi-branch architecture with multi-scale patch embedding to the
proposed Transformer, which embeds features by overlapping deformable
convolution of different scales. The design of multi-scale patch embedding is
based on three key ideas: 1) various sizes of the receptive field; 2)
multi-level semantic information; 3) flexible shapes of the receptive field.
Our model, named Multi-branch Transformer expanded by Taylor formula
(MB-TaylorFormer), can embed coarse to fine features more flexibly at the patch
embedding stage and capture long-distance pixel interactions with limited
computational cost. Experimental results on several dehazing benchmarks show
that MB-TaylorFormer achieves state-of-the-art (SOTA) performance with a light
computational burden. The source code and pre-trained models are available at
https://github.com/FVL2020/ICCV-2023-MB-TaylorFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SegViTv2: Exploring Efficient and Continual Semantic Segmentation with
  Plain Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the capability of plain Vision Transformers (ViTs)
for semantic segmentation using the encoder-decoder framework and introduces
\textbf{SegViTv2}. In this study, we introduce a novel Attention-to-Mask (\atm)
module to design a lightweight decoder effective for plain ViT. The proposed
ATM converts the global attention map into semantic masks for high-quality
segmentation results. Our decoder outperforms the popular decoder UPerNet using
various ViT backbones while consuming only about $5\%$ of the computational
cost. For the encoder, we address the concern of the relatively high
computational cost in the ViT-based encoders and propose a \emph{Shrunk++}
structure that incorporates edge-aware query-based down-sampling (EQD) and
query-based upsampling (QU) modules. The Shrunk++ structure reduces the
computational cost of the encoder by up to $50\%$ while maintaining competitive
performance. Furthermore, we propose to adapt SegViT for continual semantic
segmentation, demonstrating nearly zero forgetting of previously learned
knowledge. Experiments show that our proposed SegViTv2 surpasses recent
segmentation methods on three popular benchmarks including ADE20k,
COCO-Stuff-10k and PASCAL-Context datasets. The code is available through the
following link: \url{https://github.com/zbwxp/SegVit}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCV 2023 accepted, 21 pages, 8 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> BinaryViT: Towards Efficient and Accurate Binary Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14730v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14730v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have emerged as the fundamental architecture for
most computer vision fields, but the considerable memory and computation costs
hinders their application on resource-limited devices. As one of the most
powerful compression methods, binarization reduces the computation of the
neural network by quantizing the weights and activation values as $\pm$1.
Although existing binarization methods have demonstrated excellent performance
on Convolutional Neural Networks (CNNs), the full binarization of ViTs is still
under-studied and suffering a significant performance drop. In this paper, we
first argue empirically that the severe performance degradation is mainly
caused by the weight oscillation in the binarization training and the
information distortion in the activation of ViTs. Based on these analyses, we
propose $\textbf{BinaryViT}$, an accurate full binarization scheme for ViTs,
which pushes the quantization of ViTs to the limit. Specifically, we propose a
novel gradient regularization scheme (GRS) for driving a bimodal distribution
of the weights to reduce oscillation in binarization training. Moreover, we
design an activation shift module (ASM) to adaptively tune the activation
distribution to reduce the information distortion caused by binarization.
Extensive experiments on ImageNet dataset show that our BinaryViT consistently
surpasses the strong baseline by 2.05% and improve the accuracy of fully
binarized ViTs to a usable level. Furthermore, our method achieves impressive
savings of 16.2$\times$ and 17.7$\times$ in model size and OPs compared to the
full-precision DeiT-S.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Evaluating the Quality and Diversity of DC<span class="highlight-title">GAN</span>-based <span class="highlight-title">Generative</span>ly
  Synthesized Diabetic Retinopathy Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05593v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05593v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>'<span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>y<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publicly available diabetic retinopathy (DR) datasets are imbalanced,
containing limited numbers of images with DR. This imbalance contributes to
overfitting when training machine learning classifiers. The impact of this
imbalance is exacerbated as the severity of the DR stage increases, affecting
the classifiers' diagnostic capacity. The imbalance can be addressed using
Generative Adversarial Networks (GANs) to augment the datasets with synthetic
images. Generating synthetic images is advantageous if high-quality and
diversified images are produced. To evaluate the quality and diversity of
synthetic images, several evaluation metrics, such as Multi-Scale Structural
Similarity Index (MS-SSIM), Cosine Distance (CD), and Fr\'echet Inception
Distance (FID) are used. Understanding the effectiveness of each metric in
evaluating the quality and diversity of GAN-based synthetic images is critical
to select images for augmentation. To date, there has been limited analysis of
the appropriateness of these metrics in the context of biomedical imagery. This
work contributes an empirical assessment of these evaluation metrics as applied
to synthetic Proliferative DR imagery generated by a Deep Convolutional GAN
(DCGAN). Furthermore, the metrics' capacity to indicate the quality and
diversity of synthetic images and a correlation with classifier performance is
undertaken. This enables a quantitative selection of synthetic imagery and an
informed augmentation strategy. Results indicate that FID is suitable for
evaluating the quality, while MS-SSIM and CD are suitable for evaluating the
diversity of synthetic imagery. Furthermore, the superior performance of
Convolutional Neural Network (CNN) and EfficientNet classifiers, as indicated
by the F1 and AUC scores, for the augmented datasets demonstrates the efficacy
of synthetic imagery to augment the imbalanced dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 Pages, 8 Figures, submitted to MEDAL23: Advances in Deep
  Generative Models for Medical Artificial Intelligence (Springer Nature
  series)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Food Classification using Joint Representation of Visual and Textual
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food classification is an important task in health care. In this work, we
propose a multimodal classification framework that uses the modified version of
EfficientNet with the Mish activation function for image classification, and
the traditional BERT transformer-based network is used for text classification.
The proposed network and the other state-of-the-art methods are evaluated on a
large open-source dataset, UPMC Food-101. The experimental results show that
the proposed network outperforms the other methods, a significant difference of
11.57% and 6.34% in accuracy is observed for image and text classification,
respectively, when compared with the second-best performing method. We also
compared the performance in terms of accuracy, precision, and recall for text
classification using both machine learning and deep learning-based models. The
comparative analysis from the prediction results of both images and text
demonstrated the efficiency and robustness of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated results and discussions to be posted and some sections needed
  to be expanded</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> NBV-SC: Next Best View Planning based on Shape Completion for Fruit
  Mapping and Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15376v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15376v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active perception for fruit mapping and harvesting is a difficult task since
occlusions occur frequently and the location as well as size of fruits change
over time. State-of-the-art viewpoint planning approaches utilize
computationally expensive ray casting operations to find good viewpoints aiming
at maximizing information gain and covering the fruits in the scene. In this
paper, we present a novel viewpoint planning approach that explicitly uses
information about the predicted fruit shapes to compute targeted viewpoints
that observe as yet unobserved parts of the fruits. Furthermore, we formulate
the concept of viewpoint dissimilarity to reduce the sampling space for more
efficient selection of useful, dissimilar viewpoints. Our simulation
experiments with a UR5e arm equipped with an RGB-D sensor provide a
quantitative demonstration of the efficacy of our iterative next best view
planning method based on shape completion. In comparative experiments with a
state-of-the-art viewpoint planner, we demonstrate improvement not only in the
estimation of the fruit sizes, but also in their reconstruction, while
significantly reducing the planning time. Finally, we show the viability of our
approach for mapping sweet peppers plants with a real robotic system in a
commercial glasshouse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Agricultural Automation, Viewpoint Planning, Active Perception, Shape
  Completion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Three-stage binarization of color document images based on discrete
  wavelet transform and <span class="highlight-title">generative</span> adversarial networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16098v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16098v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficient segmentation of foreground text information from the background
in degraded color document images is a critical challenge in the preservation
of ancient manuscripts. The imperfect preservation of ancient manuscripts over
time has led to various types of degradation, such as staining, yellowing, and
ink seepage, significantly affecting image binarization results. This work
proposes a three-stage method using Generative Adversarial Networks (GAN) for
enhancing and binarizing degraded color document images through Discrete
Wavelet Transform (DWT). Stage-1 involves applying DWT and retaining the
Low-Low (LL) subband images for image enhancement. In Stage-2, the original
input image is divided into four single-channel images (Red, Green, Blue, and
Gray), and each is trained with independent adversarial networks to extract
color foreground information. In Stage-3, the output image from Stage-2 and the
original input image are used to train independent adversarial networks for
document binarization, enabling the integration of global and local features.
The experimental results demonstrate that our proposed method outperforms other
classic and state-of-the-art (SOTA) methods on the Document Image Binarization
Contest (DIBCO) datasets. We have released our implementation code at
https://github.com/abcpp12383/ThreeStageBinarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Implicit neural representation for change detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying changes in a pair of 3D aerial LiDAR point clouds, obtained
during two distinct time periods over the same geographic region presents a
significant challenge due to the disparities in spatial coverage and the
presence of noise in the acquisition system. The most commonly used approaches
to detecting changes in point clouds are based on supervised methods which
necessitate extensive labelled data often unavailable in real-world
applications. To address these issues, we propose an unsupervised approach that
comprises two components: Implicit Neural Representation (INR) for continuous
shape reconstruction and a Gaussian Mixture Model for categorising changes. INR
offers a grid-agnostic representation for encoding bi-temporal point clouds,
with unmatched spatial support that can be regularised to enhance
high-frequency details and reduce noise. The reconstructions at each timestamp
are compared at arbitrary spatial scales, leading to a significant increase in
detection capabilities. We apply our method to a benchmark dataset comprising
simulated LiDAR point clouds for urban sprawling. This dataset encompasses
diverse challenging scenarios, varying in resolutions, input modalities and
noise levels. This enables a comprehensive multi-scenario evaluation, comparing
our method with the current state-of-the-art approach. We outperform the
previous methods by a margin of 10% in the intersection over union metric. In
addition, we put our techniques to practical use by applying them in a
real-world scenario to identify instances of illicit excavation of
archaeological sites and validate our results by comparing them with findings
from field experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main article is 10 pages + 6 pages of supplementary. Conference style
  paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> What do neural networks learn in image classification? A frequency
  shortcut perspective <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frequency analysis is useful for understanding the mechanisms of
representation learning in neural networks (NNs). Most research in this area
focuses on the learning dynamics of NNs for regression tasks, while little for
classification. This study empirically investigates the latter and expands the
understanding of frequency shortcuts. First, we perform experiments on
synthetic datasets, designed to have a bias in different frequency bands. Our
results demonstrate that NNs tend to find simple solutions for classification,
and what they learn first during training depends on the most distinctive
frequency characteristics, which can be either low- or high-frequencies.
Second, we confirm this phenomenon on natural images. We propose a metric to
measure class-wise frequency characteristics and a method to identify frequency
shortcuts. The results show that frequency shortcuts can be texture-based or
shape-based, depending on what best simplifies the objective. Third, we
validate the transferability of frequency shortcuts on out-of-distribution
(OOD) test sets. Our results suggest that frequency shortcuts can be
transferred across datasets and cannot be fully avoided by larger model
capacity and data augmentation. We recommend that future research should focus
on effective training schemes mitigating frequency shortcut learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Poincaré ResNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>x<span class="highlight-author"></span> <span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>r<span class="highlight-author"></span>w<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an end-to-end residual network that operates entirely
on the Poincar\'e ball model of hyperbolic space. Hyperbolic learning has
recently shown great potential for visual understanding, but is currently only
performed in the penultimate layer(s) of deep networks. All visual
representations are still learned through standard Euclidean networks. In this
paper we investigate how to learn hyperbolic representations of visual data
directly from the pixel-level. We propose Poincar\'e ResNet, a hyperbolic
counterpart of the celebrated residual network, starting from Poincar\'e 2D
convolutions up to Poincar\'e residual connections. We identify three
roadblocks for training convolutional networks entirely in hyperbolic space and
propose a solution for each: (i) Current hyperbolic network initializations
collapse to the origin, limiting their applicability in deeper networks. We
provide an identity-based initialization that preserves norms over many layers.
(ii) Residual networks rely heavily on batch normalization, which comes with
expensive Fr\'echet mean calculations in hyperbolic space. We introduce
Poincar\'e midpoint batch normalization as a faster and equally effective
alternative. (iii) Due to the many intermediate operations in Poincar\'e
layers, we lastly find that the computation graphs of deep learning libraries
blow up, limiting our ability to train on deep hyperbolic networks. We provide
manual backward derivations of core hyperbolic operations to maintain
manageable computation graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Computer Vision 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Dynamic Depth-Supervised <span class="highlight-title">NeRF</span> for Multi-View RGB-D Operating Room Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>v<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The operating room (OR) is an environment of interest for the development of
sensing systems, enabling the detection of people, objects, and their semantic
relations. Due to frequent occlusions in the OR, these systems often rely on
input from multiple cameras. While increasing the number of cameras generally
increases algorithm performance, there are hard limitations to the number and
locations of cameras in the OR. Neural Radiance Fields (NeRF) can be used to
render synthetic views from arbitrary camera positions, virtually enlarging the
number of cameras in the dataset. In this work, we explore the use of NeRF for
view synthesis of dynamic scenes in the OR, and we show that regularisation
with depth supervision from RGB-D sensor data results in higher image quality.
We optimise a dynamic depth-supervised NeRF with up to six synchronised cameras
that capture the surgical field in five distinct phases before and during a
knee replacement surgery. We qualitatively inspect views rendered by a virtual
camera that moves 180 degrees around the surgical field at differing time
values. Quantitatively, we evaluate view synthesis from an unseen camera
position in terms of PSNR, SSIM and LPIPS for the colour channels and in MAE
and error percentage for the estimated depth. We find that NeRFs can be used to
generate geometrically consistent views, also from interpolated camera
positions and at interpolated time intervals. Views are generated from an
unseen camera pose with an average PSNR of 18.2 and a depth estimation error of
2.0%. Our results show the potential of a dynamic NeRF for view synthesis in
the OR and stress the relevance of depth supervision in a clinical setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Workshop on Ambient Intelligence for HealthCare 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> NSF: Neural Surface Fields for Human Modeling from Monocular Depth <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14847v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14847v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>-<span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining personalized 3D animatable avatars from a monocular camera has
several real world applications in gaming, virtual try-on, animation, and
VR/XR, etc. However, it is very challenging to model dynamic and fine-grained
clothing deformations from such sparse data. Existing methods for modeling 3D
humans from depth data have limitations in terms of computational efficiency,
mesh coherency, and flexibility in resolution and topology. For instance,
reconstructing shapes using implicit functions and extracting explicit meshes
per frame is computationally expensive and cannot ensure coherent meshes across
frames. Moreover, predicting per-vertex deformations on a pre-designed human
template with a discrete surface lacks flexibility in resolution and topology.
To overcome these limitations, we propose a novel method `\keyfeature: Neural
Surface Fields' for modeling 3D clothed humans from monocular depth. NSF
defines a neural field solely on the base surface which models a continuous and
flexible displacement field. NSF can be adapted to the base surface with
different resolution and topology without retraining at inference time.
Compared to existing approaches, our method eliminates the expensive per-frame
surface extraction while maintaining mesh coherency, and is capable of
reconstructing meshes with arbitrary resolution without retraining. To foster
research in this direction, we release our code in project page at:
https://yuxuan-xue.com/nsf.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpted to ICCV 2023; Homepage at: https://yuxuan-xue.com/nsf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Collaborative Perception in Autonomous Driving: Methods, Datasets and
  Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06262v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06262v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative perception is essential to address occlusion and sensor failure
issues in autonomous driving. In recent years, theoretical and experimental
investigations of novel works for collaborative perception have increased
tremendously. So far, however, few reviews have focused on systematical
collaboration modules and large-scale collaborative perception datasets. This
work reviews recent achievements in this field to bridge this gap and motivate
future research. We start with a brief overview of collaboration schemes. After
that, we systematically summarize the collaborative perception methods for
ideal scenarios and real-world issues. The former focuses on collaboration
modules and efficiency, and the latter is devoted to addressing the problems in
actual application. Furthermore, we present large-scale public datasets and
summarize quantitative results on these benchmarks. Finally, we highlight gaps
and overlook challenges between current academic research and real-world
applications. The project page is
https://github.com/CatOneTwo/Collaborative-Perception-in-Autonomous-Driving
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures. Accepted by IEEE Intelligent Transportation
  Systems Magazine. URL:
  https://github.com/CatOneTwo/Collaborative-Perception-in-Autonomous-Driving</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Elucidating the Exposure Bias in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>u<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have demonstrated impressive generative capabilities, but
their 'exposure bias' problem, described as the input mismatch between training
and sampling, lacks in-depth exploration. In this paper, we systematically
investigate the exposure bias problem in diffusion models by first analytically
modelling the sampling distribution, based on which we then attribute the
prediction error at each sampling step as the root cause of the exposure bias
issue. Furthermore, we discuss potential solutions to this issue and propose an
intuitive metric for it. Along with the elucidation of exposure bias, we
propose a simple, yet effective, training-free method called Epsilon Scaling to
alleviate the exposure bias. We show that Epsilon Scaling explicitly moves the
sampling trajectory closer to the vector field learned in the training phase by
scaling down the network output (Epsilon), mitigating the input mismatch
between training and sampling. Experiments on various diffusion frameworks
(ADM, DDPM/DDIM, LDM), unconditional and conditional settings, and
deterministic vs. stochastic sampling verify the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, code available soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Case-Aware Adversarial Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.09398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.09398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The neural network (NN) becomes one of the most heated type of models in
various signal processing applications. However, NNs are extremely vulnerable
to adversarial examples (AEs). To defend AEs, adversarial training (AT) is
believed to be the most effective method while due to the intensive
computation, AT is limited to be applied in most applications. In this paper,
to resolve the problem, we design a generic and efficient AT improvement
scheme, namely case-aware adversarial training (CAT). Specifically, the
intuition stems from the fact that a very limited part of informative samples
can contribute to most of model performance. Alternatively, if only the most
informative AEs are used in AT, we can lower the computation complexity of AT
significantly as maintaining the defense effect. To achieve this, CAT achieves
two breakthroughs. First, a method to estimate the information degree of
adversarial examples is proposed for AE filtering. Second, to further enrich
the information that the NN can obtain from AEs, CAT involves a weight
estimation and class-level balancing based sampling strategy to increase the
diversity of AT at each iteration. Extensive experiments show that CAT is
faster than vanilla AT by up to 3x while achieving competitive defense effect.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Tranfer Learning of Semantic Segmentation Methods for Identifying Buried
  Archaeological Structures on LiDAR Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>G<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>-<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>Ž<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>j<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When applying deep learning to remote sensing data in archaeological
research, a notable obstacle is the limited availability of suitable datasets
for training models. The application of transfer learning is frequently
employed to mitigate this drawback. However, there is still a need to explore
its effectiveness when applied across different archaeological datasets. This
paper compares the performance of various transfer learning configurations
using two semantic segmentation deep neural networks on two LiDAR datasets. The
experimental results indicate that transfer learning-based approaches in
archaeology can lead to performance improvements, although a systematic
enhancement has not yet been observed. We provide specific insights about the
validity of such techniques that can serve as a baseline for future works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Geoscience and Remote Sensing
  Symposium 2023 (IGARSS 2023) @IEEE copyright</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Is Complexity Required for Neural Network Pruning? A Case Study on
  Global Magnitude Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14624v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14624v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>b<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>w<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning neural networks has become popular in the last decade when it was
shown that a large number of weights can be safely removed from modern neural
networks without compromising accuracy. Numerous pruning methods have been
proposed since then, each claiming to be better than the previous. Many
state-of-the-art (SOTA) techniques today rely on complex pruning methodologies
utilizing importance scores, getting feedback through back-propagation or
having heuristics-based pruning rules amongst others. In this work, we question
whether this pattern of introducing complexity is really necessary to achieve
better pruning results. We benchmark these SOTA techniques against a naive
pruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks
weights in order of their magnitudes and prunes the smallest ones. Hence, in
its vanilla form, it is one of the simplest pruning techniques. Surprisingly,
we find that vanilla Global MP outperforms all the other SOTA techniques and
achieves a new SOTA result. It also achieves promising performance on FLOPs
sparsification, which we find is enhanced, when pruning is conducted in a
gradual fashion. We also find that Global MP is generalizable across tasks,
datasets, and models with superior performance. Moreover, a common issue that
many pruning algorithms run into at high sparsity rates, namely,
layer-collapse, can be easily fixed in Global MP by setting a minimum threshold
of weights to be retained in each layer. Lastly, unlike many other SOTA
techniques, Global MP does not require any additional algorithm specific
hyper-parameters and is very straightforward to tune and implement. We showcase
our findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1
and FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is
available at https://github.com/manasgupta-1/GlobalMP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> HHTrack: Hyperspectral Object Tracking Using Hybrid Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral imagery provides abundant spectral information beyond the
visible RGB bands, offering rich discriminative details about objects in a
scene. Leveraging such data has the potential to enhance visual tracking
performance. In this paper, we propose a hyperspectral object tracker based on
hybrid attention (HHTrack). The core of HHTrack is a hyperspectral hybrid
attention (HHA) module that unifies feature extraction and fusion within one
component through token interactions. A hyperspectral bands fusion (HBF) module
is also introduced to selectively aggregate spatial and spectral signatures
from the full hyperspectral input. Extensive experiments demonstrate the
state-of-the-art performance of HHTrack on benchmark Near Infrared (NIR), Red
Near Infrared (Red-NIR), and Visible (VIS) hyperspectral tracking datasets. Our
work provides new insights into harnessing the strengths of transformers and
hyperspectral fusion to advance robust object tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Semi-supervised Semantic Segmentation with Mutual Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11499v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11499v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consistency regularization has been widely studied in recent semisupervised
semantic segmentation methods, and promising performance has been achieved. In
this work, we propose a new consistency regularization framework, termed mutual
knowledge distillation (MKD), combined with data and feature augmentation. We
introduce two auxiliary mean-teacher models based on consistency
regularization. More specifically, we use the pseudo-labels generated by a mean
teacher to supervise the student network to achieve a mutual knowledge
distillation between the two branches. In addition to using image-level strong
and weak augmentation, we also discuss feature augmentation. This involves
considering various sources of knowledge to distill the student network. Thus,
we can significantly increase the diversity of the training samples.
Experiments on public benchmarks show that our framework outperforms previous
state-of-the-art (SOTA) methods under various semi-supervised settings. Code is
available at semi-mmseg.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Efficient Adaptive Ensembling for Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.07394v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.07394v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, with the exception of sporadic cases, the trend in Computer
Vision is to achieve minor improvements compared to considerable increases in
complexity.
  To reverse this trend, we propose a novel method to boost image
classification performances without increasing complexity.
  To this end, we revisited ensembling, a powerful approach, often not used
properly due to its more complex nature and the training time, so as to make it
feasible through a specific design choice. First, we trained two
EfficientNet-b0 end-to-end models (known to be the architecture with the best
overall accuracy/complexity trade-off for image classification) on disjoint
subsets of data (i.e. bagging). Then, we made an efficient adaptive ensemble by
performing fine-tuning of a trainable combination layer. In this way, we were
able to outperform the state-of-the-art by an average of 0.5$\%$ on the
accuracy, with restrained complexity both in terms of the number of parameters
(by 5-60 times), and the FLoating point Operations Per Second (FLOPS) by 10-100
times on several major benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Scene Matters: Model-based Deep Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>L<span class="highlight-author"></span>v<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video compression has always been a popular research area, where many
traditional and deep video compression methods have been proposed. These
methods typically rely on signal prediction theory to enhance compression
performance by designing high efficient intra and inter prediction strategies
and compressing video frames one by one. In this paper, we propose a novel
model-based video compression (MVC) framework that regards scenes as the
fundamental units for video sequences. Our proposed MVC directly models the
intensity variation of the entire video sequence in one scene, seeking
non-redundant representations instead of reducing redundancy through
spatio-temporal predictions. To achieve this, we employ implicit neural
representation as our basic modeling architecture. To improve the efficiency of
video modeling, we first propose context-related spatial positional embedding
and frequency domain supervision in spatial context enhancement. For temporal
correlation capturing, we design the scene flow constrain mechanism and
temporal contrastive loss. Extensive experimental results demonstrate that our
method achieves up to a 20\% bitrate reduction compared to the latest video
coding standard H.266 and is more efficient in decoding than existing video
coding strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend
  <span class="highlight-title">3D</span> Talking Faces <span class="chip">ACM MM 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10799v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10799v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven 3D face animation technique, extending its applications to
various multimedia fields. Previous research has generated promising realistic
lip movements and facial expressions from audio signals. However, traditional
regression models solely driven by data face several essential problems, such
as difficulties in accessing precise labels and domain gaps between different
modalities, leading to unsatisfactory results lacking precision and coherence.
To enhance the visual accuracy of generated lip movement while reducing the
dependence on labeled data, we propose a novel framework SelfTalk, by involving
self-supervision in a cross-modals network system to learn 3D talking faces.
The framework constructs a network system consisting of three modules: facial
animator, speech recognizer, and lip-reading interpreter. The core of SelfTalk
is a commutative training diagram that facilitates compatible features exchange
among audio, text, and lip shape, enabling our models to learn the intricate
connection between these factors. The proposed framework leverages the
knowledge learned from the lip-reading interpreter to generate more plausible
lip shapes. Extensive experiments and user studies demonstrate that our
proposed approach achieves state-of-the-art performance both qualitatively and
quantitatively. We recommend watching the supplementary video.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Estimating <span class="highlight-title">3D</span> Dental Structures using Simulated Panoramic Radiographs
  and Neural Ray Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04027v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04027v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic radiography (Panoramic X-ray, PX) is a widely used imaging modality
for dental examination. However, PX only provides a flattened 2D image, lacking
in a 3D view of the oral structure. In this paper, we propose a framework to
estimate 3D oral structures from real-world PX. Our framework tackles full 3D
reconstruction for varying subjects (patients) where each reconstruction is
based only on a single panoramic image. We create an intermediate
representation called simulated PX (SimPX) from 3D Cone-beam computed
tomography (CBCT) data based on the Beer-Lambert law of X-ray rendering and
rotational principles of PX imaging. SimPX aims at not only truthfully
simulating PX, but also facilitates the reverting process back to 3D data. We
propose a novel neural model based on ray tracing which exploits both global
and local input features to convert SimPX to 3D output. At inference, a real PX
image is translated to a SimPX-style image with semantic regularization, and
the translated image is processed by generation module to produce high-quality
outputs. Experiments show that our method outperforms prior state-of-the-art in
reconstruction tasks both quantitatively and qualitatively. Unlike prior
methods, Our method does not require any prior information such as the shape of
dental arches, nor the matched PX-CBCT dataset for training, which is difficult
to obtain in clinical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Pre-trained transformer for adversarial purification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01762v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01762v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With more and more deep neural networks being deployed as various daily
services, their reliability is essential. It's frightening that deep neural
networks are vulnerable and sensitive to adversarial attacks, the most common
one of which for the services is evasion-based. Recent works usually strengthen
the robustness by adversarial training or leveraging the knowledge of an amount
of clean data. However, in practical terms, retraining and redeploying the
model need a large computational budget, leading to heavy losses to the online
service. In addition, when adversarial examples of a certain attack are
detected, only limited adversarial examples are available for the service
provider, while much clean data may not be accessible. Given the mentioned
problems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is
to rapidly defend against a certain attack for the frozen original service
model with limitations of few clean and adversarial examples. Motivated by the
generalization and the universal computation ability of pre-trained transformer
models, we come up with a new defender method, CeTaD, which stands for
Considering Pre-trained Transformers as Defenders. In particular, we evaluate
the effectiveness and the transferability of CeTaD in the case of one-shot
adversarial examples and explore the impact of different parts of CeTaD as well
as training data conditions. CeTaD is flexible, able to be embedded into an
arbitrary differentiable model, and suitable for various types of attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Human Motion Diffusion as a <span class="highlight-title">Generative</span> Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has demonstrated the significant potential of denoising diffusion
models for generating human motion, including text-to-motion capabilities.
However, these methods are restricted by the paucity of annotated motion data,
a focus on single-person motions, and a lack of detailed control. In this
paper, we introduce three forms of composition based on diffusion priors:
sequential, parallel, and model composition. Using sequential composition, we
tackle the challenge of long sequence generation. We introduce DoubleTake, an
inference-time method with which we generate long animations consisting of
sequences of prompted intervals and their transitions, using a prior trained
only for short clips. Using parallel composition, we show promising steps
toward two-person generation. Beginning with two fixed priors as well as a few
two-person training examples, we learn a slim communication block, ComMDM, to
coordinate interaction between the two resulting motions. Lastly, using model
composition, we first train individual priors to complete motions that realize
a prescribed motion for a given joint. We then introduce DiffusionBlending, an
interpolation mechanism to effectively blend several such models to enable
flexible and efficient fine-grained joint and trajectory-level control and
editing. We evaluate the composition methods using an off-the-shelf motion
diffusion model, and further compare the results to dedicated models trained
for these specific tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> FineDance: A Fine-grained Choreography Dataset for <span class="highlight-title">3D</span> Full Body Dance
  Generation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03741v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03741v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating full-body and multi-genre dance sequences from given music is a
challenging task, due to the limitations of existing datasets and the inherent
complexity of the fine-grained hand motion and dance genres. To address these
problems, we propose FineDance, which contains 14.6 hours of music-dance paired
data, with fine-grained hand motions, fine-grained genres (22 dance genres),
and accurate posture. To the best of our knowledge, FineDance is the largest
music-dance paired dataset with the most dance genres. Additionally, to address
monotonous and unnatural hand movements existing in previous methods, we
propose a full-body dance generation network, which utilizes the diverse
generation capabilities of the diffusion model to solve monotonous problems,
and use expert nets to solve unreal problems. To further enhance the
genre-matching and long-term stability of generated dances, we propose a
Genre&Coherent aware Retrieval Module. Besides, we propose a novel metric named
Genre Matching Score to evaluate the genre-matching degree between dance and
music. Quantitative and qualitative experiments demonstrate the quality of
FineDance, and the state-of-the-art performance of FineNet. The FineDance
Dataset and more qualitative samples can be found at our website.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> iWarp<span class="highlight-title">GAN</span>: Disentangling Identity and Style to Generate Synthetic Iris
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have shown success in approximating
complex distributions for synthetic image generation. However, current
GAN-based methods for generating biometric images, such as iris, have certain
limitations: (a) the synthetic images often closely resemble images in the
training dataset; (b) the generated images lack diversity in terms of the
number of unique identities represented in them; and (c) it is difficult to
generate multiple images pertaining to the same identity. To overcome these
issues, we propose iWarpGAN that disentangles identity and style in the context
of the iris modality by using two transformation pathways: Identity
Transformation Pathway to generate unique identities from the training set, and
Style Transformation Pathway to extract the style code from a reference image
and output an iris image using this style. By concatenating the transformed
identity code and reference style code, iWarpGAN generates iris images with
both inter- and intra-class variations. The efficacy of the proposed method in
generating such iris DeepFakes is evaluated both qualitatively and
quantitatively using ISO/IEC 29794-6 Standard Quality Metrics and the VeriEye
iris matcher. Further, the utility of the synthetically generated images is
demonstrated by improving the performance of deep learning based iris matchers
that augment synthetic data with real data during the training process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> EvHandPose: Event-based <span class="highlight-title">3D</span> Hand Pose Estimation with Sparse Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event camera shows great potential in 3D hand pose estimation, especially
addressing the challenges of fast motion and high dynamic range in a low-power
way. However, due to the asynchronous differential imaging mechanism, it is
challenging to design event representation to encode hand motion information
especially when the hands are not moving (causing motion ambiguity), and it is
infeasible to fully annotate the temporally dense event stream. In this paper,
we propose EvHandPose with novel hand flow representations in Event-to-Pose
module for accurate hand pose estimation and alleviating the motion ambiguity
issue. To solve the problem under sparse annotation, we design contrast
maximization and hand-edge constraints in Pose-to-IWE (Image with Warped
Events) module and formulate EvHandPose in a weakly-supervision framework. We
further build EvRealHands, the first large-scale real-world event-based hand
pose dataset on several challenging scenes to bridge the real-synthetic domain
gap. Experiments on EvRealHands demonstrate that EvHandPose outperforms
previous event-based methods under all evaluation scenes, achieves accurate and
stable hand pose estimation with high temporal resolution in fast motion and
strong light scenes compared with RGB-based methods, generalizes well to
outdoor scenes and another type of event camera, and shows the potential for
the hand gesture recognition task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant
  Analysis <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07543v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07543v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are susceptible to generating overconfident yet
erroneous predictions when presented with data beyond known concepts. This
challenge underscores the importance of detecting out-of-distribution (OOD)
samples in the open world. In this work, we propose a novel feature-space OOD
detection score based on class-specific and class-agnostic information.
Specifically, the approach utilizes Whitened Linear Discriminant Analysis to
project features into two subspaces - the discriminative and residual subspaces
- for which the in-distribution (ID) classes are maximally separated and
closely clustered, respectively. The OOD score is then determined by combining
the deviation from the input data to the ID pattern in both subspaces. The
efficacy of our method, named WDiscOOD, is verified on the large-scale
ImageNet-1k benchmark, with six OOD datasets that cover a variety of
distribution shifts. WDiscOOD demonstrates superior performance on deep
classifiers with diverse backbone architectures, including CNN and vision
transformer. Furthermore, we also show that WDiscOOD more effectively detects
novel concepts in representation spaces trained with contrastive objectives,
including supervised contrastive loss and multi-modality contrastive loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023. Code is available at:
  https://github.com/ivalab/WDiscOOD.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Generalized Universal Domain Adaptation with <span class="highlight-title">Generative</span> Flow Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new problem in unsupervised domain adaptation, termed as
Generalized Universal Domain Adaptation (GUDA), which aims to achieve precise
prediction of all target labels including unknown categories. GUDA bridges the
gap between label distribution shift-based and label space mismatch-based
variants, essentially categorizing them as a unified problem, guiding to a
comprehensive framework for thoroughly solving all the variants. The key
challenge of GUDA is developing and identifying novel target categories while
estimating the target label distribution. To address this problem, we take
advantage of the powerful exploration capability of generative flow networks
and propose an active domain adaptation algorithm named GFlowDA, which selects
diverse samples with probabilities proportional to a reward function. To
enhance the exploration capability and effectively perceive the target label
distribution, we tailor the states and rewards, and introduce an efficient
solution for parent exploration and state transition. We also propose a
training paradigm for GUDA called Generalized Universal Adversarial Network
(GUAN), which involves collaborative optimization between GUAN and GFlowNet.
Theoretical analysis highlights the importance of exploration, and extensive
experiments on benchmark datasets demonstrate the superiority of GFlowDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Universal Domain Adaptation via Compressive Attention Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11862v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11862v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal domain adaptation (UniDA) aims to transfer knowledge from the
source domain to the target domain without any prior knowledge about the label
set. The challenge lies in how to determine whether the target samples belong
to common categories. The mainstream methods make judgments based on the sample
features, which overemphasizes global information while ignoring the most
crucial local objects in the image, resulting in limited accuracy. To address
this issue, we propose a Universal Attention Matching (UniAM) framework by
exploiting the self-attention mechanism in vision transformer to capture the
crucial object information. The proposed framework introduces a novel
Compressive Attention Matching (CAM) approach to explore the core information
by compressively representing attentions. Furthermore, CAM incorporates a
residual-based measurement to determine the sample commonness. By utilizing the
measurement, UniAM achieves domain-wise and category-wise Common Feature
Alignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the first
method utilizing the attention in vision transformer directly to perform
classification tasks. Extensive experiments show that UniAM outperforms the
current state-of-the-art methods on various benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Exploring the Relationship between Samples and Masks for Robust Defect
  Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10720v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10720v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defect detection aims to detect and localize regions out of the normal
distribution.Previous approaches model normality and compare it with the input
to identify defective regions, potentially limiting their generalizability.This
paper proposes a one-stage framework that detects defective patterns directly
without the modeling process.This ability is adopted through the joint efforts
of three parties: a generative adversarial network (GAN), a newly proposed
scaled pattern loss, and a dynamic masked cycle-consistent auxiliary network.
Explicit information that could indicate the position of defects is
intentionally excluded to avoid learning any direct mapping.Experimental
results on the texture class of the challenging MVTec AD dataset show that the
proposed method is 2.9\% higher than the SOTA methods in F1-Score, while
substantially outperforming SOTA methods in generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> UniM$^2$AE: Multi-modal Masked Autoencoders with Unified <span class="highlight-title">3D</span>
  Representation for <span class="highlight-title">3D</span> Perception in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Autoencoders (MAE) play a pivotal role in learning potent
representations, delivering outstanding results across various 3D perception
tasks essential for autonomous driving. In real-world driving scenarios, it's
commonplace to deploy multiple sensors for comprehensive environment
perception. While integrating multi-modal features from these sensors can
produce rich and powerful features, there is a noticeable gap in MAE methods
addressing this integration. This research delves into multi-modal Masked
Autoencoders tailored for a unified representation space in autonomous driving,
aiming to pioneer a more efficient fusion of two distinct modalities. To
intricately marry the semantics inherent in images with the geometric
intricacies of LiDAR point clouds, the UniM$^2$AE is proposed. This model
stands as a potent yet straightforward, multi-modal self-supervised
pre-training framework, mainly consisting of two designs. First, it projects
the features from both modalities into a cohesive 3D volume space, ingeniously
expanded from the bird's eye view (BEV) to include the height dimension. The
extension makes it possible to back-project the informative features, obtained
by fusing features from both modalities, into their native modalities to
reconstruct the multiple masked inputs. Second, the Multi-modal 3D Interactive
Module (MMIM) is invoked to facilitate the efficient inter-modal interaction
during the interaction process. Extensive experiments conducted on the nuScenes
Dataset attest to the efficacy of UniM$^2$AE, indicating enhancements in 3D
object detection and BEV map segmentation by 1.2\%(NDS) and 6.5\% (mIoU),
respectively. Code is available at https://github.com/hollow-503/UniM2AE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at https://github.com/hollow-503/UniM2AE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> EA-LSS: Edge-aware Lift-splat-shot Framework for <span class="highlight-title">3D</span> BEV Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17895v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17895v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, great progress has been made in the Lift-Splat-Shot-based
(LSS-based) 3D object detection method. However, inaccurate depth estimation
remains an important constraint to the accuracy of camera-only and multi-model
3D object detection models, especially in regions where the depth changes
significantly (i.e., the "depth jump" problem). In this paper, we proposed a
novel Edge-aware Lift-splat-shot (EA-LSS) framework. Specifically, edge-aware
depth fusion (EADF) module is proposed to alleviate the "depth jump" problem
and fine-grained depth (FGD) module to further enforce refined supervision on
depth. Our EA-LSS framework is compatible for any LSS-based 3D object detection
models, and effectively boosts their performances with negligible increment of
inference time. Experiments on nuScenes benchmarks demonstrate that EA-LSS is
effective in either camera-only or multi-model models. It is worth mentioning
that EA-LSS achieved the state-of-the-art performance on nuScenes test
benchmarks with mAP and NDS of 76.5% and 77.6%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Few-Shot Object Detection via Synthetic Features with Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot object detection aims to simultaneously localize and classify the
objects in an image with limited training samples. However, most existing
few-shot object detection methods focus on extracting the features of a few
samples of novel classes that lack diversity. Hence, they may not be sufficient
to capture the data distribution. To address that limitation, in this paper, we
propose a novel approach in which we train a generator to generate synthetic
data for novel classes. Still, directly training a generator on the novel class
is not effective due to the lack of novel data. To overcome that issue, we
leverage the large-scale dataset of base classes. Our overarching goal is to
train a generator that captures the data variations of the base dataset. We
then transform the captured variations into novel classes by generating
synthetic data with the trained generator. To encourage the generator to
capture data variations on base classes, we propose to train the generator with
an optimal transport loss that minimizes the optimal transport distance between
the distributions of real and synthetic data. Extensive experiments on two
benchmark datasets demonstrate that the proposed method outperforms the state
of the art. Source code will be available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> RAFT: Reward rAnked FineTuning for <span class="highlight-title">Generative</span> Foundation Model Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06767v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06767v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative foundation models are susceptible to implicit biases that can
arise from extensive unsupervised training data. Such biases can produce
suboptimal samples, skewed outcomes, and unfairness, with potentially serious
consequences. Consequently, aligning these models with human ethics and
preferences is an essential step toward ensuring their responsible and
effective deployment in real-world applications. Prior research has primarily
employed Reinforcement Learning from Human Feedback (RLHF) to address this
problem, where generative models are fine-tuned with RL algorithms guided by a
human-feedback-informed reward model. However, the inefficiencies and
instabilities associated with RL algorithms frequently present substantial
obstacles to the successful alignment, necessitating the development of a more
robust and streamlined approach. To this end, we introduce a new framework,
Reward rAnked FineTuning (RAFT), designed to align generative models
effectively. Utilizing a reward model and a sufficient number of samples, our
approach selects the high-quality samples, discarding those that exhibit
undesired behavior, and subsequently enhancing the model by fine-tuning on
these filtered samples. Our studies show that RAFT can effectively improve the
model performance in both reward learning and other automated metrics in both
large language models and diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-08-29T00:00:00Z">2023-08-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Task-Parallel Approach for Localized Topological Data Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unstructured meshes are characterized by data points irregularly distributed
in the Euclidian space. Due to the irregular nature of these data, computing
connectivity information between the mesh elements requires much more time and
memory than on uniformly distributed data. To lower storage costs, dynamic data
structures have been proposed. These data structures compute connectivity
information on the fly and discard them when no longer needed. However,
on-the-fly computation slows down algorithms and results in a negative impact
on the time performance. To address this issue, we propose a new task-parallel
approach to proactively compute mesh connectivity. Unlike previous approaches
implementing data-parallel models, where all threads run the same type of
instructions, our task-parallel approach allows threads to run different
functions. Specifically, some threads run the algorithm of choice while other
threads compute connectivity information before they are actually needed. The
approach was implemented in the new Accelerated Clustered TOPOlogical (ACTOPO)
data structure, which can support any processing algorithm requiring mesh
connectivity information. Our experiments show that ACTOPO combines the
benefits of state-of-the-art memory-efficient (TTK CompactTriangulation) and
time-efficient (TTK ExplicitTriangulation) topological data structures. It
occupies a similar amount of memory as TTK CompactTriangulation while providing
up to 5x speedup. Moreover, it achieves comparable time performance as TTK
ExplicitTriangulation while using only half of the memory space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 13 figures, accepted at 2023 IEEE Visualization Conference
  (VIS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Efficient Ray Sampling for Radiance Fields Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating neural radiance fields training is of substantial practical
value, as the ray sampling strategy profoundly impacts network convergence.
More efficient ray sampling can thus directly enhance existing NeRF models'
training efficiency. We therefore propose a novel ray sampling approach for
neural radiance fields that improves training efficiency while retaining
photorealistic rendering results. First, we analyze the relationship between
the pixel loss distribution of sampled rays and rendering quality. This reveals
redundancy in the original NeRF's uniform ray sampling. Guided by this finding,
we develop a sampling method leveraging pixel regions and depth boundaries. Our
main idea is to sample fewer rays in training views, yet with each ray more
informative for scene fitting. Sampling probability increases in pixel areas
exhibiting significant color and depth variation, greatly reducing wasteful
rays from other regions without sacrificing precision. Through this method, not
only can the convergence of the network be accelerated, but the spatial
geometry of a scene can also be perceived more accurately. Rendering outputs
are enhanced, especially for texture-complex regions. Experiments demonstrate
that our method significantly outperforms state-of-the-art techniques on public
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Intrinsic<span class="highlight-title">NeRF</span>: Learning Intrinsic Neural Radiance Fields for Editable
  Novel View Synthesis <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing inverse rendering combined with neural rendering methods can only
perform editable novel view synthesis on object-specific scenes, while we
present intrinsic neural radiance fields, dubbed IntrinsicNeRF, which introduce
intrinsic decomposition into the NeRF-based neural rendering method and can
extend its application to room-scale scenes. Since intrinsic decomposition is a
fundamentally under-constrained inverse problem, we propose a novel
distance-aware point sampling and adaptive reflectance iterative clustering
optimization method, which enables IntrinsicNeRF with traditional intrinsic
decomposition constraints to be trained in an unsupervised manner, resulting in
multi-view consistent intrinsic decomposition results. To cope with the problem
that different adjacent instances of similar reflectance in a scene are
incorrectly clustered together, we further propose a hierarchical clustering
method with coarse-to-fine optimization to obtain a fast hierarchical indexing
representation. It supports compelling real-time augmented applications such as
recoloring and illumination variation. Extensive experiments and editing
samples on both object-specific/room-scale scenes and synthetic/real-word data
demonstrate that we can obtain consistent intrinsic decomposition results and
high-fidelity novel view synthesis even for challenging sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV2023, Project webpage:
  https://zju3dv.github.io/intrinsic_nerf/, code:
  https://github.com/zju3dv/IntrinsicNeRF</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">46</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Multimodal Foundation Models For Echocardiogram Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deep learning foundation models can learn the relationship between
images and text. In the context of medical imaging, mapping images to language
concepts reflects the clinical task of diagnostic image interpretation, however
current general-purpose foundation models do not perform well in this context
because their training corpus have limited medical text and images. To address
this challenge and account for the range of cardiac physiology, we leverage
1,032,975 cardiac ultrasound videos and corresponding expert interpretations to
develop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP
displays strong zero-shot (not explicitly trained) performance in cardiac
function assessment (external validation left ventricular ejection fraction
mean absolute error (MAE) of 7.1%) and identification of implanted intracardiac
devices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and
artificial heart valves). We also developed a long-context variant (EchoCLIP-R)
with a custom echocardiography report text tokenizer which can accurately
identify unique patients across multiple videos (AUC of 0.86), identify
clinical changes such as orthotopic heart transplants (AUC of 0.79) or cardiac
surgery (AUC 0.77), and enable robust image-to-text search (mean cross-modal
retrieval rank in the top 1% of candidate text reports). These emergent
capabilities can be used for preliminary assessment and summarization of
echocardiographic findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Bridging Distribution Learning and Image Clustering in High-dimensional
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distribution learning focuses on learning the probability density function
from a set of data samples. In contrast, clustering aims to group similar
objects together in an unsupervised manner. Usually, these two tasks are
considered unrelated. However, the relationship between the two may be
indirectly correlated, with Gaussian Mixture Models (GMM) acting as a bridge.
In this paper, we focus on exploring the correlation between distribution
learning and clustering, with the motivation to fill the gap between these two
fields, utilizing an autoencoder (AE) to encode images into a high-dimensional
latent space. Then, Monte-Carlo Marginalization (MCMarg) and Kullback-Leibler
(KL) divergence loss are used to fit the Gaussian components of the GMM and
learn the data distribution. Finally, image clustering is achieved through each
Gaussian component of GMM. Yet, the "curse of dimensionality" poses severe
challenges for most clustering algorithms. Compared with the classic
Expectation-Maximization (EM) Algorithm, experimental results show that MCMarg
and KL divergence can greatly alleviate the difficulty. Based on the
experimental results, we believe distribution learning can exploit the
potential of GMM in image clustering within high-dimensional space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Unveiling Camouflage: A Learnable Fourier-based Augmentation for
  Camouflaged Object Detection and Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>Q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camouflaged object detection (COD) and camouflaged instance segmentation
(CIS) aim to recognize and segment objects that are blended into their
surroundings, respectively. While several deep neural network models have been
proposed to tackle those tasks, augmentation methods for COD and CIS have not
been thoroughly explored. Augmentation strategies can help improve the
performance of models by increasing the size and diversity of the training data
and exposing the model to a wider range of variations in the data. Besides, we
aim to automatically learn transformations that help to reveal the underlying
structure of camouflaged objects and allow the model to learn to better
identify and segment camouflaged objects. To achieve this, we propose a
learnable augmentation method in the frequency domain for COD and CIS via
Fourier transform approach, dubbed CamoFourier. Our method leverages a
conditional generative adversarial network and cross-attention mechanism to
generate a reference image and an adaptive hybrid swapping with parameters to
mix the low-frequency component of the reference image and the high-frequency
component of the input image. This approach aims to make camouflaged objects
more visible for detection and segmentation models. Without bells and whistles,
our proposed augmentation method boosts the performance of camouflaged object
detectors and camouflaged instance segmenters by large margins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Detection of Mild Cognitive Impairment Using Facial Features in Video
  Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early detection of Mild Cognitive Impairment (MCI) leads to early
interventions to slow the progression from MCI into dementia. Deep Learning
(DL) algorithms could help achieve early non-invasive, low-cost detection of
MCI. This paper presents the detection of MCI in older adults using DL models
based only on facial features extracted from video-recorded conversations at
home. We used the data collected from the I-CONECT behavioral intervention
study (NCT02871921), where several sessions of semi-structured interviews
between socially isolated older individuals and interviewers were video
recorded. We develop a framework that extracts spatial holistic facial features
using a convolutional autoencoder and temporal information using transformers.
Our proposed DL model was able to detect the I-CONECT study participants'
cognitive conditions (MCI vs. those with normal cognition (NC)) using facial
features. The segments and sequence information of the facial features improved
the prediction performance compared with the non-temporal features. The
detection accuracy using this combined method reached 88% whereas 84% is the
accuracy without applying the segments and sequences information of the facial
features within a video on a certain theme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> RACR-MIL: Weakly Supervised Skin Cancer Grading using Rank-Aware
  Contextual Reasoning on Whole Slide Images <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>p<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer
in the US. It is diagnosed by manual multi-class tumor grading using a tissue
whole slide image (WSI), which is subjective and suffers from inter-pathologist
variability. We propose an automated weakly-supervised grading approach for
cSCC WSIs that is trained using WSI-level grade and does not require
fine-grained tumor annotations. The proposed model, RACR-MIL, transforms each
WSI into a bag of tiled patches and leverages attention-based multiple-instance
learning to assign a WSI-level grade. We propose three key innovations to
address general as well as cSCC-specific challenges in tumor grading. First, we
leverage spatial and semantic proximity to define a WSI graph that encodes both
local and non-local dependencies between tumor regions and leverage graph
attention convolution to derive contextual patch features. Second, we introduce
a novel ordinal ranking constraint on the patch attention network to ensure
that higher-grade tumor regions are assigned higher attention. Third, we use
tumor depth as an auxiliary task to improve grade classification in a multitask
learning framework. RACR-MIL achieves 2-9% improvement in grade classification
over existing weakly-supervised approaches on a dataset of 718 cSCC tissue
images and localizes the tumor better. The model achieves 5-20% higher accuracy
in difficult-to-classify high-risk grade classes and is robust to class
imbalance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages main text, 2 page references, 3 page appendix; submitted to
  AAAI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Prototype Fission: Closing Set for Robust Open-set Semi-supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised Learning (SSL) has been proven vulnerable to
out-of-distribution (OOD) samples in realistic large-scale unsupervised
datasets due to over-confident pseudo-labeling OODs as in-distribution (ID). A
key underlying problem is class-wise latent space spreading from closed seen
space to open unseen space, and the bias is further magnified in SSL's
self-training loops. To close the ID distribution set so that OODs are better
rejected for safe SSL, we propose Prototype Fission(PF) to divide class-wise
latent spaces into compact sub-spaces by automatic fine-grained latent space
mining, driven by coarse-grained labels only. Specifically, we form multiple
unique learnable sub-class prototypes for each class, optimized towards both
diversity and consistency. The Diversity Modeling term encourages samples to be
clustered by one of the multiple sub-class prototypes, while the Consistency
Modeling term clusters all samples of the same class to a global prototype.
Instead of "opening set", i.e., modeling OOD distribution, Prototype Fission
"closes set" and makes it hard for OOD samples to fit in sub-class latent
space. Therefore, PF is compatible with existing methods for further
performance gains. Extensive experiments validate the effectiveness of our
method in open-set SSL settings in terms of successfully forming sub-classes,
discriminating OODs from IDs and improving overall accuracy. Codes will be
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Learning Sequential Information in Task-based fMRI for Synthetic Data
  Augmentation <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>v<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Insufficiency of training data is a persistent issue in medical image
analysis, especially for task-based functional magnetic resonance images (fMRI)
with spatio-temporal imaging data acquired using specific cognitive tasks. In
this paper, we propose an approach for generating synthetic fMRI sequences that
can then be used to create augmented training datasets in downstream learning
tasks. To synthesize high-resolution task-specific fMRI, we adapt the
$\alpha$-GAN structure, leveraging advantages of both GAN and variational
autoencoder models, and propose different alternatives in aggregating temporal
information. The synthetic images are evaluated from multiple perspectives
including visualizations and an autism spectrum disorder (ASD) classification
task. The results show that the synthetic task-based fMRI can provide effective
data augmentation in learning the ASD classification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Machine Learning in Clinical Neuroimaging 2023 (MICCAI
  workshop), preprint version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Pseudo-Boolean Polynomials Approach for Image Edge Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel approach for image edge detection based on
pseudo-Boolean polynomials for image patches. We show that patches covering
edge regions in the image result in pseudo-Boolean polynomials with higher
degrees compared to patches that cover blob regions. The proposed approach is
based on reduction of polynomial degree and equivalence properties of
penalty-based pseudo-Boolean polynomials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Efficient Ray Sampling for Radiance Fields Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accelerating neural radiance fields training is of substantial practical
value, as the ray sampling strategy profoundly impacts network convergence.
More efficient ray sampling can thus directly enhance existing NeRF models'
training efficiency. We therefore propose a novel ray sampling approach for
neural radiance fields that improves training efficiency while retaining
photorealistic rendering results. First, we analyze the relationship between
the pixel loss distribution of sampled rays and rendering quality. This reveals
redundancy in the original NeRF's uniform ray sampling. Guided by this finding,
we develop a sampling method leveraging pixel regions and depth boundaries. Our
main idea is to sample fewer rays in training views, yet with each ray more
informative for scene fitting. Sampling probability increases in pixel areas
exhibiting significant color and depth variation, greatly reducing wasteful
rays from other regions without sacrificing precision. Through this method, not
only can the convergence of the network be accelerated, but the spatial
geometry of a scene can also be perceived more accurately. Rendering outputs
are enhanced, especially for texture-complex regions. Experiments demonstrate
that our method significantly outperforms state-of-the-art techniques on public
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> DebSDF: Delving into the Details and Bias of Neural Indoor Scene
  Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the neural implicit surface has emerged as a powerful
representation for multi-view surface reconstruction due to its simplicity and
state-of-the-art performance. However, reconstructing smooth and detailed
surfaces in indoor scenes from multi-view images presents unique challenges.
Indoor scenes typically contain large texture-less regions, making the
photometric loss unreliable for optimizing the implicit surface. Previous work
utilizes monocular geometry priors to improve the reconstruction in indoor
scenes. However, monocular priors often contain substantial errors in thin
structure regions due to domain gaps and the inherent inconsistencies when
derived independently from different views. This paper presents \textbf{DebSDF}
to address these challenges, focusing on the utilization of uncertainty in
monocular priors and the bias in SDF-based volume rendering. We propose an
uncertainty modeling technique that associates larger uncertainties with larger
errors in the monocular priors. High-uncertainty priors are then excluded from
optimization to prevent bias. This uncertainty measure also informs an
importance-guided ray sampling and adaptive smoothness regularization,
enhancing the learning of fine structures. We further introduce a bias-aware
signed distance function to density transformation that takes into account the
curvature and the angle between the view direction and the SDF normals to
reconstruct fine details better. Our approach has been validated through
extensive experiments on several challenging datasets, demonstrating improved
qualitative and quantitative results in reconstructing thin structures in
indoor scenes, thereby outperforming previous work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> <span class="highlight-title">3D</span> Adversarial Augmentations for Robust Out-of-Domain Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>-<span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since real-world training datasets cannot properly sample the long tail of
the underlying data distribution, corner cases and rare out-of-domain samples
can severely hinder the performance of state-of-the-art models. This problem
becomes even more severe for dense tasks, such as 3D semantic segmentation,
where points of non-standard objects can be confidently associated to the wrong
class. In this work, we focus on improving the generalization to out-of-domain
data. We achieve this by augmenting the training set with adversarial examples.
First, we learn a set of vectors that deform the objects in an adversarial
fashion. To prevent the adversarial examples from being too far from the
existing data distribution, we preserve their plausibility through a series of
constraints, ensuring sensor-awareness and shapes smoothness. Then, we perform
adversarial augmentation by applying the learned sample-independent vectors to
the available objects when training a model. We conduct extensive experiments
across a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D
object detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D
semantic segmentation. Despite training on a standard single dataset, our
approach substantially improves the robustness and generalization of both 3D
object detection and 3D semantic segmentation methods to out-of-domain data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> An Adaptive Tangent Feature Perspective of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15478v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15478v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>J<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to better understand feature learning in neural networks, we propose
a framework for understanding linear models in tangent feature space where the
features are allowed to be transformed during training. We consider linear
transformations of features, resulting in a joint optimization over parameters
and transformations with a bilinear interpolation constraint. We show that this
optimization problem has an equivalent linearly constrained optimization with
structured regularization that encourages approximately low rank solutions.
Specializing to neural network structure, we gain insights into how the
features and thus the kernel function change, providing additional nuance to
the phenomenon of kernel alignment when the target function is poorly
represented using tangent features. In addition to verifying our theoretical
observations in real neural networks on a simple regression problem, we
empirically show that an adaptive feature implementation of tangent feature
classification has an order of magnitude lower sample complexity than the fixed
tangent feature model on MNIST and CIFAR-10.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A General-Purpose Self-Supervised Model for Computational Pathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>s<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>b<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tissue phenotyping is a fundamental computational pathology (CPath) task in
learning objective characterizations of histopathologic biomarkers in anatomic
pathology. However, whole-slide imaging (WSI) poses a complex computer vision
problem in which the large-scale image resolutions of WSIs and the enormous
diversity of morphological phenotypes preclude large-scale data annotation.
Current efforts have proposed using pretrained image encoders with either
transfer learning from natural image datasets or self-supervised pretraining on
publicly-available histopathology datasets, but have not been extensively
developed and evaluated across diverse tissue types at scale. We introduce UNI,
a general-purpose self-supervised model for pathology, pretrained using over
100 million tissue patches from over 100,000 diagnostic haematoxylin and
eosin-stained WSIs across 20 major tissue types, and evaluated on 33
representative CPath clinical tasks in CPath of varying diagnostic
difficulties. In addition to outperforming previous state-of-the-art models, we
demonstrate new modeling capabilities in CPath such as resolution-agnostic
tissue classification, slide classification using few-shot class prototypes,
and disease subtyping generalization in classifying up to 108 cancer types in
the OncoTree code classification system. UNI advances unsupervised
representation learning at scale in CPath in terms of both pretraining data and
downstream evaluation, enabling data-efficient AI models that can generalize
and transfer to a gamut of diagnostically-challenging tasks and clinical
workflows in anatomic pathology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Learning Modulated Transformation in <span class="highlight-title">GAN</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of style-based generators largely benefits from style modulation,
which helps take care of the cross-instance variation within data. However, the
instance-wise stochasticity is typically introduced via regular convolution,
where kernels interact with features at some fixed locations, limiting its
capacity for modeling geometric variation. To alleviate this problem, we equip
the generator in generative adversarial networks (GANs) with a plug-and-play
module, termed as modulated transformation module (MTM). This module predicts
spatial offsets under the control of latent codes, based on which the
convolution operation can be applied at variable locations for different
instances, and hence offers the model an additional degree of freedom to handle
geometry deformation. Extensive experiments suggest that our approach can be
faithfully generalized to various generative tasks, including image generation,
3D-aware image synthesis, and video generation, and get compatible with
state-of-the-art frameworks without any hyper-parameter tuning. It is
noteworthy that, towards human generation on the challenging TaiChi dataset, we
improve the FID of StyleGAN3 from 21.36 to 13.60, demonstrating the efficacy of
learning modulated geometry transformation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Multimodal Contrastive Learning and Tabular Attention for Automated
  Alzheimer's Disease Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD)
datasets contain valuable tabular data including AD biomarkers and clinical
assessments. Existing computer vision approaches struggle to utilize this
additional information. To address these needs, we propose a generalizable
framework for multimodal contrastive learning of image data and tabular data, a
novel tabular attention module for amplifying and ranking salient features in
tables, and the application of these techniques onto Alzheimer's disease
prediction. Experimental evaulations demonstrate the strength of our framework
by detecting Alzheimer's disease (AD) from over 882 MR image slices from the
ADNI database. We take advantage of the high interpretability of tabular data
and our novel tabular attention approach and through attribution of the
attention scores for each row of the table, we note and rank the most
predominant features. Results show that the model is capable of an accuracy of
over 83.8%, almost a 10% increase from previous state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Input margins can predict generalization too 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding generalization in deep neural networks is an active area of
research. A promising avenue of exploration has been that of margin
measurements: the shortest distance to the decision boundary for a given sample
or its representation internal to the network. While margins have been shown to
be correlated with the generalization ability of a model when measured at its
hidden representations (hidden margins), no such link between large margins and
generalization has been established for input margins. We show that while input
margins are not generally predictive of generalization, they can be if the
search space is appropriately constrained. We develop such a measure based on
input margins, which we refer to as `constrained margins'. The predictive power
of this new measure is demonstrated on the 'Predicting Generalization in Deep
Learning' (PGDL) dataset and contrasted with hidden representation margins. We
find that constrained margins achieve highly competitive scores and outperform
other margin measurements in general. This provides a novel insight on the
relationship between generalization and classification margins, and highlights
the importance of considering the data manifold for investigations of
generalization in DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Online Overexposed Pixels Hallucination in Videos with Adaptive
  Reference Frame Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>m<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>x<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low dynamic range (LDR) cameras cannot deal with wide dynamic range inputs,
frequently leading to local overexposure issues. We present a learning-based
system to reduce these artifacts without resorting to complex acquisition
mechanisms like alternating exposures or costly processing that are typical of
high dynamic range (HDR) imaging. We propose a transformer-based deep neural
network (DNN) to infer the missing HDR details. In an ablation study, we show
the importance of using a multiscale DNN and train it with the proper cost
function to achieve state-of-the-art quality. To aid the reconstruction of the
overexposed areas, our DNN takes a reference frame from the past as an
additional input. This leverages the commonly occurring temporal instabilities
of autoexposure to our advantage: since well-exposed details in the current
frame may be overexposed in the future, we use reinforcement learning to train
a reference frame selection DNN that decides whether to adopt the current frame
as a future reference. Without resorting to alternating exposures, we obtain
therefore a causal, HDR hallucination algorithm with potential application in
common video acquisition settings. Our demo video can be found at
https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The demo video can be found at
  https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Canonical Factors for Hybrid Neural Fields <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factored feature volumes offer a simple way to build more compact, efficient,
and intepretable neural fields, but also introduce biases that are not
necessarily beneficial for real-world data. In this work, we (1) characterize
the undesirable biases that these architectures have for axis-aligned signals
-- they can lead to radiance field reconstruction differences of as high as 2
PSNR -- and (2) explore how learning a set of canonicalizing transformations
can improve representations by removing these biases. We prove in a
two-dimensional model problem that simultaneously learning these
transformations together with scene appearance succeeds with drastically
improved efficiency. We validate the resulting architectures, which we call
TILTED, using image, signed distance, and radiance field reconstruction tasks,
where we observe improvements across quality, robustness, compactness, and
runtime. Results demonstrate that TILTED can enable capabilities comparable to
baselines that are 2x larger, while highlighting weaknesses of neural field
evaluation procedures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023. Project webpage: https://brentyi.github.io/tilted/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Pseudo-Boolean Polynomials Approach To Edge Detection And Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>p<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a deterministic approach to edge detection and image
segmentation by formulating pseudo-Boolean polynomials on image patches. The
approach works by applying a binary classification of blob and edge regions in
an image based on the degrees of pseudo-Boolean polynomials calculated on
patches extracted from the provided image. We test our method on simple images
containing primitive shapes of constant and contrasting colour and establish
the feasibility before applying it to complex instances like aerial landscape
images. The proposed method is based on the exploitation of the reduction,
polynomial degree, and equivalence properties of penalty-based pseudo-Boolean
polynomials.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, submitted to the International Conference Data
  Analysis, Optimization and Their Applications on the Occasion of Boris
  Mirkin's 80th Birthday January 30-31, 2023, Dolgoprudny, Moscow Region,
  Moscow Institute of Physics and Technology
  https://mipt.ru/education/chairs/dm/conferences/data-analysis-optimization-and-their-applications-2023.php</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Document AI: A Comparative Study of Transformer-Based, Graph-Based
  Models, and Convolutional Neural Networks For Document Layout Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15517v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15517v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document AI aims to automatically analyze documents by leveraging natural
language processing and computer vision techniques. One of the major tasks of
Document AI is document layout analysis, which structures document pages by
interpreting the content and spatial relationships of layout, image, and text.
This task can be image-centric, wherein the aim is to identify and label
various regions such as authors and paragraphs, or text-centric, where the
focus is on classifying individual words in a document. Although there are
increasingly sophisticated methods for improving layout analysis, doubts remain
about the extent to which their findings can be generalized to a broader
context. Specifically, prior work developed systems based on very different
architectures, such as transformer-based, graph-based, and CNNs. However, no
work has mentioned the effectiveness of these models in a comparative analysis.
Moreover, while language-independent Document AI models capable of knowledge
transfer have been developed, it remains to be investigated to what degree they
can effectively transfer knowledge. In this study, we aim to fill these gaps by
conducting a comparative evaluation of state-of-the-art models in document
layout analysis and investigating the potential of cross-lingual layout
analysis by utilizing machine translation techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Complementing Onboard Sensors with Satellite Map: A New Perspective for
  HD Map Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15427v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15427v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-Definition (HD) maps play a crucial role in autonomous driving systems.
Recent methods have attempted to construct HD maps in real-time based on
information obtained from vehicle onboard sensors. However, the performance of
these methods is significantly susceptible to the environment surrounding the
vehicle due to the inherent limitation of onboard sensors, such as weak
capacity for long-range detection. In this study, we demonstrate that
supplementing onboard sensors with satellite maps can enhance the performance
of HD map construction methods, leveraging the broad coverage capability of
satellite maps. For the purpose of further research, we release the satellite
map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose
a hierarchical fusion module that enables better fusion of satellite maps
information with existing methods. Specifically, we design an attention mask
based on segmentation and distance, applying the cross-attention mechanism to
fuse onboard Bird's Eye View (BEV) features and satellite features in
feature-level fusion. An alignment module is introduced before concatenation in
BEV-level fusion to mitigate the impact of misalignment between the two
features. The experimental results on the augmented nuScenes dataset showcase
the seamless integration of our module into three existing HD map construction
methods. It notably enhances their performance in both HD map semantic
segmentation and instance detection tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> WrappingNet: Mesh Autoencoder via Deep Sphere Deformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>E<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There have been recent efforts to learn more meaningful representations via
fixed length codewords from mesh data, since a mesh serves as a complete model
of underlying 3D shape compared to a point cloud. However, the mesh
connectivity presents new difficulties when constructing a deep learning
pipeline for meshes. Previous mesh unsupervised learning approaches typically
assume category-specific templates, e.g., human face/body templates. It
restricts the learned latent codes to only be meaningful for objects in a
specific category, so the learned latent spaces are unable to be used across
different types of objects. In this work, we present WrappingNet, the first
mesh autoencoder enabling general mesh unsupervised learning over heterogeneous
objects. It introduces a novel base graph in the bottleneck dedicated to
representing mesh connectivity, which is shown to facilitate learning a shared
latent space representing object shape. The superiority of WrappingNet mesh
learning is further demonstrated via improved reconstruction quality and
competitive classification compared to point cloud learning, as well as latent
interpolation between meshes of different categories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Robust Long-Tailed Learning via Label-Aware Bounded CVaR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data in the real-world classification problems are always imbalanced or
long-tailed, wherein the majority classes have the most of the samples that
dominate the model training. In such setting, the naive model tends to have
poor performance on the minority classes. Previously, a variety of loss
modifications have been proposed to address the long-tailed leaning problem,
while these methods either treat the samples in the same class
indiscriminatingly or lack a theoretical guarantee. In this paper, we propose
two novel approaches based on CVaR (Conditional Value at Risk) to improve the
performance of long-tailed learning with a solid theoretical ground.
Specifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss
to overcome the pessimistic result of the original CVaR, and further design the
optimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we
additionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to
stabilize the optimization process, where we also offer the theoretical
support. Extensive experiments on real-world datasets with long-tailed label
distributions verify the superiority of our proposed methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Color Aesthetics: Fuzzy based User-driven Method for Harmony and
  Preference Prediction <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color is the most important intrinsic sensory feature that has a powerful
impact on product sales. Color is even responsible for raising the aesthetic
senses in our brains. Account for individual differences is crucial in color
aesthetics. It requires user-driven mechanisms for various e-commerce
applications. We propose a method for quantitative evaluation of all types of
perceptual responses to color(s): distinct color preference, color harmony, and
color combination preference. Preference for color schemes can be predicted by
combining preferences for the basic colors and ratings of color harmony.
Harmonious pallets are extracted from big data set using comparison algorithms
based on fuzzy similarity and grouping. The proposed model results in useful
predictions of harmony and preference of multicolored images. For example, in
the context of apparel coordination, it allows predicting a preference for a
look based on clothing colors. Our approach differs from standard aesthetic
models, since in accounts for a personal variation. In addition, it can process
not only lower-order color pairs, but also groups of several colors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>It was accepted as a short paper. IFSA-SCIS 2017 Conference held in
  Otsu, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Shatter and Gather: Learning Referring Image Segmentation with Text
  Supervision <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Referring image segmentation, the task of segmenting any arbitrary entities
described in free-form texts, opens up a variety of vision applications.
However, manual labeling of training data for this task is prohibitively
costly, leading to lack of labeled data for training. We address this issue by
a weakly supervised learning approach using text descriptions of training
images as the only source of supervision. To this end, we first present a new
model that discovers semantic entities in input image and then combines such
entities relevant to text query to predict the mask of the referent. We also
present a new loss function that allows the model to be trained without any
further supervision. Our method was evaluated on four public benchmarks for
referring image segmentation, where it clearly outperformed the existing method
for the same task and recent open-vocabulary segmentation models on all the
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Shape-Margin Knowledge Augmented Network for Thyroid Nodule Segmentation
  and Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thyroid nodule segmentation is a crucial step in the diagnostic procedure of
physicians and computer-aided diagnosis systems. Mostly, current studies treat
segmentation and diagnosis as independent tasks without considering the
correlation between these tasks. The sequence steps of these independent tasks
in computer-aided diagnosis systems may lead to the accumulation of errors.
Therefore, it is worth combining them as a whole through exploring the
relationship between thyroid nodule segmentation and diagnosis. According to
the thyroid imaging reporting and data system (TI-RADS), the assessment of
shape and margin characteristics is the prerequisite for the discrimination of
benign and malignant thyroid nodules. These characteristics can be observed in
the thyroid nodule segmentation masks. Inspired by the diagnostic procedure of
TI-RADS, this paper proposes a shape-margin knowledge augmented network
(SkaNet) for simultaneously thyroid nodule segmentation and diagnosis. Due to
the similarity in visual features between segmentation and diagnosis, SkaNet
shares visual features in the feature extraction stage and then utilizes a
dual-branch architecture to perform thyroid nodule segmentation and diagnosis
tasks simultaneously. To enhance effective discriminative features, an
exponential mixture module is devised, which incorporates convolutional feature
maps and self-attention maps by exponential weighting. Then, SkaNet is jointly
optimized by a knowledge augmented multi-task loss function with a constraint
penalty term. It embeds shape and margin characteristics through numerical
computation and models the relationship between the thyroid nodule diagnosis
results and segmentation masks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> On the Robustness of Object Detection Models in Aerial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robustness of object detection models is a major concern when applied to
real-world scenarios. However, the performance of most object detection models
degrades when applied to images subjected to corruptions, since they are
usually trained and evaluated on clean datasets. Enhancing the robustness of
object detection models is of utmost importance, especially for those designed
for aerial images, which feature complex backgrounds, substantial variations in
scales and orientations of objects. This paper addresses the challenge of
assessing the robustness of object detection models in aerial images, with a
specific emphasis on scenarios where images are affected by clouds. In this
study, we introduce two novel benchmarks based on DOTA-v1.0. The first
benchmark encompasses 19 prevalent corruptions, while the second focuses on
cloud-corrupted images-a phenomenon uncommon in natural pictures yet frequent
in aerial photography. We systematically evaluate the robustness of mainstream
object detection models and perform numerous ablation experiments. Through our
investigations, we find that enhanced model architectures, larger networks,
well-crafted modules, and judicious data augmentation strategies collectively
enhance the robustness of aerial object detection models. The benchmarks we
propose and our comprehensive experimental analyses can facilitate research on
robust object detection in aerial images. Codes and datasets are available at:
(https://github.com/hehaodong530/DOTA-C)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Efficient Model Personalization in Federated Learning via
  Client-Specific Prompt Generation <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15367v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15367v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>F<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>E<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) emerges as a decentralized learning framework which
trains models from multiple distributed clients without sharing their data to
preserve privacy. Recently, large-scale pre-trained models (e.g., Vision
Transformer) have shown a strong capability of deriving robust representations.
However, the data heterogeneity among clients, the limited computation
resources, and the communication bandwidth restrict the deployment of
large-scale models in FL frameworks. To leverage robust representations from
large-scale models while enabling efficient model personalization for
heterogeneous clients, we propose a novel personalized FL framework of
client-specific Prompt Generation (pFedPG), which learns to deploy a
personalized prompt generator at the server for producing client-specific
visual prompts that efficiently adapts frozen backbones to local data
distributions. Our proposed framework jointly optimizes the stages of
personalized prompt adaptation locally and personalized prompt generation
globally. The former aims to train visual prompts that adapt foundation models
to each client, while the latter observes local optimization directions to
generate personalized prompts for all clients. Through extensive experiments on
benchmark datasets, we show that our pFedPG is favorable against
state-of-the-art personalized FL methods under various types of data
heterogeneity, allowing computation and communication efficient model
personalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have
demonstrated the capability of understanding images and achieved remarkable
performance in various visual tasks. Despite their strong abilities in
recognizing common objects due to extensive training datasets, they lack
specific domain knowledge and have a weaker understanding of localized details
within objects, which hinders their effectiveness in the Industrial Anomaly
Detection (IAD) task. On the other hand, most existing IAD methods only provide
anomaly scores and necessitate the manual setting of thresholds to distinguish
between normal and abnormal samples, which restricts their practical
implementation. In this paper, we explore the utilization of LVLM to address
the IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. We
generate training data by simulating anomalous images and producing
corresponding textual descriptions for each image. We also employ an image
decoder to provide fine-grained semantic and design a prompt learner to
fine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the need
for manual threshold adjustments, thus directly assesses the presence and
locations of anomalies. Additionally, AnomalyGPT supports multi-turn dialogues
and exhibits impressive few-shot in-context learning capabilities. With only
one normal shot, AnomalyGPT achieves the state-of-the-art performance with an
accuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3%
on the MVTec-AD dataset. Code is available at
https://github.com/CASIA-IVA-Lab/AnomalyGPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Ego-Motion Estimation and Dynamic Motion Separation from <span class="highlight-title">3D</span> Point Clouds
  for Accumulating Data and Improving <span class="highlight-title">3D</span> Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New 3+1D high-resolution radar sensors are gaining importance for 3D object
detection in the automotive domain due to their relative affordability and
improved detection compared to classic low-resolution radar sensors. One
limitation of high-resolution radar sensors, compared to lidar sensors, is the
sparsity of the generated point cloud. This sparsity could be partially
overcome by accumulating radar point clouds of subsequent time steps. This
contribution analyzes limitations of accumulating radar point clouds on the
View-of-Delft dataset. By employing different ego-motion estimation approaches,
the dataset's inherent constraints, and possible solutions are analyzed.
Additionally, a learning-based instance motion estimation approach is deployed
to investigate the influence of dynamic motion on the accumulated point cloud
for object detection. Experiments document an improved object detection
performance by applying an ego-motion estimation and dynamic motion correction
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at: AmE 2023 - Automotive meets Electronics; 14. GMM
  Symposium (https://ieeexplore.ieee.org/document/10227711)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Detect, Augment, Compose, and Adapt: Four Steps for Unsupervised Domain
  Adaptation in Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>f<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised domain adaptation (UDA) plays a crucial role in object detection
when adapting a source-trained detector to a target domain without annotated
data. In this paper, we propose a novel and effective four-step UDA approach
that leverages self-supervision and trains source and target data concurrently.
We harness self-supervised learning to mitigate the lack of ground truth in the
target domain. Our method consists of the following steps: (1) identify the
region with the highest-confidence set of detections in each target image,
which serve as our pseudo-labels; (2) crop the identified region and generate a
collection of its augmented versions; (3) combine these latter into a composite
image; (4) adapt the network to the target domain using the composed image.
Through extensive experiments under cross-camera, cross-weather, and
synthetic-to-real scenarios, our approach achieves state-of-the-art
performance, improving upon the nearest competitor by more than 2% in terms of
mean Average Precision (mAP). The code is available at
https://github.com/MohamedTEV/DACA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Enhancing Mobile Face Anti-Spoofing: A Robust Framework for Diverse
  Attack Types under Screen Flash 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anti-spoofing (FAS) is crucial for securing face recognition systems.
However, existing FAS methods with handcrafted binary or pixel-wise labels have
limitations due to diverse presentation attacks (PAs). In this paper, we
propose an attack type robust face anti-spoofing framework under light flash,
called ATR-FAS. Due to imaging differences caused by various attack types,
traditional FAS methods based on single binary classification network may
result in excessive intra-class distance of spoof faces, leading to a challenge
of decision boundary learning. Therefore, we employed multiple networks to
reconstruct multi-frame depth maps as auxiliary supervision, and each network
experts in one type of attack. A dual gate module (DGM) consisting of a type
gate and a frame-attention gate is introduced, which perform attack type
recognition and multi-frame attention generation, respectively. The outputs of
DGM are utilized as weight to mix the result of multiple expert networks. The
multi-experts mixture enables ATR-FAS to generate spoof-differentiated depth
maps, and stably detects spoof faces without being affected by different types
of PAs. Moreover, we design a differential normalization procedure to convert
original flash frames into differential frames. This simple but effective
processing enhances the details in flash frames, aiding in the generation of
depth maps. To verify the effectiveness of our framework, we collected a
large-scale dataset containing 12,660 live and spoof videos with diverse PAs
under dynamic flash from the smartphone screen. Extensive experiments
illustrate that the proposed ATR-FAS significantly outperforms existing
state-of-the-art methods. The code and dataset will be available at
https://github.com/Chaochao-Lin/ATR-FAS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> IndGIC: Supervised Action Recognition under Low Illumination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Technologies of human action recognition in the dark are gaining more and
more attention as huge demand in surveillance, motion control and
human-computer interaction. However, because of limitation in image enhancement
method and low-lighting video datasets, e.g. labeling cost, existing methods
meet some problems. Some video-based approached are effect and efficient in
specific datasets but cannot generalize to most cases while others methods
using multiple sensors rely heavily to prior knowledge to deal with noisy
nature from video stream. In this paper, we proposes action recognition method
using deep multi-input network. Furthermore, we proposed a Independent Gamma
Intensity Corretion (Ind-GIC) to enhance poor-illumination video, generating
one gamma for one frame to increase enhancement performance. To prove our
method is effective, there is some evaluation and comparison between our method
and existing methods. Experimental results show that our model achieves high
accuracy in on ARID dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Imperceptible Adversarial Attack on Deep Neural Networks from Image
  Boundary 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Deep Neural Networks (DNNs), such as the convolutional neural
networks (CNN) and Vision Transformers (ViTs), have been successfully applied
in the field of computer vision, they are demonstrated to be vulnerable to
well-sought Adversarial Examples (AEs) that can easily fool the DNNs. The
research in AEs has been active, and many adversarial attacks and explanations
have been proposed since they were discovered in 2014. The mystery of the AE's
existence is still an open question, and many studies suggest that DNN training
algorithms have blind spots. The salient objects usually do not overlap with
boundaries; hence, the boundaries are not the DNN model's attention.
Nevertheless, recent studies show that the boundaries can dominate the behavior
of the DNN models. Hence, this study aims to look at the AEs from a different
perspective and proposes an imperceptible adversarial attack that systemically
attacks the input image boundary for finding the AEs. The experimental results
have shown that the proposed boundary attacking method effectively attacks six
CNN models and the ViT using only 32% of the input image content (from the
boundaries) with an average success rate (SR) of 95.2% and an average peak
signal-to-noise ratio of 41.37 dB. Correlation analyses are conducted,
including the relation between the adversarial boundary's width and the SR and
how the adversarial boundary changes the DNN model's attention. This paper's
discoveries can potentially advance the understanding of AEs and provide a
different perspective on how AEs can be constructed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Human from Blur: Human Pose Tracking from Blurry Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>o<span class="highlight-author"></span>z<span class="highlight-author"></span>u<span class="highlight-author"></span>m<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>O<span class="highlight-author"></span>t<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>s<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to estimate 3D human poses from substantially blurred
images. The key idea is to tackle the inverse problem of image deblurring by
modeling the forward problem with a 3D human model, a texture map, and a
sequence of poses to describe human motion. The blurring process is then
modeled by a temporal image aggregation step. Using a differentiable renderer,
we can solve the inverse problem by backpropagating the pixel-wise reprojection
error to recover the best human motion representation that explains a single or
multiple input images. Since the image reconstruction loss alone is
insufficient, we present additional regularization terms. To the best of our
knowledge, we present the first method to tackle this problem. Our method
consistently outperforms other methods on significantly blurry inputs since
they lack one or multiple key functionalities that our method unifies, i.e.
image deblurring with sub-frame accuracy and explicit 3D modeling of non-rigid
human motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Parkinson gait modelling from an anomaly deep representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>E<span class="highlight-author"></span>d<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>z<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parkinson's Disease (PD) is associated with gait movement disorders, such as
bradykinesia, stiffness, tremors and postural instability, caused by
progressive dopamine deficiency. Today, some approaches have implemented
learning representations to quantify kinematic patterns during locomotion,
supporting clinical procedures such as diagnosis and treatment planning. These
approaches assumes a large amount of stratified and labeled data to optimize
discriminative representations. Nonetheless these considerations may restrict
the approaches to be operable in real scenarios during clinical practice. This
work introduces a self-supervised generative representation to learn
gait-motion-related patterns, under the pretext of video reconstruction and an
anomaly detection framework. This architecture is trained following a one-class
weakly supervised learning to avoid inter-class variance and approach the
multiple relationships that represent locomotion. The proposed approach was
validated with 14 PD patients and 23 control subjects, and trained with the
control population only, achieving an AUC of 95%, homocedasticity level of 70%
and shapeness level of 70% in the classification task considering its
generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal not submitted to any editorial</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02422v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02422v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The mobile cloud gaming industry has been rapidly growing over the last
decade. When streaming gaming videos are transmitted to customers' client
devices from cloud servers, algorithms that can monitor distorted video quality
without having any reference video available are desirable tools. However,
creating No-Reference Video Quality Assessment (NR VQA) models that can
accurately predict the quality of streaming gaming videos rendered by computer
graphics engines is a challenging problem, since gaming content generally
differs statistically from naturalistic videos, often lacks detail, and
contains many smooth regions. Until recently, the problem has been further
complicated by the lack of adequate subjective quality databases of mobile
gaming content. We have created a new gaming-specific NR VQA model called the
Gaming Video Quality Evaluator (GAMIVAL), which combines and leverages the
advantages of spatial and temporal gaming distorted scene statistics models, a
neural noise model, and deep semantic features. Using a support vector
regression (SVR) as a regressor, GAMIVAL achieves superior performance on the
new LIVE-Meta Mobile Cloud Gaming (LIVE-Meta MCG) video quality database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE SPL 2023. The implementation of GAMIVAL has been
  made available online: https://github.com/lskdream/GAMIVAL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Robust affine point matching via quadratic assignment on Grassmannians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02698v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02698v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>x<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Affine matching with Grassmannians (RAG) is a new algorithm to perform
affine registration of point clouds. The algorithm is based on minimizing the
Frobenius distance between two elements of the Grassmannian. For this purpose,
an indefinite relaxation of the Quadratic Assignment Problem (QAP) is used, and
several approaches to affine feature matching are studied and compared.
Experiments demonstrate that RAG is more robust to noise and point discrepancy
than previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 23 figures; GitHub repository at
  (https://github.com/sashakolpakov/rag)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GazeGNN: A Gaze-Guided Graph Neural Network for Chest X-ray
  Classification <span class="chip">WACV 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>b<span class="highlight-author"></span>o<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>f<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>k<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span> <span class="highlight-author"></span>U<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>p<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>U<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eye tracking research is important in computer vision because it can help us
understand how humans interact with the visual world. Specifically for
high-risk applications, such as in medical imaging, eye tracking can help us to
comprehend how radiologists and other medical professionals search, analyze,
and interpret images for diagnostic and clinical purposes. Hence, the
application of eye tracking techniques in disease classification has become
increasingly popular in recent years. Contemporary works usually transform gaze
information collected by eye tracking devices into visual attention maps (VAMs)
to supervise the learning process. However, this is a time-consuming
preprocessing step, which stops us from applying eye tracking to radiologists'
daily work. To solve this problem, we propose a novel gaze-guided graph neural
network (GNN), GazeGNN, to leverage raw eye-gaze data without being converted
into VAMs. In GazeGNN, to directly integrate eye gaze into image
classification, we create a unified representation graph that models both
images and gaze pattern information. With this benefit, we develop a real-time,
real-world, end-to-end disease classification algorithm for the first time in
the literature. This achievement demonstrates the practicality and feasibility
of integrating real-time eye tracking techniques into the daily work of
radiologists. To our best knowledge, GazeGNN is the first work that adopts GNN
to integrate image and eye-gaze data. Our experiments on the public chest X-ray
dataset show that our proposed method exhibits the best classification
performance compared to existing methods. The code is available at
https://github.com/ukaukaaaa/GazeGNN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> CASSPR: Cross Attention Single Scan Place Recognition <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>U<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>ã<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>q<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition based on point clouds (LiDAR) is an important component for
autonomous robots or self-driving vehicles. Current SOTA performance is
achieved on accumulated LiDAR submaps using either point-based or voxel-based
structures. While voxel-based approaches nicely integrate spatial context
across multiple scales, they do not exhibit the local precision of point-based
methods. As a result, existing methods struggle with fine-grained matching of
subtle geometric features in sparse single-shot Li- DAR scans. To overcome
these limitations, we propose CASSPR as a method to fuse point-based and
voxel-based approaches using cross attention transformers. CASSPR leverages a
sparse voxel branch for extracting and aggregating information at lower
resolution and a point-wise branch for obtaining fine-grained local
information. CASSPR uses queries from one branch to try to match structures in
the other branch, ensuring that both extract self-contained descriptors of the
point cloud (rather than one branch dominating), but using both to inform the
output global descriptor of the point cloud. Extensive experiments show that
CASSPR surpasses the state-of-the-art by a large margin on several datasets
(Oxford RobotCar, TUM, USyd). For instance, it achieves AR@1 of 85.6% on the
TUM dataset, surpassing the strongest prior model by ~15%. Our code is publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> MetaCOG: Learning a Metacognition to Recover What Objects Are Actually
  There 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.03105v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.03105v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>-<span class="highlight-author"></span>E<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans not only form representations about the world based on what we see,
but also learn meta-cognitive representations about how our own vision works.
This enables us to recognize when our vision is unreliable (e.g., when we
realize that we are experiencing a visual illusion) and enables us to question
what we see. Inspired by this human capacity, we present MetaCOG: a model that
increases the robustness of object detectors by learning representations of
their reliability, and does so without feedback. Specifically, MetaCOG is a
hierarchical probabilistic model that expresses a joint distribution over the
objects in a 3D scene and the outputs produced by a detector. When paired with
an off-the-shelf object detector, MetaCOG takes detections as input and infers
the detector's tendencies to miss objects of certain categories and to
hallucinate objects that are not actually present, all without access to
ground-truth object labels. When paired with three modern neural object
detectors, MetaCOG learns useful and accurate meta-cognitive representations,
resulting in improved performance on the detection task. Additionally, we show
that MetaCOG is robust to varying levels of error in the detections. Our
results are a proof-of-concept for a novel approach to the problem of
correcting a faulty vision system's errors. The model code, datasets, results,
and demos are available:
https://osf.io/8b9qt/?view_only=8c1b1c412c6b4e1697e3c7859be2fce6
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Fairness-aware Vision Transformer via Debiased Self-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformer (ViT) has recently gained significant interest in solving
computer vision (CV) problems due to its capability of extracting informative
features and modeling long-range dependencies through the self-attention
mechanism. To fully realize the advantages of ViT in real-world applications,
recent works have explored the trustworthiness of ViT, including its robustness
and explainability. However, another desiderata, fairness has not yet been
adequately addressed in the literature. We establish that the existing
fairness-aware algorithms (primarily designed for CNNs) do not perform well on
ViT. This necessitates the need for developing our novel framework via Debiased
Self-Attention (DSA). DSA is a fairness-through-blindness approach that
enforces ViT to eliminate spurious features correlated with the sensitive
attributes for bias mitigation. Notably, adversarial examples are leveraged to
locate and mask the spurious features in the input image patches. In addition,
DSA utilizes an attention weights alignment regularizer in the training
objective to encourage learning informative features for target prediction.
Importantly, our DSA framework leads to improved fairness guarantees over prior
works on multiple prediction tasks without compromising target prediction
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> An Empirical Investigation of the Role of Pre-training in Lifelong
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.09153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.09153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>E<span class="highlight-author"></span>m<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>l<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lifelong learning paradigm in machine learning is an attractive
alternative to the more prominent isolated learning scheme not only due to its
resemblance to biological learning but also its potential to reduce energy
waste by obviating excessive model re-training. A key challenge to this
paradigm is the phenomenon of catastrophic forgetting. With the increasing
popularity and success of pre-trained models in machine learning, we pose the
question: What role does pre-training play in lifelong learning, specifically
with respect to catastrophic forgetting? We investigate existing methods in the
context of large, pre-trained models and evaluate their performance on a
variety of text and image classification tasks, including a large-scale study
using a novel data set of 15 diverse NLP tasks. Across all settings, we observe
that generic pre-training implicitly alleviates the effects of catastrophic
forgetting when learning multiple tasks sequentially compared to randomly
initialized models. We then further investigate why pre-training alleviates
forgetting in this setting. We study this phenomenon by analyzing the loss
landscape, finding that pre-trained weights appear to ease forgetting by
leading to wider minima. Based on this insight, we propose jointly optimizing
for current task loss and loss basin sharpness to explicitly encourage wider
basins during sequential fine-tuning. We show that this optimization approach
outperforms several state-of-the-art task-sequential continual learning
algorithms across multiple settings, occasionally even without retaining a
memory that scales in size with the number of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Beyond Document Page Classification: Design, Datasets, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12896v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12896v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>d<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>-<span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper highlights the need to bring document classification benchmarking
closer to real-world applications, both in the nature of data tested ($X$:
multi-channel, multi-paged, multi-industry; $Y$: class distributions and label
set variety) and in classification tasks considered ($f$: multi-page document,
page stream, and document bundle classification, ...). We identify the lack of
public multi-page document classification datasets, formalize different
classification tasks arising in application scenarios, and motivate the value
of targeting efficient multi-page document representations. An experimental
study on proposed multi-page document classification datasets demonstrates that
current benchmarks have become irrelevant and need to be updated to evaluate
complete documents, as they naturally occur in practice. This reality check
also calls for more mature evaluation methodologies, covering calibration
evaluation, inference complexity (time-memory), and a range of realistic
distribution shifts (e.g., born-digital vs. scanning noise, shifting page
order). Our study ends on a hopeful note by recommending concrete avenues for
future improvements.}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Learning Content-enhanced Mask Transformer for Domain Generalized
  Urban-Scene Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain-generalized urban-scene semantic segmentation (USSS) aims to learn
generalized semantic predictions across diverse urban-scene styles. Unlike
domain gap challenges, USSS is unique in that the semantic categories are often
similar in different urban scenes, while the styles can vary significantly due
to changes in urban landscapes, weather conditions, lighting, and other
factors. Existing approaches typically rely on convolutional neural networks
(CNNs) to learn the content of urban scenes.
  In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for
domain-generalized USSS. The main idea is to enhance the focus of the
fundamental component, the mask attention mechanism, in Transformer
segmentation models on content information. To achieve this, we introduce a
novel content-enhanced mask attention mechanism. It learns mask queries from
both the image feature and its down-sampled counterpart, as lower-resolution
image features usually contain more robust content information and are less
sensitive to style variations. These features are fused into a Transformer
decoder and integrated into a multi-resolution content-enhanced mask attention
learning scheme.
  Extensive experiments conducted on various domain-generalized urban-scene
segmentation datasets demonstrate that the proposed CMFormer significantly
outperforms existing CNN-based methods for domain-generalized semantic
segmentation, achieving improvements of up to 14.00\% in terms of mIoU (mean
intersection over union). The source code for CMFormer will be made available
at this
\href{https://github.com/BiQiWHU/domain-generalized-urban-scene-segmentation}{repository}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Few-shot $\mathbf{1/a}$ Anomalies Feedback : Damage Vision Mining
  Opportunity and Embedding Feature Imbalance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.12676v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.12676v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>o<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, previous balanced datasets have been used to advance
deep learning algorithms for industrial applications. In urban infrastructures
and living environments, damage data mining cannot avoid imbalanced data issues
because of rare unseen events and the high-quality status of improved
operations. For visual inspection, the deteriorated class acquired from the
surface of concrete and steel components are occasionally imbalanced. From
numerous related surveys, we conclude that imbalanced data problems can be
categorised into four types: 1) missing range of target and label valuables, 2)
majority-minority class imbalance, 3) foreground background of spatial
imbalance, and 4) long-tailed class of pixel-wise imbalance. Since 2015, many
imbalanced studies have been conducted using deep-learning approaches,
including regression, image classification, object detection, and semantic
segmentation. However, anomaly detection for imbalanced data is not well known.
In this study, we highlight a one-class anomaly detection application, whether
anomalous class or not, and demonstrate clear examples of imbalanced vision
datasets: medical disease, hazardous behaviour, material deterioration, plant
disease, river sludge, and disaster damage. We provide key results on the
advantage of damage-vision mining, hypothesising that the more effective the
range of the positive ratio, the higher the accuracy gain of the anomalies
feedback. In our imbalanced studies, compared with the balanced case with a
positive ratio of $1/1$, we find that there is an applicable positive ratio
$1/a$ where the accuracy is consistently high. However, the extremely
imbalanced range is from one shot to $1/2a$, the accuracy of which is inferior
to that of the applicable ratio. In contrast, with a positive ratio ranging
over $2/a$, it shifts in the over-mining phase without an effective gain in
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 53 figures, 28 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-08-28T00:00:00Z">2023-08-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Automated Conversion of Music Videos into Lyric Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>g<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Musicians and fans often produce lyric videos, a form of music videos that
showcase the song's lyrics, for their favorite songs. However, making such
videos can be challenging and time-consuming as the lyrics need to be added in
synchrony and visual harmony with the video. Informed by prior work and close
examination of existing lyric videos, we propose a set of design guidelines to
help creators make such videos. Our guidelines ensure the readability of the
lyric text while maintaining a unified focus of attention. We instantiate these
guidelines in a fully automated pipeline that converts an input music video
into a lyric video. We demonstrate the robustness of our pipeline by generating
lyric videos from a diverse range of input sources. A user study shows that
lyric videos generated by our pipeline are effective in maintaining text
readability and unifying the focus of attention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck
  Muscle Contraction <span class="chip">SIGGRAPH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>x<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR
experiences. While VR/AR head-mounted displays unlock users' natural wide-range
head movements during viewing, their neck muscle comfort is inevitably
compromised by the added hardware weight. Unfortunately, little quantitative
knowledge for understanding and addressing such an issue is available so far.
  Leveraging electromyography devices, we measure, model, and predict VR users'
neck muscle contraction levels (MCL) while they move their heads to interact
with the virtual environment. Specifically, by learning from collected
physiological data, we develop a bio-physically inspired computational model to
predict neck MCL under diverse head kinematic states. Beyond quantifying the
cumulative MCL of completed head movements, our model can also predict
potential MCL requirements with target head poses only. A series of objective
evaluations and user studies demonstrate its prediction accuracy and
generality, as well as its ability in reducing users' neck discomfort by
optimizing the layout of visual targets. We hope this research will motivate
new ergonomic-centered designs for VR/AR and interactive graphics applications.
Source code is released at:
https://github.com/NYU-ICL/xr-ergonomics-neck-comfort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM SIGGRAPH 2023 Conference Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> MagicAvatar: Multimodal Avatar Generation and Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>f<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report presents MagicAvatar, a framework for multimodal video generation
and animation of human avatars. Unlike most existing methods that generate
avatar-centric videos directly from multimodal inputs (e.g., text prompts),
MagicAvatar explicitly disentangles avatar video generation into two stages:
(1) multimodal-to-motion and (2) motion-to-video generation. The first stage
translates the multimodal inputs into motion/ control signals (e.g., human
pose, depth, DensePose); while the second stage generates avatar-centric video
guided by these motion signals. Additionally, MagicAvatar supports avatar
animation by simply providing a few images of the target person. This
capability enables the animation of the provided human identity according to
the specific motion derived from the first stage. We demonstrate the
flexibility of MagicAvatar through various applications, including text-guided
and video-guided avatar generation, as well as multimodal avatar animation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://magic-avatar.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Total Selfie: Generating Full-Body Selfies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>I<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>-<span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>v<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>z<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a method to generate full-body selfies -- photos that you take of
yourself, but capturing your whole body as if someone else took the photo of
you from a few feet away. Our approach takes as input a pre-captured video of
your body, a target pose photo, and a selfie + background pair for each
location. We introduce a novel diffusion-based approach to combine all of this
information into high quality, well-composed photos of you with the desired
pose and background.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://homes.cs.washington.edu/~boweiche/project_page/totalselfie/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Flexible Techniques for Differentiable Rendering with <span class="highlight-title">3D</span> Gaussians 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fast, reliable shape reconstruction is an essential ingredient in many
computer vision applications. Neural Radiance Fields demonstrated that
photorealistic novel view synthesis is within reach, but was gated by
performance requirements for fast reconstruction of real scenes and objects.
Several recent approaches have built on alternative shape representations, in
particular, 3D Gaussians. We develop extensions to these renderers, such as
integrating differentiable optical flow, exporting watertight meshes and
rendering per-ray normals. Additionally, we show how two of the recent methods
are interoperable with each other. These reconstructions are quick, robust, and
easily performed on GPU or CPU. For code and visual examples, see
https://leonidk.github.io/fmb-plus
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> HoloFusion: Towards Photo-realistic <span class="highlight-title">3D</span> <span class="highlight-title">Generative</span> Modeling <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.14244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.14244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>n<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>V<span class="highlight-author"></span>e<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>o<span class="highlight-author"></span>t<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based image generators can now produce high-quality and diverse
samples, but their success has yet to fully translate to 3D generation:
existing diffusion methods can either generate low-resolution but 3D consistent
outputs, or detailed 2D views of 3D objects but with potential structural
defects and lacking view consistency or realism. We present HoloFusion, a
method that combines the best of these approaches to produce high-fidelity,
plausible, and diverse 3D samples while learning from a collection of
multi-view 2D images only. The method first generates coarse 3D samples using a
variant of the recently proposed HoloDiffusion generator. Then, it
independently renders and upsamples a large number of views of the coarse 3D
model, super-resolves them to add detail, and distills those into a single,
high-fidelity implicit 3D representation, which also ensures view consistency
of the final renders. The super-resolution network is trained as an integral
part of HoloFusion, end-to-end, and the final distillation uses a new sampling
scheme to capture the space of super-resolved signals. We compare our method
against existing baselines, including DreamFusion, Get3D, EG3D, and
HoloDiffusion, and achieve, to the best of our knowledge, the most realistic
results on the challenging CO3Dv2 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023 conference; project page at:
  https://holodiffusion.github.io/holofusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Tensorformer: Normalized Matrix Attention Transformer for High-quality
  Point Cloud Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15989v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15989v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surface reconstruction from raw point clouds has been studied for decades in
the computer graphics community, which is highly demanded by modeling and
rendering applications nowadays. Classic solutions, such as Poisson surface
reconstruction, require point normals as extra input to perform reasonable
results. Modern transformer-based methods can work without normals, while the
results are less fine-grained due to limited encoding performance in local
fusion from discrete points. We introduce a novel normalized matrix attention
transformer (Tensorformer) to perform high-quality reconstruction. The proposed
matrix attention allows for simultaneous point-wise and channel-wise message
passing, while the previous vector attention loses neighbor point information
across different channels. It brings more degree of freedom in feature learning
and thus facilitates better modeling of local geometries. Our method achieves
state-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and
attains 4% improvements on IOU on ShapeNet. Code can be accessed
https://github.com/THHHomas/Tensorformer6.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> GINA-<span class="highlight-title">3D</span>: Learning to Generate Implicit Neural Assets in the Wild <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>o<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>D<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modeling the 3D world from sensor data for simulation is a scalable way of
developing testing and validation environments for robotic learning problems
such as autonomous driving. However, manually creating or re-creating
real-world-like environments is difficult, expensive, and not scalable. Recent
generative model techniques have shown promising progress to address such
challenges by learning 3D assets using only plentiful 2D images -- but still
suffer limitations as they leverage either human-curated image datasets or
renderings from manually-created synthetic 3D environments. In this paper, we
introduce GINA-3D, a generative model that uses real-world driving data from
camera and LiDAR sensors to create realistic 3D implicit neural assets of
diverse vehicles and pedestrians. Compared to the existing image datasets, the
real-world driving setting poses new challenges due to occlusions,
lighting-variations and long-tail distributions. GINA-3D tackles these
challenges by decoupling representation learning and generative modeling into
two stages with a learned tri-plane latent structure, inspired by recent
advances in generative modeling of images. To evaluate our approach, we
construct a large-scale object-centric dataset containing over 1.2M images of
vehicles and pedestrians from the Waymo Open Dataset, and a new set of 80K
images of long-tail instances such as construction equipment, garbage trucks,
and cable cars. We compare our model with existing approaches and demonstrate
that it achieves state-of-the-art performance in quality and diversity for both
generated images and geometries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023; Our WOD-ObjectAsset can be accessed through
  waymo.com/open</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Ego-Body Pose Estimation via Ego-Head Pose Estimation <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04636v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04636v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating 3D human motion from an egocentric video sequence plays a critical
role in human behavior understanding and has various applications in VR/AR.
However, naively learning a mapping between egocentric videos and human motions
is challenging, because the user's body is often unobserved by the front-facing
camera placed on the head of the user. In addition, collecting large-scale,
high-quality datasets with paired egocentric videos and 3D human motions
requires accurate motion capture devices, which often limit the variety of
scenes in the videos to lab-like environments. To eliminate the need for paired
egocentric video and human motions, we propose a new method, Ego-Body Pose
Estimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem
into two stages, connected by the head motion as an intermediate
representation. EgoEgo first integrates SLAM and a learning approach to
estimate accurate head motion. Subsequently, leveraging the estimated head pose
as input, EgoEgo utilizes conditional diffusion to generate multiple plausible
full-body motions. This disentanglement of head and body pose eliminates the
need for training datasets with paired egocentric videos and 3D human motion,
enabling us to leverage large-scale egocentric video datasets and motion
capture datasets separately. Moreover, for systematic benchmarking, we develop
a synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric
videos and human motion. On both ARES and real data, our EgoEgo model performs
significantly better than the current state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 (Award Candidate)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-08-27T00:00:00Z">2023-08-27</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Res2NetFuse: A Fusion Method for Infrared and Visible Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.14540v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.14540v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>-<span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel Res2Net-based fusion framework for infrared and
visible images. The proposed fusion model has three parts: an encoder, a fusion
layer and a decoder, respectively. The Res2Net-based encoder is used to extract
multi-scale features of source images, the paper introducing a new training
strategy for training a Res2Net-based encoder that uses only a single image.
Then, a new fusion strategy is developed based on the attention model. Finally,
the fused image is reconstructed by the decoder. The proposed approach is also
analyzed in detail. Experiments show that our method achieves state-of-the-art
fusion performance in objective and subjective assessment by comparing with the
existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are some errors that need to be corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> FFEINR: Flow Feature-Enhanced Implicit Neural Representation for
  Spatio-temporal Super-Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12508v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12508v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale numerical simulations are capable of generating data up to
terabytes or even petabytes. As a promising method of data reduction,
super-resolution (SR) has been widely studied in the scientific visualization
community. However, most of them are based on deep convolutional neural
networks (CNNs) or generative adversarial networks (GANs) and the scale factor
needs to be determined before constructing the network. As a result, a single
training session only supports a fixed factor and has poor generalization
ability. To address these problems, this paper proposes a Feature-Enhanced
Implicit Neural Representation (FFEINR) for spatio-temporal super-resolution of
flow field data. It can take full advantage of the implicit neural
representation in terms of model structure and sampling resolution. The neural
representation is based on a fully connected network with periodic activation
functions, which enables us to obtain lightweight models. The learned
continuous representation can decode the low-resolution flow field input data
to arbitrary spatial and temporal resolutions, allowing for flexible
upsampling. The training process of FFEINR is facilitated by introducing
feature enhancements for the input layer, which complements the contextual
information of the flow field. To demonstrate the effectiveness of the proposed
method, a series of experiments are conducted on different datasets by setting
different hyperparameters. The results show that FFEINR achieves significantly
better results than the trilinear interpolation method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted and published by ChinaVis
  2023(2023.7.21-24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Seal-<span class="highlight-title">3D</span>: Interactive Pixel-Level Editing for Neural Radiance Fields <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the popularity of implicit neural representations, or neural radiance
fields (NeRF), there is a pressing need for editing methods to interact with
the implicit 3D models for tasks like post-processing reconstructed scenes and
3D content creation. While previous works have explored NeRF editing from
various perspectives, they are restricted in editing flexibility, quality, and
speed, failing to offer direct editing response and instant preview. The key
challenge is to conceive a locally editable neural representation that can
directly reflect the editing instructions and update instantly. To bridge the
gap, we propose a new interactive editing method and system for implicit
representations, called Seal-3D, which allows users to edit NeRF models in a
pixel-level and free manner with a wide range of NeRF-like backbone and preview
the editing effects instantly. To achieve the effects, the challenges are
addressed by our proposed proxy function mapping the editing instructions to
the original space of NeRF models in the teacher model and a two-stage training
strategy for the student model with local pretraining and global finetuning. A
NeRF editing system is built to showcase various editing types. Our system can
achieve compelling editing effects with an interactive speed of about 1 second.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023. Project Page:
  https://windingwind.github.io/seal-3d/ Code:
  https://github.com/windingwind/seal-3d/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-08-26T00:00:00Z">2023-08-26</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Patch-Grid: An Efficient and Feature-Preserving Neural Implicit Surface
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>p<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit representations are known to be more compact for depicting 3D
shapes than traditional discrete representations. However, the neural
representations tend to round sharp corners or edges and struggle to represent
surfaces with open boundaries. Moreover, they are slow to train. We present a
unified neural implicit representation, called Patch-Grid, that fits to complex
shapes efficiently, preserves sharp features, and effectively models surfaces
with open boundaries and thin geometric features. Our superior efficiency comes
from embedding each surface patch into a local latent volume and decoding it
using a shared MLP decoder, which is pretrained on various local surface
geometries. With this pretrained decoder fixed, fitting novel shapes and local
shape updates can be done efficiently. The faithful preservation of sharp
features is enabled by adopting a novel merge grid to perform local
constructive solid geometry (CSG) combinations of surface patches in the cells
of an adaptive Octree, yielding better robustness than using a global CSG
construction as proposed in the literature. Experiments show that our
Patch-Grid method faithfully captures shapes with complex sharp features, open
boundaries and thin structures, and outperforms existing learning-based methods
in both efficiency and quality for surface fitting and local shape updates.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-08-25T00:00:00Z">2023-08-25</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Multi-Focus Querying of the Human Genome Information on Desktop and in
  Virtual Reality: an Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>I<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human genome is incredibly information-rich, consisting of approximately
25,000 protein-coding genes spread out over 3.2 billion nucleotide base pairs
contained within 24 unique chromosomes. The genome is important in maintaining
spatial context, which assists in understanding gene interactions and
relationships. However, existing methods of genome visualization that utilize
spatial awareness are inefficient and prone to limitations in presenting gene
information and spatial context. This study proposed an innovative approach to
genome visualization and exploration utilizing virtual reality. To determine
the optimal placement of gene information and evaluate its essentiality in a VR
environment, we implemented and conducted a user study with three different
interaction methods. Two interaction methods were developed in virtual reality
to determine if gene information is better suited to be embedded within the
chromosome ideogram or separate from the ideogram. The final ideogram
interaction method was performed on a desktop and served as a benchmark to
evaluate the potential benefits associated with the use of VR. Our study
findings reveal a preference for VR, despite longer task completion times. In
addition, the placement of gene information within the visualization had a
notable impact on the ability of a user to complete tasks. Specifically, gene
information embedded within the chromosome ideogram was better suited for
single target identification and summarization tasks, while separating gene
information from the ideogram better supported region comparison tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at ISMAR 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Relighting Neural Radiance Fields with Shadow and Highlight Hints <span class="chip">SIGGRAPH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel neural implicit radiance representation for free
viewpoint relighting from a small set of unstructured photographs of an object
lit by a moving point light source different from the view position. We express
the shape as a signed distance function modeled by a multi layer perceptron. In
contrast to prior relightable implicit neural representations, we do not
disentangle the different reflectance components, but model both the local and
global reflectance at each point by a second multi layer perceptron that, in
addition, to density features, the current position, the normal (from the
signed distace function), view direction, and light position, also takes shadow
and highlight hints to aid the network in modeling the corresponding high
frequency light transport effects. These hints are provided as a suggestion,
and we leave it up to the network to decide how to incorporate these in the
final relit result. We demonstrate and validate our neural implicit
representation on synthetic and real scenes exhibiting a wide variety of
shapes, material properties, and global illumination light transport.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH 2023. Author's version. Project page:
  https://nrhints.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Pose Modulated Avatars from Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>g<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is now possible to reconstruct dynamic human motion and shape from a
sparse set of cameras using Neural Radiance Fields (NeRF) driven by an
underlying skeleton. However, a challenge remains to model the deformation of
cloth and skin in relation to skeleton pose. Unlike existing avatar models that
are learned implicitly or rely on a proxy surface, our approach is motivated by
the observation that different poses necessitate unique frequency assignments.
Neglecting this distinction yields noisy artifacts in smooth areas or blurs
fine-grained texture and shape details in sharp regions. We develop a
two-branch neural network that is adaptive and explicit in the frequency
domain. The first branch is a graph neural network that models correlations
among body parts locally, taking skeleton pose as input. The second branch
combines these correlation features to a set of global frequencies and then
modulates the feature encoding. Our experiments demonstrate that our network
outperforms state-of-the-art methods in terms of preserving details and
generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Simulating the Geometric Growth of the Marine Sponge Crella Incrustans <span class="chip">IEEE VIS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.00474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.00474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>J<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>'<span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>w<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>m<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating marine sponge growth helps marine biologists analyze, measure, and
predict the effects that the marine environment has on marine sponges, and vice
versa. This paper describes a way to simulate and grow geometric models of the
marine sponge Crella incrustans while considering environmental factors
including fluid flow and nutrients. The simulation improves upon prior work by
changing the skeletal architecture of the sponge in the growth model to better
suit the structure of Crella incrustans. The change in skeletal architecture
and other simulation parameters are then evaluated qualitatively against photos
of a real-life Crella incrustans sponge. The results support the hypothesis
that changing the skeletal architecture from radiate accretive to Halichondrid
produces a sponge model which is closer in resemblance to Crella incrustans
than the prior work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 5 figures, IEEE VIS 2023, short paper, 9 supplementary
  figures, 1 supplementary table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Recovering <span class="highlight-title">3D</span> Human Mesh from Monocular Images: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01923v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01923v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating human pose and shape from monocular images is a long-standing
problem in computer vision. Since the release of statistical body models, 3D
human mesh recovery has been drawing broader attention. With the same goal of
obtaining well-aligned and physically plausible mesh results, two paradigms
have been developed to overcome challenges in the 2D-to-3D lifting process: i)
an optimization-based paradigm, where different data terms and regularization
terms are exploited as optimization objectives; and ii) a regression-based
paradigm, where deep learning techniques are embraced to solve the problem in
an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving
the quality of 3D mesh labels for a wide range of datasets. Though remarkable
progress has been achieved in the past decade, the task is still challenging
due to flexible body motions, diverse appearances, complex environments, and
insufficient in-the-wild annotations. To the best of our knowledge, this is the
first survey that focuses on the task of monocular 3D human mesh recovery. We
start with the introduction of body models and then elaborate recovery
frameworks and training objectives by providing in-depth analyses of their
strengths and weaknesses. We also summarize datasets, evaluation metrics, and
benchmark results. Open issues and future directions are discussed in the end,
hoping to motivate researchers and facilitate their research in this area. A
regularly updated project page can be found at
https://github.com/tinatiansjz/hmr-survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE TPAMI, Survey on monocular 3D human mesh recovery,
  Project page: https://github.com/tinatiansjz/hmr-survey</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-08-24T00:00:00Z">2023-08-24</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin
  Shell Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>c<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>t<span class="highlight-author"></span>, <span class="highlight-author"></span>V<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>l<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span> <span class="highlight-author"></span>G<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth simulation is an extensively studied problem, with a plethora of
solutions available in computer graphics literature. Existing cloth simulators
produce realistic cloth deformations that obey different types of boundary
conditions. Nevertheless, their operational principle remains limited in
several ways: They operate on explicit surface representations with a fixed
spatial resolution, perform a series of discretised updates (which bounds their
temporal resolution), and require comparably large amounts of storage.
Moreover, back-propagating gradients through the existing solvers is often not
straightforward, which poses additional challenges when integrating them into
modern neural architectures. In response to the limitations mentioned above,
this paper takes a fundamentally different perspective on physically-plausible
cloth simulation and re-thinks this long-standing problem: We propose
NeuralClothSim, i.e., a new cloth simulation approach using thin shells, in
which surface evolution is encoded in neural network weights. Our
memory-efficient and differentiable solver operates on a new continuous
coordinate-based representation of dynamic surfaces, i.e., neural deformation
fields (NDFs); it supervises NDF evolution with the rules of the non-linear
Kirchhoff-Love shell theory. NDFs are adaptive in the sense that they 1)
allocate their capacity to the deformation details as the latter arise during
the cloth evolution and 2) allow surface state queries at arbitrary spatial and
temporal resolutions without retraining. We show how to train our
NeuralClothSim solver while imposing hard boundary conditions and demonstrate
multiple applications, such as material interpolation and simulation editing.
The experimental results highlight the effectiveness of our formulation and its
potential impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 22 figures and 3 tables; project page:
  https://4dqv.mpi-inf.mpg.de/NeuralClothSim/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Dense Text-to-Image Generation with Attention Modulation <span class="chip">ICCV2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>H<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>-<span class="highlight-author"></span>W<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>-<span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing text-to-image diffusion models struggle to synthesize realistic
images given dense captions, where each text prompt provides a detailed
description for a specific image region. To address this, we propose
DenseDiffusion, a training-free method that adapts a pre-trained text-to-image
model to handle such dense captions while offering control over the scene
layout. We first analyze the relationship between generated images' layouts and
the pre-trained model's intermediate attention maps. Next, we develop an
attention modulation method that guides objects to appear in specific regions
according to layout guidance. Without requiring additional fine-tuning or
datasets, we improve image generation performance given dense captions
regarding both automatic and human evaluation scores. In addition, we achieve
similar-quality visual results with models specifically trained with layout
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV2023. Code and data are available at
  https://github.com/naver-ai/DenseDiffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Motion In-Betweening with Phase Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>P<span class="highlight-author"></span>a<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>b<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>o<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>c<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel data-driven motion in-betweening system to
reach target poses of characters by making use of phases variables learned by a
Periodic Autoencoder. Our approach utilizes a mixture-of-experts neural network
model, in which the phases cluster movements in both space and time with
different expert weights. Each generated set of weights then produces a
sequence of poses in an autoregressive manner between the current and target
state of the character. In addition, to satisfy poses which are manually
modified by the animators or where certain end effectors serve as constraints
to be reached by the animation, a learned bi-directional control scheme is
implemented to satisfy such constraints. The results demonstrate that using
phases for motion in-betweening tasks sharpen the interpolated movements, and
furthermore stabilizes the learning process. Moreover, using phases for motion
in-betweening tasks can also synthesize more challenging movements beyond
locomotion behaviors. Additionally, style control is enabled between given
target keyframes. Our proposed framework can compete with popular
state-of-the-art methods for motion in-betweening in terms of motion quality
and generalization, especially in the existence of long transition durations.
Our framework contributes to faster prototyping workflows for creating animated
character sequences, which is of enormous interest for the game and film
industry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> The GENEA Challenge 2023: A large scale evaluation of gesture generation
  models in monadic and dyadic settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>u<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>j<span class="highlight-author"></span>m<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>a<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>w<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>d<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>N<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>l<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>s<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>o<span class="highlight-author"></span>v<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span> <span class="highlight-author"></span>E<span class="highlight-author"></span>j<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reports on the GENEA Challenge 2023, in which participating teams
built speech-driven gesture-generation systems using the same speech and motion
dataset, followed by a joint evaluation. This year's challenge provided data on
both sides of a dyadic interaction, allowing teams to generate full-body motion
for an agent given its speech (text and audio) and the speech and motion of the
interlocutor. We evaluated 12 submissions and 2 baselines together with
held-out motion-capture data in several large-scale user studies. The studies
focused on three aspects: 1) the human-likeness of the motion, 2) the
appropriateness of the motion for the agent's own speech whilst controlling for
the human-likeness of the motion, and 3) the appropriateness of the motion for
the behaviour of the interlocutor in the interaction, using a setup that
controls for both the human-likeness of the motion and the agent's own speech.
We found a large span in human-likeness between challenge submissions, with a
few systems rated close to human mocap. Appropriateness seems far from being
solved, with most submissions performing in a narrow range slightly above
chance, far behind natural motion. The effect of the interlocutor is even more
subtle, with submitted systems at best performing barely above chance.
Interestingly, a dyadic system being highly appropriate for agent speech does
not necessarily imply high appropriateness for the interlocutor. Additional
material is available via the project website at
https://svito-zar.github.io/GENEAchallenge2023/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors made equal contributions. Accepted for
  publication at the ACM International Conference on Multimodal Interaction
  (ICMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Sce<span class="highlight-title">neRF</span>: Self-Supervised Monocular <span class="highlight-title">3D</span> Scene Reconstruction with Radiance
  Fields <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02501v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02501v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>A<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>-<span class="highlight-author"></span>Q<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D reconstruction from a single 2D image was extensively covered in the
literature but relies on depth supervision at training time, which limits its
applicability. To relax the dependence to depth we propose SceneRF, a
self-supervised monocular scene reconstruction method using only posed image
sequences for training. Fueled by the recent progress in neural radiance fields
(NeRF) we optimize a radiance field though with explicit depth optimization and
a novel probabilistic sampling strategy to efficiently handle large scenes. At
inference, a single input image suffices to hallucinate novel depth views which
are fused together to obtain 3D scene reconstruction. Thorough experiments
demonstrate that we outperform all baselines for novel depth views synthesis
and scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI.
Code is available at https://astra-vision.github.io/SceneRF .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023. Project page: https://astra-vision.github.io/SceneRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> MIPS-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online
  Neural RGB-D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.08741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.08741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MIPS-Fusion, a robust and scalable online RGB-D reconstruction
method based on a novel neural implicit representation --
multi-implicit-submap. Different from existing neural RGB-D reconstruction
methods lacking either flexibility with a single neural map or scalability due
to extra storage of feature grids, we propose a pure neural representation
tackling both difficulties with a divide-and-conquer design. In our method,
neural submaps are incrementally allocated alongside the scanning trajectory
and efficiently learned with local neural bundle adjustments. The submaps can
be refined individually in a back-end optimization and optimized jointly to
realize submap-level loop closure. Meanwhile, we propose a hybrid tracking
approach combining randomized and gradient-based pose optimizations. For the
first time, randomized optimization is made possible in neural tracking with
several key designs to the learning process, enabling efficient and robust
tracking even under fast camera motions. The extensive evaluation demonstrates
that our method attains higher reconstruction quality than the state of the
arts for large-scale scenes and under fast camera motions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> A Visualization System for Hexahedral Mesh Quality Study <span class="chip">IEEE VIS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12158v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12158v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new 3D hex mesh visual analysis system that
emphasizes poor-quality areas with an aggregated glyph, highlights overlapping
elements, and provides detailed boundary error inspection in three forms. By
supporting multi-level analysis through multiple views, our system effectively
evaluates various mesh models and compares the performance of mesh generation
and optimization algorithms for hexahedral meshes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE VIS 2023 Short Papers and will be published on IEEE
  Xplore. Paper contains 4 pages, and 1 reference page. Supplemental includes 4
  pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>y<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Convolutional Networks (GCNs) can capture non-Euclidean spatial
dependence between different brain regions, and the graph pooling operator in
GCNs is key to enhancing the representation learning capability and acquiring
abnormal brain maps. However, the majority of existing research designs graph
pooling operators only from the perspective of nodes while disregarding the
original edge features, in a way that not only confines graph pooling
application scenarios, but also diminishes its ability to capture critical
substructures. In this study, a clustering graph pooling method that first
supports multidimensional edge features, called Edge-aware hard clustering
graph pooling (EHCPool), is developed. EHCPool proposes the first
'Edge-to-node' score evaluation criterion based on edge features to assess node
feature significance. To more effectively capture the critical subgraphs, a
novel Iteration n-top strategy is further designed to adaptively learn sparse
hard clustering assignments for graphs. Subsequently, an innovative N-E
Aggregation strategy is presented to aggregate node and edge feature
information in each independent subgraph. The proposed model was evaluated on
multi-site brain imaging public datasets and yielded state-of-the-art
performance. We believe this method is the first deep learning tool with the
potential to probe different types of abnormal functional brain networks from
data-driven perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Alternately denoising and reconstructing unoriented point sets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00391v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00391v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>Z<span class="highlight-author"></span>u<span class="highlight-author"></span>o<span class="highlight-author"></span>q<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new strategy to bridge point cloud denoising and surface
reconstruction by alternately updating the denoised point clouds and the
reconstructed surfaces. In Poisson surface reconstruction, the implicit
function is generated by a set of smooth basis functions centered at the
octnodes. When the octree depth is properly selected, the reconstructed surface
is a good smooth approximation of the noisy point set. Our method projects the
noisy points onto the surface and alternately reconstructs and projects the
point set. We use the iterative Poisson surface reconstruction (iPSR) to
support unoriented surface reconstruction. Our method iteratively performs iPSR
and acts as an outer loop of iPSR. Considering that the octree depth
significantly affects the reconstruction results, we propose an adaptive depth
selection strategy to ensure an appropriate depth choice. To manage the
oversmoothing phenomenon near the sharp features, we propose a
$\lambda$-projection method, which means to project the noisy points onto the
surface with an individual control coefficient $\lambda_{i}$ for each point.
The coefficients are determined through a Voronoi-based feature detection
method. Experimental results show that our method achieves high performance in
point cloud denoising and unoriented surface reconstruction within different
noise scales, and exhibits well-rounded performance in various types of inputs.
The source code is available
at~\url{https://github.com/Submanifold/AlterUpdate}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Computers & Graphics from CAD/Graphics 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Patternshop: Editing Point Patterns by Image Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.10517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.10517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>X<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>o<span class="highlight-author"></span>b<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>s<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>-<span class="highlight-author"></span>P<span class="highlight-author"></span>e<span class="highlight-author"></span>t<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>i<span class="highlight-author"></span>d<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>, <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>e<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>G<span class="highlight-author"></span>u<span class="highlight-author"></span>r<span class="highlight-author"></span>p<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point patterns are characterized by their density and correlation. While
spatial variation of density is well-understood, analysis and synthesis of
spatially-varying correlation is an open challenge. No tools are available to
intuitively edit such point patterns, primarily due to the lack of a compact
representation for spatially varying correlation. We propose a low-dimensional
perceptual embedding for point correlations. This embedding can map point
patterns to common three-channel raster images, enabling manipulation with
off-the-shelf image editing software. To synthesize back point patterns, we
propose a novel edge-aware objective that carefully handles sharp variations in
density and correlation. The resulting framework allows intuitive and
backward-compatible manipulation of point patterns, such as recoloring,
relighting to even texture synthesis that have not been available to 2D point
pattern design before. Effectiveness of our approach is tested in several user
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Neural Fourier Filter Bank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01735v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01735v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>K<span class="highlight-author"></span>w<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-08-23T00:00:00Z">2023-08-23</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Graphics <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for
  <span class="highlight-title">3D</span> Scene Stylization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12452v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12452v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>W<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>c<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span> <span class="highlight-author"></span>O<span class="highlight-author"></span>z<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>r<span class="highlight-author"></span>e<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The radiance fields style transfer is an emerging field that has recently
gained popularity as a means of 3D scene stylization, thanks to the outstanding
performance of neural radiance fields in 3D reconstruction and view synthesis.
We highlight a research gap in radiance fields style transfer, the lack of
sufficient perceptual controllability, motivated by the existing concept in the
2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style
transfer framework offering manageable control over perceptual factors, to
systematically explore the perceptual controllability in 3D scene stylization.
Four distinct types of controls - color preservation control, (style pattern)
scale control, spatial (selective stylization area) control, and depth
enhancement control - are proposed and integrated into this framework. Results
from real-world datasets, both quantitative and qualitative, show that the four
types of controls in our ARF-Plus framework successfully accomplish their
corresponding perceptual controls when stylizing 3D scenes. These techniques
work well for individual style inputs as well as for the simultaneous
application of multiple styles within a scene. This unlocks a realm of
limitless possibilities, allowing customized modifications of stylization
effects and flexible merging of the strengths of different styles, ultimately
enabling the creation of novel and eye-catching stylistic effects on 3D scenes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Dance with You: The Diversity Controllable Dancer Generation via
  Diffusion Models <span class="chip">ACM MM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.13551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.13551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>S<span class="highlight-author"></span>i<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>j<span class="highlight-author"></span>i<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>l<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, digital humans for interpersonal interaction in virtual
environments have gained significant attention. In this paper, we introduce a
novel multi-dancer synthesis task called partner dancer generation, which
involves synthesizing virtual human dancers capable of performing dance with
users. The task aims to control the pose diversity between the lead dancer and
the partner dancer. The core of this task is to ensure the controllable
diversity of the generated partner dancer while maintaining temporal
coordination with the lead dancer. This scenario varies from earlier research
in generating dance motions driven by music, as our emphasis is on
automatically designing partner dancer postures according to pre-defined
diversity, the pose of lead dancer, as well as the accompanying tunes. To
achieve this objective, we propose a three-stage framework called
Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to
collect a wide range of basic dance poses as references for motion generation.
Then, we introduce a hyper-parameter that coordinates the similarity between
dancers by masking poses to prevent the generation of sequences that are
over-diverse or consistent. To avoid the rigidity of movements, we design a
Dance Pre-generated stage to pre-generate these masked poses instead of filling
them with zeros. After that, a Dance Motion Transfer stage is adopted with
leader sequences and music, in which a multi-conditional sampling formula is
rewritten to transfer the pre-generated poses into a sequence with a partner
style. In practice, to address the lack of multi-person datasets, we introduce
AIST-M, a new dataset for partner dancer generation, which is publicly
availiable. Comprehensive evaluations on our AIST-M dataset demonstrate that
the proposed DanY can synthesize satisfactory partner dancer results with
controllable diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACM MM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Blending-<span class="highlight-title">NeRF</span>: Text-Driven Localized Editing in Neural Radiance Fields <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>H<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>p<span class="highlight-author"></span> <span class="highlight-author"></span>S<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>o<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>s<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>D<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>l<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>e<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>e<span class="highlight-author"></span>h<span class="highlight-author"></span>y<span class="highlight-author"></span>e<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>K<span class="highlight-author"></span>i<span class="highlight-author"></span>m<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-driven localized editing of 3D objects is particularly difficult as
locally mixing the original 3D object with the intended new object and style
effects without distorting the object's form is not a straightforward process.
To address this issue, we propose a novel NeRF-based model, Blending-NeRF,
which consists of two NeRF networks: pretrained NeRF and editable NeRF.
Additionally, we introduce new blending operations that allow Blending-NeRF to
properly edit target regions which are localized by text. By using a pretrained
vision-language aligned model, CLIP, we guide Blending-NeRF to add new objects
with varying colors and densities, modify textures, and remove parts of the
original object. Our extensive experiments demonstrate that Blending-NeRF
produces naturally and locally edited 3D objects from various text prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023. The first two authors contributed equally to
  this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Factorized Inverse Path Tracing for Efficient and Accurate
  Material-Lighting Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>w<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>u<span class="highlight-author"></span>s<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>f<span class="highlight-author"></span>a<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>.<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>d<span class="highlight-author"></span>i<span class="highlight-author"></span>z<span class="highlight-author"></span>, <span class="highlight-author"></span>Y<span class="highlight-author"></span>i<span class="highlight-author"></span>n<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>o<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>r<span class="highlight-author"></span>b<span class="highlight-author"></span>e<span class="highlight-author"></span>k<span class="highlight-author"></span> <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>F<span class="highlight-author"></span>a<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>h<span class="highlight-author"></span> <span class="highlight-author"></span>P<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>i<span class="highlight-author"></span>k<span class="highlight-author"></span>l<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>z<span class="highlight-author"></span>u<span class="highlight-author"></span>-<span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>M<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>d<span class="highlight-author"></span>r<span class="highlight-author"></span>a<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span>r<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>v<span class="highlight-author"></span>i<span class="highlight-author"></span> <span class="highlight-author"></span>R<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>a<span class="highlight-author"></span>m<span class="highlight-author"></span>o<span class="highlight-author"></span>o<span class="highlight-author"></span>r<span class="highlight-author"></span>t<span class="highlight-author"></span>h<span class="highlight-author"></span>i<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse path tracing has recently been applied to joint material and lighting
estimation, given geometry and multi-view HDR observations of an indoor scene.
However, it has two major limitations: path tracing is expensive to compute,
and ambiguities exist between reflection and emission. Our Factorized Inverse
Path Tracing (FIPT) addresses these challenges by using a factored light
transport formulation and finds emitters driven by rendering errors. Our
algorithm enables accurate material and lighting optimization faster than
previous work, and is more effective at resolving ambiguities. The exhaustive
experiments on synthetic scenes show that our method (1) outperforms
state-of-the-art indoor inverse rendering and relighting methods particularly
in the presence of complex illumination effects; (2) speeds up inverse path
tracing optimization to less than an hour. We further demonstrate robustness to
noisy inputs through material and lighting estimates that allow plausible
relighting in a real scene. The source code is available at:
https://github.com/lwwu2/fipt
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated experiment results; modified real-world sections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> HiFace: High-Fidelity <span class="highlight-title">3D</span> Face Reconstruction by Learning Static and
  Dynamic Details <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11225v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11225v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        <span class="highlight-author"></span>Z<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>k<span class="highlight-author"></span>e<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>y<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>H<span class="highlight-author"></span>e<span class="highlight-author"></span>, <span class="highlight-author"></span>X<span class="highlight-author"></span>u<span class="highlight-author"></span> <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>d<span class="highlight-author"></span>a<span class="highlight-author"></span>s<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>a<span class="highlight-author"></span>l<span class="highlight-author"></span>t<span class="highlight-author"></span>r<span class="highlight-author"></span>u<span class="highlight-author"></span>š<span class="highlight-author"></span>a<span class="highlight-author"></span>i<span class="highlight-author"></span>t<span class="highlight-author"></span>i<span class="highlight-author"></span>s<span class="highlight-author"></span>, <span class="highlight-author"></span>H<span class="highlight-author"></span>s<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span>T<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span> <span class="highlight-author"></span>W<span class="highlight-author"></span>u<span class="highlight-author"></span>, <span class="highlight-author"></span>R<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span>n<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>L<span class="highlight-author"></span>i<span class="highlight-author"></span>, <span class="highlight-author"></span>S<span class="highlight-author"></span>h<span class="highlight-author"></span>e<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>Z<span class="highlight-author"></span>h<span class="highlight-author"></span>a<span class="highlight-author"></span>o<span class="highlight-author"></span>, <span class="highlight-author"></span>C<span class="highlight-author"></span>h<span class="highlight-author"></span>u<span class="highlight-author"></span>n<span class="highlight-author"></span> <span class="highlight-author"></span>Y<span class="highlight-author"></span>u<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>, <span class="highlight-author"></span>J<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>g<span class="highlight-author"></span> <span class="highlight-author"></span>B<span class="highlight-author"></span>i<span class="highlight-author"></span>a<span class="highlight-author"></span>n<span class="highlight-author"></span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Morphable Models (3DMMs) demonstrate great potential for reconstructing
faithful and animatable 3D facial surfaces from a single image. The facial
surface is influenced by the coarse shape, as well as the static detail (e,g.,
person-specific appearance) and dynamic detail (e.g., expression-driven
wrinkles). Previous work struggles to decouple the static and dynamic details
through image-level supervision, leading to reconstructions that are not
realistic. In this paper, we aim at high-fidelity 3D face reconstruction and
propose HiFace to explicitly model the static and dynamic details.
Specifically, the static detail is modeled as the linear combination of a
displacement basis, while the dynamic detail is modeled as the linear
interpolation of two displacement maps with polarized expressions. We exploit
several loss functions to jointly learn the coarse shape and fine details with
both synthetic and real-world datasets, which enable HiFace to reconstruct
high-fidelity 3D shapes with animatable details. Extensive quantitative and
qualitative experiments demonstrate that HiFace presents state-of-the-art
reconstruction quality and faithfully recovers both the static and dynamic
details. Our project page can be found at https://project-hiface.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICCV 2023, camera-ready version; Project page:
  https://project-hiface.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-08-31T05:25:30.260335983Z">
            2023-08-31 05:25:30 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
