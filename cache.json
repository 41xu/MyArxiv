{"2023-08-30T00:00:00Z":{"Graphics":[{"id":"http://arxiv.org/abs/2308.16084v1","updated":"2023-08-30T15:16:56Z","published":"2023-08-30T15:16:56Z","title":"P2M: A Fast Solver for Querying Distance from Point to Mesh Surface","summary":"  Most of the existing point-to-mesh distance query solvers, such as Proximity\nQuery Package (PQP), Embree and Fast Closest Point Query (FCPW), are based on\nbounding volume hierarchy (BVH). The hierarchical organizational structure\nenables one to eliminate the vast majority of triangles that do not help find\nthe closest point. In this paper, we develop a totally different algorithmic\nparadigm, named P2M, to speed up point-to-mesh distance queries. Our original\nintention is to precompute a KD tree (KDT) of mesh vertices to approximately\nencode the geometry of a mesh surface containing vertices, edges and faces.\nHowever, it is very likely that the closest primitive to the query point is an\nedge e (resp., a face f), but the KDT reports a mesh vertex \\u{psion} instead.\nWe call \\u{psion} an interceptor of e (resp., f). The main contribution of this\npaper is to invent a simple yet effective interception inspection rule and an\nefficient flooding interception inspection algorithm for quickly finding out\nall the interception pairs. Once the KDT and the interception table are\nprecomputed, the query stage proceeds by first searching the KDT and then\nlooking up the interception table to retrieve the closest geometric primitive.\nStatistics show that our query algorithm runs many times faster than the\nstate-of-the-art solvers.\n","authors":["Chen Zong","Jiacheng Xu","Jiantao Song","Shuangmin Chen","Shiqing Xin","Wenping Wang","Changhe Tu"],"pdf_url":"https://arxiv.org/pdf/2308.16084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15920v1","updated":"2023-08-30T09:53:35Z","published":"2023-08-30T09:53:35Z","title":"Saving temporary exhibitions in virtual environments: the Digital\n  Renaissance of Ulisse Aldrovandi","summary":"  Our goal was to obtain the digital twin of the temporary exhibition \"The\nOther Renaissance: Ulisse Aldrovandi and the Wonders of the World\", to make it\naccessible online to users using various devices (from smartphones to VR\nheadsets). We started with a preliminary assessment of the exhibition,\nfocussing on possible acquisition constraints - time, space, and materials -\nand related solutions. Then, we proceeded with the creation of the digital twin\nby acquiring, processing, modelling, optimising, exporting, metadating, and\nuploading the exhibition. We adopted a hybrid use of two distinct acquisition\ntechniques, i.e. structured light projection scanning and photogrammetry, to\ncreate new digital cultural heritage objects and environments, and we used open\ntechnologies, formats and protocols to make available the final digital\nproduct. We described the process to collect and curate bibliographical\n(meta)data of the exhibition and digital twin creation process to foster its\nfindability, accessibility, interoperability and reusability.\n","authors":["Roberto Balzani","Sebastian Barzaghi","Gabriele Bitelli","Federica Bonifazi","Alice Bordignon","Luca Cipriani","Simona Colitti","Federica Collina","Marilena Daquino","Francesca Fabbri","Bruno Fanini","Filippo Fantini","Daniele Ferdani","Giulia Fiorini","Elena Formia","Anna Forte","Federica Giacomini","Valentina Alena Girelli","Bianca Gualandi","Ivan Heibi","Alessandro Iannucci","Rachele Manganelli Del Fà","Arcangelo Massari","Arianna Moretti","Silvio Peroni","Sofia Pescarin","Giulia Renda","Diego Ronchi","Mattia Sullini","Maria Alessandra Tini","Francesca Tomasi","Laura Travaglini","Luca Vittuari"],"pdf_url":"https://arxiv.org/pdf/2308.15920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01418v3","updated":"2023-08-30T04:41:10Z","published":"2023-03-02T17:09:27Z","title":"Human Motion Diffusion as a Generative Prior","summary":"  Recent work has demonstrated the significant potential of denoising diffusion\nmodels for generating human motion, including text-to-motion capabilities.\nHowever, these methods are restricted by the paucity of annotated motion data,\na focus on single-person motions, and a lack of detailed control. In this\npaper, we introduce three forms of composition based on diffusion priors:\nsequential, parallel, and model composition. Using sequential composition, we\ntackle the challenge of long sequence generation. We introduce DoubleTake, an\ninference-time method with which we generate long animations consisting of\nsequences of prompted intervals and their transitions, using a prior trained\nonly for short clips. Using parallel composition, we show promising steps\ntoward two-person generation. Beginning with two fixed priors as well as a few\ntwo-person training examples, we learn a slim communication block, ComMDM, to\ncoordinate interaction between the two resulting motions. Lastly, using model\ncomposition, we first train individual priors to complete motions that realize\na prescribed motion for a given joint. We then introduce DiffusionBlending, an\ninterpolation mechanism to effectively blend several such models to enable\nflexible and efficient fine-grained joint and trajectory-level control and\nediting. We evaluate the composition methods using an off-the-shelf motion\ndiffusion model, and further compare the results to dedicated models trained\nfor these specific tasks.\n","authors":["Yonatan Shafir","Guy Tevet","Roy Kapon","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2303.01418v3.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2308.16187v1","updated":"2023-08-30T17:59:11Z","published":"2023-08-30T17:59:11Z","title":"Boosting Detection in Crowd Analysis via Underutilized Output Features","summary":"  Detection-based methods have been viewed unfavorably in crowd analysis due to\ntheir poor performance in dense crowds. However, we argue that the potential of\nthese methods has been underestimated, as they offer crucial information for\ncrowd analysis that is often ignored. Specifically, the area size and\nconfidence score of output proposals and bounding boxes provide insight into\nthe scale and density of the crowd. To leverage these underutilized features,\nwe propose Crowd Hat, a plug-and-play module that can be easily integrated with\nexisting detection models. This module uses a mixed 2D-1D compression technique\nto refine the output features and obtain the spatial and numerical distribution\nof crowd-specific information. Based on these features, we further propose\nregion-adaptive NMS thresholds and a decouple-then-align paradigm that address\nthe major limitations of detection-based methods. Our extensive evaluations on\nvarious crowd analysis tasks, including crowd counting, localization, and\ndetection, demonstrate the effectiveness of utilizing output features and the\npotential of detection-based methods in crowd analysis.\n","authors":["Shaokai Wu","Fengyu Yang"],"pdf_url":"https://arxiv.org/pdf/2308.16187v1.pdf","comment":"project page: https://fredfyyang.github.io/Crowd-Hat/"},{"id":"http://arxiv.org/abs/2308.16184v1","updated":"2023-08-30T17:59:02Z","published":"2023-08-30T17:59:02Z","title":"SAM-Med2D","summary":"  The Segment Anything Model (SAM) represents a state-of-the-art research\nadvancement in natural image segmentation, achieving impressive results with\ninput prompts such as points and bounding boxes. However, our evaluation and\nrecent research indicate that directly applying the pretrained SAM to medical\nimage segmentation does not yield satisfactory performance. This limitation\nprimarily arises from significant domain gap between natural images and medical\nimages. To bridge this gap, we introduce SAM-Med2D, the most comprehensive\nstudies on applying SAM to medical 2D images. Specifically, we first collect\nand curate approximately 4.6M images and 19.7M masks from public and private\ndatasets, constructing a large-scale medical image segmentation dataset\nencompassing various modalities and objects. Then, we comprehensively fine-tune\nSAM on this dataset and turn it into SAM-Med2D. Unlike previous methods that\nonly adopt bounding box or point prompts as interactive segmentation approach,\nwe adapt SAM to medical image segmentation through more comprehensive prompts\ninvolving bounding boxes, points, and masks. We additionally fine-tune the\nencoder and decoder of the original SAM to obtain a well-performed SAM-Med2D,\nleading to the most comprehensive fine-tuning strategies to date. Finally, we\nconducted a comprehensive evaluation and analysis to investigate the\nperformance of SAM-Med2D in medical image segmentation across various\nmodalities, anatomical structures, and organs. Concurrently, we validated the\ngeneralization capability of SAM-Med2D on 9 datasets from MICCAI 2023\nchallenge. Overall, our approach demonstrated significantly superior\nperformance and generalization capability compared to SAM.\n","authors":["Junlong Cheng","Jin Ye","Zhongying Deng","Jianpin Chen","Tianbin Li","Haoyu Wang","Yanzhou Su","Ziyan Huang","Jilong Chen","Lei Jiang","Hui Sun","Junjun He","Shaoting Zhang","Min Zhu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2308.16184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16182v1","updated":"2023-08-30T17:58:50Z","published":"2023-08-30T17:58:50Z","title":"GREC: Generalized Referring Expression Comprehension","summary":"  The objective of Classic Referring Expression Comprehension (REC) is to\nproduce a bounding box corresponding to the object mentioned in a given textual\ndescription. Commonly, existing datasets and techniques in classic REC are\ntailored for expressions that pertain to a single target, meaning a sole\nexpression is linked to one specific object. Expressions that refer to multiple\ntargets or involve no specific target have not been taken into account. This\nconstraint hinders the practical applicability of REC. This study introduces a\nnew benchmark termed as Generalized Referring Expression Comprehension (GREC).\nThis benchmark extends the classic REC by permitting expressions to describe\nany number of target objects. To achieve this goal, we have built the first\nlarge-scale GREC dataset named gRefCOCO. This dataset encompasses a range of\nexpressions: those referring to multiple targets, expressions with no specific\ntarget, and the single-target expressions. The design of GREC and gRefCOCO\nensures smooth compatibility with classic REC. The proposed gRefCOCO dataset, a\nGREC method implementation code, and GREC evaluation code are available at\nhttps://github.com/henghuiding/gRefCOCO.\n","authors":["Shuting He","Henghui Ding","Chang Liu","Xudong Jiang"],"pdf_url":"https://arxiv.org/pdf/2308.16182v1.pdf","comment":"GREC Technical Report, Project Page:\n  https://henghuiding.github.io/GRES"},{"id":"http://arxiv.org/abs/2303.17590v2","updated":"2023-08-30T17:46:17Z","published":"2023-03-30T17:57:43Z","title":"Going Beyond Nouns With Vision & Language Models Using Synthetic Data","summary":"  Large-scale pre-trained Vision & Language (VL) models have shown remarkable\nperformance in many applications, enabling replacing a fixed set of supported\nclasses with zero-shot open vocabulary reasoning over (almost arbitrary)\nnatural language prompts. However, recent works have uncovered a fundamental\nweakness of these models. For example, their difficulty to understand Visual\nLanguage Concepts (VLC) that go 'beyond nouns' such as the meaning of\nnon-object words (e.g., attributes, actions, relations, states, etc.), or\ndifficulty in performing compositional reasoning such as understanding the\nsignificance of the order of the words in a sentence. In this work, we\ninvestigate to which extent purely synthetic data could be leveraged to teach\nthese models to overcome such shortcomings without compromising their zero-shot\ncapabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale\nsynthetic dataset and data generation codebase allowing to generate additional\nsuitable data to improve VLC understanding and compositional reasoning of VL\nmodels. Additionally, we propose a general VL finetuning strategy for\neffectively leveraging SyViC towards achieving these improvements. Our\nextensive experiments and ablations on VL-Checklist, Winoground, and ARO\nbenchmarks demonstrate that it is possible to adapt strong pre-trained VL\nmodels with synthetic data significantly enhancing their VLC understanding\n(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in their\nzero-shot accuracy.\n","authors":["Paola Cascante-Bonilla","Khaled Shehada","James Seale Smith","Sivan Doveh","Donghyun Kim","Rameswar Panda","Gül Varol","Aude Oliva","Vicente Ordonez","Rogerio Feris","Leonid Karlinsky"],"pdf_url":"https://arxiv.org/pdf/2303.17590v2.pdf","comment":"Accepted to ICCV 2023. Project page: https://synthetic-vic.github.io/"},{"id":"http://arxiv.org/abs/2308.16154v1","updated":"2023-08-30T17:20:46Z","published":"2023-08-30T17:20:46Z","title":"MMVP: Motion-Matrix-based Video Prediction","summary":"  A central challenge of video prediction lies where the system has to reason\nthe objects' future motions from image frames while simultaneously maintaining\nthe consistency of their appearances across frames. This work introduces an\nend-to-end trainable two-stream video prediction framework, Motion-Matrix-based\nVideo Prediction (MMVP), to tackle this challenge. Unlike previous methods that\nusually handle motion prediction and appearance maintenance within the same set\nof modules, MMVP decouples motion and appearance information by constructing\nappearance-agnostic motion matrices. The motion matrices represent the temporal\nsimilarity of each and every pair of feature patches in the input frames, and\nare the sole input of the motion prediction module in MMVP. This design\nimproves video prediction in both accuracy and efficiency, and reduces the\nmodel size. Results of extensive experiments demonstrate that MMVP outperforms\nstate-of-the-art systems on public data sets by non-negligible large margins\n(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% the\nsize or smaller). Please refer to\nhttps://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for the\nofficial code and the datasets used in this paper.\n","authors":["Yiqi Zhong","Luming Liang","Ilya Zharkov","Ulrich Neumann"],"pdf_url":"https://arxiv.org/pdf/2308.16154v1.pdf","comment":"ICCV 2023 (Oral)"},{"id":"http://arxiv.org/abs/2308.16150v1","updated":"2023-08-30T17:16:02Z","published":"2023-08-30T17:16:02Z","title":"Modality Cycles with Masked Conditional Diffusion for Unsupervised\n  Anomaly Segmentation in MRI","summary":"  Unsupervised anomaly segmentation aims to detect patterns that are distinct\nfrom any patterns processed during training, commonly called abnormal or\nout-of-distribution patterns, without providing any associated manual\nsegmentations. Since anomalies during deployment can lead to model failure,\ndetecting the anomaly can enhance the reliability of models, which is valuable\nin high-risk domains like medical imaging. This paper introduces Masked\nModality Cycles with Conditional Diffusion (MMCCD), a method that enables\nsegmentation of anomalies across diverse patterns in multimodal MRI. The method\nis based on two fundamental ideas. First, we propose the use of cyclic modality\ntranslation as a mechanism for enabling abnormality detection.\nImage-translation models learn tissue-specific modality mappings, which are\ncharacteristic of tissue physiology. Thus, these learned mappings fail to\ntranslate tissues or image patterns that have never been encountered during\ntraining, and the error enables their segmentation. Furthermore, we combine\nimage translation with a masked conditional diffusion model, which attempts to\n`imagine' what tissue exists under a masked area, further exposing unknown\npatterns as the generative model fails to recreate them. We evaluate our method\non a proxy task by training on healthy-looking slices of BraTS2021\nmulti-modality MRIs and testing on slices with tumors. We show that our method\ncompares favorably to previous unsupervised approaches based on image\nreconstruction and denoising with autoencoders and diffusion models.\n","authors":["Ziyun Liang","Harry Anthony","Felix Wagner","Konstantinos Kamnitsas"],"pdf_url":"https://arxiv.org/pdf/2308.16150v1.pdf","comment":"Accepted in Multiscale Multimodal Medical Imaging workshop in MICCAI\n  2023"},{"id":"http://arxiv.org/abs/2308.01981v2","updated":"2023-08-30T17:02:55Z","published":"2023-08-03T18:28:50Z","title":"CartiMorph: a framework for automated knee articular cartilage\n  morphometrics","summary":"  We introduce CartiMorph, a framework for automated knee articular cartilage\nmorphometrics. It takes an image as input and generates quantitative metrics\nfor cartilage subregions, including the percentage of full-thickness cartilage\nloss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the\npower of deep learning models for hierarchical image feature representation.\nDeep learning models were trained and validated for tissue segmentation,\ntemplate construction, and template-to-image registration. We established\nmethods for surface-normal-based cartilage thickness mapping, FCL estimation,\nand rule-based cartilage parcellation. Our cartilage thickness map showed less\nerror in thin and peripheral regions. We evaluated the effectiveness of the\nadopted segmentation model by comparing the quantitative metrics obtained from\nmodel segmentation and those from manual segmentation. The root-mean-squared\ndeviation of the FCL measurements was less than 8%, and strong correlations\nwere observed for the mean thickness (Pearson's correlation coefficient $\\rho\n\\in [0.82,0.97]$), surface area ($\\rho \\in [0.82,0.98]$) and volume ($\\rho \\in\n[0.89,0.98]$) measurements. We compared our FCL measurements with those from a\nprevious study and found that our measurements deviated less from the ground\ntruths. We observed superior performance of the proposed rule-based cartilage\nparcellation method compared with the atlas-based approach. CartiMorph has the\npotential to promote imaging biomarkers discovery for knee osteoarthritis.\n","authors":["Yongcheng Yao","Junru Zhong","Liping Zhang","Sheheryar Khan","Weitian Chen"],"pdf_url":"https://arxiv.org/pdf/2308.01981v2.pdf","comment":"To be published in Medical Image Analysis"},{"id":"http://arxiv.org/abs/2308.16145v1","updated":"2023-08-30T17:01:01Z","published":"2023-08-30T17:01:01Z","title":"CircleFormer: Circular Nuclei Detection in Whole Slide Images with\n  Circle Queries and Attention","summary":"  Both CNN-based and Transformer-based object detection with bounding box\nrepresentation have been extensively studied in computer vision and medical\nimage analysis, but circular object detection in medical images is still\nunderexplored. Inspired by the recent anchor free CNN-based circular object\ndetection method (CircleNet) for ball-shape glomeruli detection in renal\npathology, in this paper, we present CircleFormer, a Transformer-based circular\nmedical object detection with dynamic anchor circles. Specifically, queries\nwith circle representation in Transformer decoder iteratively refine the\ncircular object detection results, and a circle cross attention module is\nintroduced to compute the similarity between circular queries and image\nfeatures. A generalized circle IoU (gCIoU) is proposed to serve as a new\nregression loss of circular object detection as well. Moreover, our approach is\neasy to generalize to the segmentation task by adding a simple segmentation\nbranch to CircleFormer. We evaluate our method in circular nuclei detection and\nsegmentation on the public MoNuSeg dataset, and the experimental results show\nthat our method achieves promising performance compared with the\nstate-of-the-art approaches. The effectiveness of each component is validated\nvia ablation studies as well. Our code is released at:\n\\url{https://github.com/zhanghx-iim-ahu/CircleFormer}.\n","authors":["Hengxu Zhang","Pengpeng Liang","Zhiyong Sun","Bo Song","Erkang Cheng"],"pdf_url":"https://arxiv.org/pdf/2308.16145v1.pdf","comment":"Accepted at MICCAI 2023"},{"id":"http://arxiv.org/abs/2308.16139v1","updated":"2023-08-30T16:52:20Z","published":"2023-08-30T16:52:20Z","title":"MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer\n  Vision","summary":"  We present MedShapeNet, a large collection of anatomical shapes (e.g., bones,\norgans, vessels) and 3D surgical instrument models. Prior to the deep learning\nera, the broad application of statistical shape models (SSMs) in medical image\nanalysis is evidence that shapes have been commonly used to describe medical\ndata. Nowadays, however, state-of-the-art (SOTA) deep learning algorithms in\nmedical imaging are predominantly voxel-based. In computer vision, on the\ncontrary, shapes (including, voxel occupancy grids, meshes, point clouds and\nimplicit surface models) are preferred data representations in 3D, as seen from\nthe numerous shape-related publications in premier vision conferences, such as\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), as\nwell as the increasing popularity of ShapeNet (about 51,300 models) and\nPrinceton ModelNet (127,915 models) in computer vision research. MedShapeNet is\ncreated as an alternative to these commonly used shape benchmarks to facilitate\nthe translation of data-driven vision algorithms to medical applications, and\nit extends the opportunities to adapt SOTA vision algorithms to solve critical\nmedical problems. Besides, the majority of the medical shapes in MedShapeNet\nare modeled directly on the imaging data of real patients, and therefore it\ncomplements well existing shape benchmarks comprising of computer-aided design\n(CAD) models. MedShapeNet currently includes more than 100,000 medical shapes,\nand provides annotations in the form of paired data. It is therefore also a\nfreely available repository of 3D models for extended reality (virtual reality\n- VR, augmented reality - AR, mixed reality - MR) and medical 3D printing. This\nwhite paper describes in detail the motivations behind MedShapeNet, the shape\nacquisition procedures, the use cases, as well as the usage of the online shape\nsearch portal: https://medshapenet.ikim.nrw/\n","authors":["Jianning Li","Antonio Pepe","Christina Gsaxner","Gijs Luijten","Yuan Jin","Narmada Ambigapathy","Enrico Nasca","Naida Solak","Gian Marco Melito","Afaque R. Memon","Xiaojun Chen","Jan Stefan Kirschke","Ezequiel de la Rosa","Patrich Ferndinand Christ","Hongwei Bran Li","David G. Ellis","Michele R. Aizenberg","Sergios Gatidis","Thomas Kuestner","Nadya Shusharina","Nicholas Heller","Vincent Andrearczyk","Adrien Depeursinge","Mathieu Hatt","Anjany Sekuboyina","Maximilian Loeffler","Hans Liebl","Reuben Dorent","Tom Vercauteren","Jonathan Shapey","Aaron Kujawa","Stefan Cornelissen","Patrick Langenhuizen","Achraf Ben-Hamadou","Ahmed Rekik","Sergi Pujades","Edmond Boyer","Federico Bolelli","Costantino Grana","Luca Lumetti","Hamidreza Salehi","Jun Ma","Yao Zhang","Ramtin Gharleghi","Susann Beier","Eduardo A. Garza-Villarreal","Thania Balducci","Diego Angeles-Valdez","Roberto Souza","Leticia Rittner","Richard Frayne","Yuanfeng Ji","Soumick Chatterjee","Andreas Nuernberger","Joao Pedrosa","Carlos Ferreira","Guilherme Aresta","Antonio Cunha","Aurelio Campilho","Yannick Suter","Jose Garcia","Alain Lalande","Emmanuel Audenaert","Claudia Krebs","Timo Van Leeuwen","Evie Vereecke","Rainer Roehrig","Frank Hoelzle","Vahid Badeli","Kathrin Krieger","Matthias Gunzer","Jianxu Chen","Amin Dada","Miriam Balzer","Jana Fragemann","Frederic Jonske","Moritz Rempe","Stanislav Malorodov","Fin H. Bahnsen","Constantin Seibold","Alexander Jaus","Ana Sofia Santos","Mariana Lindo","Andre Ferreira","Victor Alves","Michael Kamp","Amr Abourayya","Felix Nensa","Fabian Hoerst","Alexander Brehmer","Lukas Heine","Lars E. Podleska","Matthias A. Fink","Julius Keyl","Konstantinos Tserpes","Moon-Sung Kim","Shireen Elhabian","Hans Lamecker","Dzenan Zukic","Beatriz Paniagua","Christian Wachinger","Martin Urschler","Luc Duong","Jakob Wasserthal","Peter F. Hoyer","Oliver Basu","Thomas Maal","Max J. H. Witjes","Ping Luo","Bjoern Menze","Mauricio Reyes","Christos Davatzikos","Behrus Puladi","Jens Kleesiek","Jan Egger"],"pdf_url":"https://arxiv.org/pdf/2308.16139v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2303.17783v3","updated":"2023-08-30T16:25:02Z","published":"2023-03-31T03:14:44Z","title":"Uncertainty-Aware Source-Free Adaptive Image Super-Resolution with\n  Wavelet Augmentation Transformer","summary":"  Unsupervised Domain Adaptation (UDA) can effectively address domain gap\nissues in real-world image Super-Resolution (SR) by accessing both the source\nand target data. Considering privacy policies or transmission restrictions of\nsource data in practical scenarios, we propose a SOurce-free Domain Adaptation\nframework for image SR (SODA-SR) to address this issue, i.e., adapt a\nsource-trained model to a target domain with only unlabeled target data.\nSODA-SR leverages the source-trained model to generate refined pseudo-labels\nfor teacher-student learning. To better utilize pseudo-labels, we propose a\nnovel wavelet-based augmentation method, named Wavelet Augmentation Transformer\n(WAT), which can be flexibly incorporated with existing networks, to implicitly\nproduce useful augmented data. WAT learns low-frequency information of varying\nlevels across diverse samples, which is aggregated efficiently via deformable\nattention. Furthermore, an uncertainty-aware self-training mechanism is\nproposed to improve the accuracy of pseudo-labels, with inaccurate predictions\nbeing rectified by uncertainty estimation. To acquire better SR results and\navoid overfitting pseudo-labels, several regularization losses are proposed to\nconstrain target LR and SR images in the frequency domain. Experiments show\nthat without accessing source data, SODA-SR outperforms state-of-the-art UDA\nmethods in both synthetic$\\rightarrow$real and real$\\rightarrow$real adaptation\nsettings, and is not constrained by specific network architectures.\n","authors":["Yuang Ai","Xiaoqiang Zhou","Huaibo Huang","Lei Zhang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2303.17783v3.pdf","comment":"9 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2306.00914v2","updated":"2023-08-30T16:24:15Z","published":"2023-06-01T17:16:37Z","title":"Conditioning Diffusion Models via Attributes and Semantic Masks for Face\n  Generation","summary":"  Deep generative models have shown impressive results in generating realistic\nimages of faces. GANs managed to generate high-quality, high-fidelity images\nwhen conditioned on semantic masks, but they still lack the ability to\ndiversify their output. Diffusion models partially solve this problem and are\nable to generate diverse samples given the same condition. In this paper, we\npropose a multi-conditioning approach for diffusion models via cross-attention\nexploiting both attributes and semantic masks to generate high-quality and\ncontrollable face images. We also studied the impact of applying\nperceptual-focused loss weighting into the latent space instead of the pixel\nspace. Our method extends the previous approaches by introducing conditioning\non more than one set of features, guaranteeing a more fine-grained control over\nthe generated face images. We evaluate our approach on the CelebA-HQ dataset,\nand we show that it can generate realistic and diverse samples while allowing\nfor fine-grained control over multiple attributes and semantic regions.\nAdditionally, we perform an ablation study to evaluate the impact of different\nconditioning strategies on the quality and diversity of the generated images.\n","authors":["Nico Giambi","Giuseppe Lisanti"],"pdf_url":"https://arxiv.org/pdf/2306.00914v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16126v1","updated":"2023-08-30T16:23:07Z","published":"2023-08-30T16:23:07Z","title":"CorrEmbed: Evaluating Pre-trained Model Image Similarity Efficacy with a\n  Novel Metric","summary":"  Detecting visually similar images is a particularly useful attribute to look\nto when calculating product recommendations. Embedding similarity, which\nutilizes pre-trained computer vision models to extract high-level image\nfeatures, has demonstrated remarkable efficacy in identifying images with\nsimilar compositions. However, there is a lack of methods for evaluating the\nembeddings generated by these models, as conventional loss and performance\nmetrics do not adequately capture their performance in image similarity search\ntasks.\n  In this paper, we evaluate the viability of the image embeddings from\nnumerous pre-trained computer vision models using a novel approach named\nCorrEmbed. Our approach computes the correlation between distances in image\nembeddings and distances in human-generated tag vectors. We extensively\nevaluate numerous pre-trained Torchvision models using this metric, revealing\nan intuitive relationship of linear scaling between ImageNet1k accuracy scores\nand tag-correlation scores. Importantly, our method also identifies deviations\nfrom this pattern, providing insights into how different models capture\nhigh-level image features.\n  By offering a robust performance evaluation of these pre-trained models,\nCorrEmbed serves as a valuable tool for researchers and practitioners seeking\nto develop effective, data-driven approaches to similar item recommendations in\nfashion retail.\n","authors":["Karl Audun Kagnes Borgersen","Morten Goodwin","Jivitesh Sharma","Tobias Aasmoe","Mari Leonhardsen","Gro Herredsvela Rørvik"],"pdf_url":"https://arxiv.org/pdf/2308.16126v1.pdf","comment":"Accepted to AI-2023 Forty-third SGAI International Conference on\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2308.16110v1","updated":"2023-08-30T16:10:21Z","published":"2023-08-30T16:10:21Z","title":"Improving Few-shot Image Generation by Structural Discrimination and\n  Textural Modulation","summary":"  Few-shot image generation, which aims to produce plausible and diverse images\nfor one category given a few images from this category, has drawn extensive\nattention. Existing approaches either globally interpolate different images or\nfuse local representations with pre-defined coefficients. However, such an\nintuitive combination of images/features only exploits the most relevant\ninformation for generation, leading to poor diversity and coarse-grained\nsemantic fusion. To remedy this, this paper proposes a novel textural\nmodulation (TexMod) mechanism to inject external semantic signals into internal\nlocal representations. Parameterized by the feedback from the discriminator,\nour TexMod enables more fined-grained semantic injection while maintaining the\nsynthesis fidelity. Moreover, a global structural discriminator (StructD) is\ndeveloped to explicitly guide the model to generate images with reasonable\nlayout and outline. Furthermore, the frequency awareness of the model is\nreinforced by encouraging the model to distinguish frequency signals. Together\nwith these techniques, we build a novel and effective model for few-shot image\ngeneration. The effectiveness of our model is identified by extensive\nexperiments on three popular datasets and various settings. Besides achieving\nstate-of-the-art synthesis performance on these datasets, our proposed\ntechniques could be seamlessly integrated into existing models for a further\nperformance boost.\n","authors":["Mengping Yang","Zhe Wang","Wenyi Feng","Qian Zhang","Ting Xiao"],"pdf_url":"https://arxiv.org/pdf/2308.16110v1.pdf","comment":"To appear in ACM MM 2023, code is available at\n  https://github.com/kobeshegu/SDTM-GAN-ACMMM-2023"},{"id":"http://arxiv.org/abs/2305.11582v2","updated":"2023-08-30T16:06:27Z","published":"2023-05-19T10:43:57Z","title":"What You Hear Is What You See: Audio Quality Metrics From Image Quality\n  Metrics","summary":"  In this study, we investigate the feasibility of utilizing state-of-the-art\nimage perceptual metrics for evaluating audio signals by representing them as\nspectrograms. The encouraging outcome of the proposed approach is based on the\nsimilarity between the neural mechanisms in the auditory and visual pathways.\nFurthermore, we customise one of the metrics which has a psychoacoustically\nplausible architecture to account for the peculiarities of sound signals. We\nevaluate the effectiveness of our proposed metric and several baseline metrics\nusing a music dataset, with promising results in terms of the correlation\nbetween the metrics and the perceived quality of audio as rated by human\nevaluators.\n","authors":["Tashi Namgyal","Alexander Hepburn","Raul Santos-Rodriguez","Valero Laparra","Jesus Malo"],"pdf_url":"https://arxiv.org/pdf/2305.11582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15745v2","updated":"2023-08-30T15:58:56Z","published":"2023-07-28T18:01:08Z","title":"Context-VQA: Towards Context-Aware and Purposeful Visual Question\n  Answering","summary":"  Visual question answering (VQA) has the potential to make the Internet more\naccessible in an interactive way, allowing people who cannot see images to ask\nquestions about them. However, multiple studies have shown that people who are\nblind or have low-vision prefer image explanations that incorporate the context\nin which an image appears, yet current VQA datasets focus on images in\nisolation. We argue that VQA models will not fully succeed at meeting people's\nneeds unless they take context into account. To further motivate and analyze\nthe distinction between different contexts, we introduce Context-VQA, a VQA\ndataset that pairs images with contexts, specifically types of websites (e.g.,\na shopping website). We find that the types of questions vary systematically\nacross contexts. For example, images presented in a travel context garner 2\ntimes more \"Where?\" questions, and images on social media and news garner 2.8\nand 1.8 times more \"Who?\" questions than the average. We also find that context\neffects are especially important when participants can't see the image. These\nresults demonstrate that context affects the types of questions asked and that\nVQA models should be context-sensitive to better meet people's needs,\nespecially in accessibility settings.\n","authors":["Nandita Naik","Christopher Potts","Elisa Kreiss"],"pdf_url":"https://arxiv.org/pdf/2307.15745v2.pdf","comment":"Proceedings of ICCV 2023 Workshop on Closing the Loop Between Vision\n  and Language"},{"id":"http://arxiv.org/abs/2308.14480v2","updated":"2023-08-30T15:33:01Z","published":"2023-08-28T10:40:16Z","title":"Priority-Centric Human Motion Generation in Discrete Latent Space","summary":"  Text-to-motion generation is a formidable task, aiming to produce human\nmotions that align with the input text while also adhering to human\ncapabilities and physical laws. While there have been advancements in diffusion\nmodels, their application in discrete spaces remains underexplored. Current\nmethods often overlook the varying significance of different motions, treating\nthem uniformly. It is essential to recognize that not all motions hold the same\nrelevance to a particular textual description. Some motions, being more salient\nand informative, should be given precedence during generation. In response, we\nintroduce a Priority-Centric Motion Discrete Diffusion Model (M2DM), which\nutilizes a Transformer-based VQ-VAE to derive a concise, discrete motion\nrepresentation, incorporating a global self-attention mechanism and a\nregularization term to counteract code collapse. We also present a motion\ndiscrete diffusion model that employs an innovative noise schedule, determined\nby the significance of each motion token within the entire motion sequence.\nThis approach retains the most salient motions during the reverse diffusion\nprocess, leading to more semantically rich and varied motions. Additionally, we\nformulate two strategies to gauge the importance of motion tokens, drawing from\nboth textual and visual indicators. Comprehensive experiments on the HumanML3D\nand KIT-ML datasets confirm that our model surpasses existing techniques in\nfidelity and diversity, particularly for intricate textual descriptions.\n","authors":["Hanyang Kong","Kehong Gong","Dongze Lian","Michael Bi Mi","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2308.14480v2.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2308.16083v1","updated":"2023-08-30T15:15:31Z","published":"2023-08-30T15:15:31Z","title":"Learned Image Reasoning Prior Penetrates Deep Unfolding Network for\n  Panchromatic and Multi-Spectral Image Fusion","summary":"  The success of deep neural networks for pan-sharpening is commonly in a form\nof black box, lacking transparency and interpretability. To alleviate this\nissue, we propose a novel model-driven deep unfolding framework with image\nreasoning prior tailored for the pan-sharpening task. Different from existing\nunfolding solutions that deliver the proximal operator networks as the\nuncertain and vague priors, our framework is motivated by the content reasoning\nability of masked autoencoders (MAE) with insightful designs. Specifically, the\npre-trained MAE with spatial masking strategy, acting as intrinsic reasoning\nprior, is embedded into unfolding architecture. Meanwhile, the pre-trained MAE\nwith spatial-spectral masking strategy is treated as the regularization term\nwithin loss function to constrain the spatial-spectral consistency. Such\ndesigns penetrate the image reasoning prior into deep unfolding networks while\nimproving its interpretability and representation capability. The uniqueness of\nour framework is that the holistic learning process is explicitly integrated\nwith the inherent physical mechanism underlying the pan-sharpening task.\nExtensive experiments on multiple satellite datasets demonstrate the\nsuperiority of our method over the existing state-of-the-art approaches. Code\nwill be released at \\url{https://manman1995.github.io/}.\n","authors":["Man Zhou","Jie Huang","Naishan Zheng","Chongyi Li"],"pdf_url":"https://arxiv.org/pdf/2308.16083v1.pdf","comment":"10 pages; Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2308.16082v1","updated":"2023-08-30T15:14:56Z","published":"2023-08-30T15:14:56Z","title":"SignDiff: Learning Diffusion Models for American Sign Language\n  Production","summary":"  The field of Sign Language Production (SLP) lacked a large-scale, pre-trained\nmodel based on deep learning for continuous American Sign Language (ASL)\nproduction in the past decade. This limitation hampers communication for all\nindividuals with disabilities relying on ASL. To address this issue, we\nundertook the secondary development and utilization of How2Sign, one of the\nlargest publicly available ASL datasets. Despite its significance, prior\nresearchers in the field of sign language have not effectively employed this\ncorpus due to the intricacies involved in American Sign Language Production\n(ASLP).\n  To conduct large-scale ASLP, we propose SignDiff based on the latest work in\nrelated fields, which is a dual-condition diffusion pre-training model that can\ngenerate human sign language speakers from a skeleton pose. SignDiff has a\nnovel Frame Reinforcement Network called FR-Net, similar to dense human pose\nestimation work, which enhances the correspondence between text lexical symbols\nand sign language dense pose frames reduce the occurrence of multiple fingers\nin the diffusion model. In addition, our ASLP method proposes two new improved\nmodules and a new loss function to improve the accuracy and quality of sign\nlanguage skeletal posture and enhance the ability of the model to train on\nlarge-scale data.\n  We propose the first baseline for ASL production and report the scores of\n17.19 and 12.85 on BLEU-4 on the How2Sign dev/test sets. We also evaluated our\nmodel on the previous mainstream dataset called PHOENIX14T, and the main\nexperiments achieved the results of SOTA. In addition, our image quality far\nexceeds all previous results by 10 percentage points on the SSIM indicator.\nFinally, we conducted ablation studies and qualitative evaluations for\ndiscussion.\n","authors":["Sen Fang","Chunyu Sui","Xuedong Zhang","Yapeng Tian"],"pdf_url":"https://arxiv.org/pdf/2308.16082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16075v1","updated":"2023-08-30T14:52:14Z","published":"2023-08-30T14:52:14Z","title":"Impact of Visual Context on Noisy Multimodal NMT: An Empirical Study for\n  English to Indian Languages","summary":"  The study investigates the effectiveness of utilizing multimodal information\nin Neural Machine Translation (NMT). While prior research focused on using\nmultimodal data in low-resource scenarios, this study examines how image\nfeatures impact translation when added to a large-scale, pre-trained unimodal\nNMT system. Surprisingly, the study finds that images might be redundant in\nthis context. Additionally, the research introduces synthetic noise to assess\nwhether images help the model deal with textual noise. Multimodal models\nslightly outperform text-only models in noisy settings, even with random\nimages. The study's experiments translate from English to Hindi, Bengali, and\nMalayalam, outperforming state-of-the-art benchmarks significantly.\nInterestingly, the effect of visual context varies with source text noise: no\nvisual context works best for non-noisy translations, cropped image features\nare optimal for low noise, and full image features work better in high-noise\nscenarios. This sheds light on the role of visual context, especially in noisy\nsettings, opening up a new research direction for Noisy Neural Machine\nTranslation in multimodal setups. The research emphasizes the importance of\ncombining visual and textual information for improved translation in various\nenvironments.\n","authors":["Baban Gain","Dibyanayan Bandyopadhyay","Samrat Mukherjee","Chandranath Adak","Asif Ekbal"],"pdf_url":"https://arxiv.org/pdf/2308.16075v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16071v1","updated":"2023-08-30T14:49:34Z","published":"2023-08-30T14:49:34Z","title":"Semantic Image Synthesis via Class-Adaptive Cross-Attention","summary":"  In semantic image synthesis, the state of the art is dominated by methods\nthat use spatially-adaptive normalization layers, which allow for excellent\nvisual generation quality and editing versatility. Granted their efficacy,\nrecent research efforts have focused toward finer-grained local style control\nand multi-modal generation. By construction though, such layers tend to\noverlook global image statistics leading to unconvincing local style editing\nand causing global inconsistencies such as color or illumination distribution\nshifts. Also, the semantic layout is required for mapping styles in the\ngenerator, putting a strict alignment constraint over the features. In\nresponse, we designed a novel architecture where cross-attention layers are\nused in place of de-normalization ones for conditioning the image generation.\nOur model inherits the advantages of both solutions, retaining state-of-the-art\nreconstruction quality, as well as improved global and local style transfer.\nCode and models available at https://github.com/TFonta/CA2SIS.\n","authors":["Tomaso Fontanini","Claudio Ferrari","Giuseppe Lisanti","Massimo Bertozzi","Andrea Prati"],"pdf_url":"https://arxiv.org/pdf/2308.16071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2107.07752v2","updated":"2023-08-30T14:39:24Z","published":"2021-07-16T08:07:22Z","title":"NeXtQSM -- A complete deep learning pipeline for data-consistent\n  quantitative susceptibility mapping trained with hybrid data","summary":"  Deep learning based Quantitative Susceptibility Mapping (QSM) has shown great\npotential in recent years, obtaining similar results to established\nnon-learning approaches. Many current deep learning approaches are not data\nconsistent, require in vivo training data or solve the QSM problem in\nconsecutive steps resulting in the propagation of errors. Here we aim to\novercome these limitations and developed a framework to solve the QSM\nprocessing steps jointly. We developed a new hybrid training data generation\nmethod that enables the end-to-end training for solving background field\ncorrection and dipole inversion in a data-consistent fashion using a\nvariational network that combines the QSM model term and a learned regularizer.\nWe demonstrate that NeXtQSM overcomes the limitations of previous deep learning\nmethods. NeXtQSM offers a new deep learning based pipeline for computing\nquantitative susceptibility maps that integrates each processing step into the\ntraining and provides results that are robust and fast.\n","authors":["Francesco Cognolato","Kieran O'Brien","Jin Jin","Simon Robinson","Frederik B. Laun","Markus Barth","Steffen Bollmann"],"pdf_url":"https://arxiv.org/pdf/2107.07752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.08637v2","updated":"2023-08-30T14:28:37Z","published":"2023-06-14T17:07:51Z","title":"TAPIR: Tracking Any Point with per-frame Initialization and temporal\n  Refinement","summary":"  We present a novel model for Tracking Any Point (TAP) that effectively tracks\nany queried point on any physical surface throughout a video sequence. Our\napproach employs two stages: (1) a matching stage, which independently locates\na suitable candidate point match for the query point on every other frame, and\n(2) a refinement stage, which updates both the trajectory and query features\nbased on local correlations. The resulting model surpasses all baseline methods\nby a significant margin on the TAP-Vid benchmark, as demonstrated by an\napproximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our model\nfacilitates fast inference on long and high-resolution video sequences. On a\nmodern GPU, our implementation has the capacity to track points faster than\nreal-time, and can be flexibly extended to higher-resolution videos. Given the\nhigh-quality trajectories extracted from a large dataset, we demonstrate a\nproof-of-concept diffusion model which generates trajectories from static\nimages, enabling plausible animations. Visualizations, source code, and\npretrained models can be found on our project webpage.\n","authors":["Carl Doersch","Yi Yang","Mel Vecerik","Dilara Gokay","Ankush Gupta","Yusuf Aytar","Joao Carreira","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2306.08637v2.pdf","comment":"Published at ICCV 2023"},{"id":"http://arxiv.org/abs/2210.05152v5","updated":"2023-08-30T14:24:46Z","published":"2022-10-11T05:11:41Z","title":"TriangleNet: Edge Prior Augmented Network for Semantic Segmentation\n  through Cross-Task Consistency","summary":"  This paper addresses the task of semantic segmentation in computer vision,\naiming to achieve precise pixel-wise classification. We investigate the joint\ntraining of models for semantic edge detection and semantic segmentation, which\nhas shown promise. However, implicit cross-task consistency learning in\nmulti-task networks is limited. To address this, we propose a novel \"decoupled\ncross-task consistency loss\" that explicitly enhances cross-task consistency.\nOur semantic segmentation network, TriangleNet, achieves a substantial 2.88\\%\nimprovement over the Baseline in mean Intersection over Union (mIoU) on the\nCityscapes test set. Notably, TriangleNet operates at 77.4\\% mIoU/46.2 FPS on\nCityscapes, showcasing real-time inference capabilities at full resolution.\nWith multi-scale inference, performance is further enhanced to 77.8\\%.\nFurthermore, TriangleNet consistently outperforms the Baseline on the FloodNet\ndataset, demonstrating its robust generalization capabilities. The proposed\nmethod underscores the significance of multi-task learning and explicit\ncross-task consistency enhancement for advancing semantic segmentation and\nhighlights the potential of multitasking in real-time semantic segmentation.\n","authors":["Dan Zhang","Rui Zheng","Luosang Gadeng","Pei Yang"],"pdf_url":"https://arxiv.org/pdf/2210.05152v5.pdf","comment":"Accepted for publication in the journal \"International Journal of\n  Intelligent Systems\""},{"id":"http://arxiv.org/abs/2302.14416v3","updated":"2023-08-30T14:22:32Z","published":"2023-02-28T08:48:45Z","title":"DREAM: Efficient Dataset Distillation by Representative Matching","summary":"  Dataset distillation aims to synthesize small datasets with little\ninformation loss from original large-scale ones for reducing storage and\ntraining costs. Recent state-of-the-art methods mainly constrain the sample\nsynthesis process by matching synthetic images and the original ones regarding\ngradients, embedding distributions, or training trajectories. Although there\nare various matching objectives, currently the strategy for selecting original\nimages is limited to naive random sampling.\n  We argue that random sampling overlooks the evenness of the selected sample\ndistribution, which may result in noisy or biased matching targets.\n  Besides, the sample diversity is also not constrained by random sampling.\nThese factors together lead to optimization instability in the distilling\nprocess and degrade the training efficiency. Accordingly, we propose a novel\nmatching strategy named as \\textbf{D}ataset distillation by\n\\textbf{RE}present\\textbf{A}tive \\textbf{M}atching (DREAM), where only\nrepresentative original images are selected for matching. DREAM is able to be\neasily plugged into popular dataset distillation frameworks and reduce the\ndistilling iterations by more than 8 times without performance drop. Given\nsufficient training time, DREAM further provides significant improvements and\nachieves state-of-the-art performances.\n","authors":["Yanqing Liu","Jianyang Gu","Kai Wang","Zheng Zhu","Wei Jiang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2302.14416v3.pdf","comment":"Efficient matching for dataset distillation"},{"id":"http://arxiv.org/abs/2308.14500v2","updated":"2023-08-30T14:18:58Z","published":"2023-08-28T11:20:48Z","title":"LAC -- Latent Action Composition for Skeleton-based Action Segmentation","summary":"  Skeleton-based action segmentation requires recognizing composable actions in\nuntrimmed videos. Current approaches decouple this problem by first extracting\nlocal visual features from skeleton sequences and then processing them by a\ntemporal model to classify frame-wise actions. However, their performances\nremain limited as the visual features cannot sufficiently express composable\nactions. In this context, we propose Latent Action Composition (LAC), a novel\nself-supervised framework aiming at learning from synthesized composable\nmotions for skeleton-based action segmentation. LAC is composed of a novel\ngeneration module towards synthesizing new sequences. Specifically, we design a\nlinear latent space in the generator to represent primitive motion. New\ncomposed motions can be synthesized by simply performing arithmetic operations\non latent representations of multiple input skeleton sequences. LAC leverages\nsuch synthesized sequences, which have large diversity and complexity, for\nlearning visual representations of skeletons in both sequence and frame spaces\nvia contrastive learning. The resulting visual encoder has a high expressive\npower and can be effectively transferred onto action segmentation tasks by\nend-to-end fine-tuning without the need for additional temporal models. We\nconduct a study focusing on transfer-learning and we show that representations\nlearned from pre-trained LAC outperform the state-of-the-art by a large margin\non TSU, Charades, PKU-MMD datasets.\n","authors":["Di Yang","Yaohui Wang","Antitza Dantcheva","Quan Kong","Lorenzo Garattoni","Gianpiero Francesca","Francois Bremond"],"pdf_url":"https://arxiv.org/pdf/2308.14500v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2303.12247v2","updated":"2023-08-30T14:09:13Z","published":"2023-03-22T01:01:14Z","title":"Exploring the Benefits of Visual Prompting in Differential Privacy","summary":"  Visual Prompting (VP) is an emerging and powerful technique that allows\nsample-efficient adaptation to downstream tasks by engineering a well-trained\nfrozen source model. In this work, we explore the benefits of VP in\nconstructing compelling neural network classifiers with differential privacy\n(DP). We explore and integrate VP into canonical DP training methods and\ndemonstrate its simplicity and efficiency. In particular, we discover that VP\nin tandem with PATE, a state-of-the-art DP training method that leverages the\nknowledge transfer from an ensemble of teachers, achieves the state-of-the-art\nprivacy-utility trade-off with minimum expenditure of privacy budget. Moreover,\nwe conduct additional experiments on cross-domain image classification with a\nsufficient domain gap to further unveil the advantage of VP in DP. Lastly, we\nalso conduct extensive ablation studies to validate the effectiveness and\ncontribution of VP under DP consideration. Our code is available at\n(https://github.com/EzzzLi/Prompt-PATE).\n","authors":["Yizhe Li","Yu-Lin Tsai","Xuebin Ren","Chia-Mu Yu","Pin-Yu Chen"],"pdf_url":"https://arxiv.org/pdf/2303.12247v2.pdf","comment":"Published at ICCV 2023"},{"id":"http://arxiv.org/abs/2306.06208v3","updated":"2023-08-30T14:07:49Z","published":"2023-06-05T23:07:01Z","title":"DeltaNN: Assessing the Impact of Computational Environment Parameters on\n  the Performance of Image Recognition Models","summary":"  Image recognition tasks typically use deep learning and require enormous\nprocessing power, thus relying on hardware accelerators like GPUs and TPUs for\nfast, timely processing. Failure in real-time image recognition tasks can occur\ndue to sub-optimal mapping on hardware accelerators during model deployment,\nwhich may lead to timing uncertainty and erroneous behavior. Mapping on\nhardware accelerators is done using multiple software components like deep\nlearning frameworks, compilers, and device libraries, that we refer to as the\ncomputational environment. Owing to the increased use of image recognition\ntasks in safety-critical applications like autonomous driving and medical\nimaging, it is imperative to assess their robustness to changes in the\ncomputational environment, as the impact of parameters like deep learning\nframeworks, compiler optimizations, and hardware devices on model performance\nand correctness is not yet well understood.\n  In this paper we present a differential testing framework, DeltaNN, that\nallows us to assess the impact of different computational environment\nparameters on the performance of image recognition models during deployment,\npost training. DeltaNN generates different implementations of a given image\nrecognition model for variations in environment parameters, namely, deep\nlearning frameworks, compiler optimizations and hardware devices and analyzes\ndifferences in model performance as a result. Using DeltaNN, we conduct an\nempirical study of robustness analysis of three popular image recognition\nmodels using the ImageNet dataset. We report the impact in terms of\nmisclassifications and inference time differences across different settings. In\ntotal, we observed up to 72% output label differences across deep learning\nframeworks, and up to 81% unexpected performance degradation in terms of\ninference time, when applying compiler optimizations.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2306.06208v3.pdf","comment":"11 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2305.08854v2","updated":"2023-08-30T14:01:36Z","published":"2023-05-15T17:59:57Z","title":"Laughing Matters: Introducing Laughing-Face Generation using Diffusion\n  Models","summary":"  Speech-driven animation has gained significant traction in recent years, with\ncurrent methods achieving near-photorealistic results. However, the field\nremains underexplored regarding non-verbal communication despite evidence\ndemonstrating its importance in human interaction. In particular, generating\nlaughter sequences presents a unique challenge due to the intricacy and nuances\nof this behaviour. This paper aims to bridge this gap by proposing a novel\nmodel capable of generating realistic laughter sequences, given a still\nportrait and an audio clip containing laughter. We highlight the failure cases\nof traditional facial animation methods and leverage recent advances in\ndiffusion models to produce convincing laughter videos. We train our model on a\ndiverse set of laughter datasets and introduce an evaluation metric\nspecifically designed for laughter. When compared with previous speech-driven\napproaches, our model achieves state-of-the-art performance across all metrics,\neven when these are re-trained for laughter generation. Our code and project\nare publicly available\n","authors":["Antoni Bigata Casademunt","Rodrigo Mira","Nikita Drobyshev","Konstantinos Vougioukas","Stavros Petridis","Maja Pantic"],"pdf_url":"https://arxiv.org/pdf/2305.08854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16041v1","updated":"2023-08-30T14:00:48Z","published":"2023-08-30T14:00:48Z","title":"From Pixels to Portraits: A Comprehensive Survey of Talking Head\n  Generation Techniques and Applications","summary":"  Recent advancements in deep learning and computer vision have led to a surge\nof interest in generating realistic talking heads. This paper presents a\ncomprehensive survey of state-of-the-art methods for talking head generation.\nWe systematically categorises them into four main approaches: image-driven,\naudio-driven, video-driven and others (including neural radiance fields (NeRF),\nand 3D-based methods). We provide an in-depth analysis of each method,\nhighlighting their unique contributions, strengths, and limitations.\nFurthermore, we thoroughly compare publicly available models, evaluating them\non key aspects such as inference time and human-rated quality of the generated\noutputs. Our aim is to provide a clear and concise overview of the current\nlandscape in talking head generation, elucidating the relationships between\ndifferent approaches and identifying promising directions for future research.\nThis survey will serve as a valuable reference for researchers and\npractitioners interested in this rapidly evolving field.\n","authors":["Shreyank N Gowda","Dheeraj Pandey","Shashank Narayana Gowda"],"pdf_url":"https://arxiv.org/pdf/2308.16041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.10611v2","updated":"2023-08-30T13:50:25Z","published":"2023-01-25T14:45:13Z","title":"Discriminator-free Unsupervised Domain Adaptation for Multi-label Image\n  Classification","summary":"  In this paper, a discriminator-free adversarial-based Unsupervised Domain\nAdaptation (UDA) for Multi-Label Image Classification (MLIC) referred to as\nDDA-MLIC is proposed. Recently, some attempts have been made for introducing\nadversarial-based UDA methods in the context of MLIC. However, these methods\nwhich rely on an additional discriminator subnet present one major shortcoming.\nThe learning of domain-invariant features may harm their task-specific\ndiscriminative power, since the classification and discrimination tasks are\ndecoupled. Herein, we propose to overcome this issue by introducing a novel\nadversarial critic that is directly deduced from the task-specific classifier.\nSpecifically, a two-component Gaussian Mixture Model (GMM) is fitted on the\nsource and target predictions in order to distinguish between two clusters.\nThis allows extracting a Gaussian distribution for each component. The\nresulting Gaussian distributions are then used for formulating an adversarial\nloss based on a Frechet distance. The proposed method is evaluated on several\nmulti-label image datasets covering three different types of domain shift. The\nobtained results demonstrate that DDA-MLIC outperforms existing\nstate-of-the-art methods in terms of precision while requiring a lower number\nof parameters. The code will be made publicly available online.\n","authors":["Indel Pal Singh","Enjie Ghorbel","Anis Kacem","Arunkumar Rathinam","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2301.10611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06157v3","updated":"2023-08-30T13:41:23Z","published":"2023-06-10T23:50:02Z","title":"Fault Localization for Buggy Deep Learning Framework Conversions in\n  Image Recognition","summary":"  When deploying Deep Neural Networks (DNNs), developers often convert models\nfrom one deep learning framework to another (e.g., TensorFlow to PyTorch).\nHowever, this process is error-prone and can impact target model accuracy. To\nidentify the extent of such impact, we perform and briefly present a\ndifferential analysis against three DNNs widely used for image recognition\n(MobileNetV2, ResNet101, and InceptionV3) converted across four well-known deep\nlearning frameworks (PyTorch, Keras, TensorFlow (TF), and TFLite), which\nrevealed numerous model crashes and output label discrepancies of up to 72%. To\nmitigate such errors, we present a novel approach towards fault localization\nand repair of buggy deep learning framework conversions, focusing on\npre-trained image recognition models. Our technique consists of four stages of\nanalysis: 1) conversion tools, 2) model parameters, 3) model hyperparameters,\nand 4) graph representation. In addition, we propose various strategies towards\nfault repair of the faults detected. We implement our technique on top of the\nApache TVM deep learning compiler, and we test it by conducting a preliminary\nfault localization analysis for the conversion of InceptionV3 from TF to\nTFLite. Our approach detected a fault in a common DNN converter tool, which\nintroduced precision errors in weights, reducing model accuracy. After our\nfault localization, we repaired the issue, reducing our conversion error to\nzero.\n","authors":["Nikolaos Louloudakis","Perry Gibson","José Cano","Ajitha Rajan"],"pdf_url":"https://arxiv.org/pdf/2306.06157v3.pdf","comment":"5 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2308.14074v2","updated":"2023-08-30T13:40:21Z","published":"2023-08-27T11:37:26Z","title":"Nonrigid Object Contact Estimation With Regional Unwrapping Transformer","summary":"  Acquiring contact patterns between hands and nonrigid objects is a common\nconcern in the vision and robotics community. However, existing learning-based\nmethods focus more on contact with rigid ones from monocular images. When\nadopting them for nonrigid contact, a major problem is that the existing\ncontact representation is restricted by the geometry of the object.\nConsequently, contact neighborhoods are stored in an unordered manner and\ncontact features are difficult to align with image cues. At the core of our\napproach lies a novel hand-object contact representation called RUPs (Region\nUnwrapping Profiles), which unwrap the roughly estimated hand-object surfaces\nas multiple high-resolution 2D regional profiles. The region grouping strategy\nis consistent with the hand kinematic bone division because they are the\nprimitive initiators for a composite contact pattern. Based on this\nrepresentation, our Regional Unwrapping Transformer (RUFormer) learns the\ncorrelation priors across regions from monocular inputs and predicts\ncorresponding contact and deformed transformations. Our experiments demonstrate\nthat the proposed framework can robustly estimate the deformed degrees and\ndeformed transformations, which makes it suitable for both nonrigid and rigid\ncontact.\n","authors":["Wei Xie","Zimeng Zhao","Shiying Li","Binghui Zuo","Yangang Wang"],"pdf_url":"https://arxiv.org/pdf/2308.14074v2.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2307.15016v2","updated":"2023-08-30T13:33:59Z","published":"2023-07-27T17:19:32Z","title":"How Good is Google Bard's Visual Understanding? An Empirical Study on\n  Open Challenges","summary":"  Google's Bard has emerged as a formidable competitor to OpenAI's ChatGPT in\nthe field of conversational AI. Notably, Bard has recently been updated to\nhandle visual inputs alongside text prompts during conversations. Given Bard's\nimpressive track record in handling textual inputs, we explore its capabilities\nin understanding and interpreting visual data (images) conditioned by text\nquestions. This exploration holds the potential to unveil new insights and\nchallenges for Bard and other forthcoming multi-modal Generative models,\nespecially in addressing complex computer vision problems that demand accurate\nvisual and language understanding. Specifically, in this study, we focus on 15\ndiverse task scenarios encompassing regular, camouflaged, medical, under-water\nand remote sensing data to comprehensively evaluate Bard's performance. Our\nprimary finding indicates that Bard still struggles in these vision scenarios,\nhighlighting the significant gap in vision-based understanding that needs to be\nbridged in future developments. We expect that this empirical study will prove\nvaluable in advancing future models, leading to enhanced capabilities in\ncomprehending and interpreting fine-grained visual data. Our project is\nreleased on https://github.com/htqin/GoogleBard-VisUnderstand\n","authors":["Haotong Qin","Ge-Peng Ji","Salman Khan","Deng-Ping Fan","Fahad Shahbaz Khan","Luc Van Gool"],"pdf_url":"https://arxiv.org/pdf/2307.15016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14036v2","updated":"2023-08-30T13:27:35Z","published":"2023-08-27T08:10:23Z","title":"MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor\n  Formula for Image Dehazing","summary":"  In recent years, Transformer networks are beginning to replace pure\nconvolutional neural networks (CNNs) in the field of computer vision due to\ntheir global receptive field and adaptability to input. However, the quadratic\ncomputational complexity of softmax-attention limits the wide application in\nimage dehazing task, especially for high-resolution images. To address this\nissue, we propose a new Transformer variant, which applies the Taylor expansion\nto approximate the softmax-attention and achieves linear computational\ncomplexity. A multi-scale attention refinement module is proposed as a\ncomplement to correct the error of the Taylor expansion. Furthermore, we\nintroduce a multi-branch architecture with multi-scale patch embedding to the\nproposed Transformer, which embeds features by overlapping deformable\nconvolution of different scales. The design of multi-scale patch embedding is\nbased on three key ideas: 1) various sizes of the receptive field; 2)\nmulti-level semantic information; 3) flexible shapes of the receptive field.\nOur model, named Multi-branch Transformer expanded by Taylor formula\n(MB-TaylorFormer), can embed coarse to fine features more flexibly at the patch\nembedding stage and capture long-distance pixel interactions with limited\ncomputational cost. Experimental results on several dehazing benchmarks show\nthat MB-TaylorFormer achieves state-of-the-art (SOTA) performance with a light\ncomputational burden. The source code and pre-trained models are available at\nhttps://github.com/FVL2020/ICCV-2023-MB-TaylorFormer.\n","authors":["Yuwei Qiu","Kaihao Zhang","Chenxi Wang","Wenhan Luo","Hongdong Li","Zhi Jin"],"pdf_url":"https://arxiv.org/pdf/2308.14036v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2308.16018v1","updated":"2023-08-30T13:20:54Z","published":"2023-08-30T13:20:54Z","title":"Topology-aware MLP for Skeleton-based Action Recognition","summary":"  Graph convolution networks (GCNs) have achieved remarkable performance in\nskeleton-based action recognition. However, existing previous GCN-based methods\nhave relied excessively on elaborate human body priors and constructed complex\nfeature aggregation mechanisms, which limits the generalizability of networks.\nTo solve these problems, we propose a novel Spatial Topology Gating Unit\n(STGU), which is an MLP-based variant without extra priors, to capture the\nco-occurrence topology features that encode the spatial dependency across all\njoints. In STGU, to model the sample-specific and completely independent\npoint-wise topology attention, a new gate-based feature interaction mechanism\nis introduced to activate the features point-to-point by the attention map\ngenerated from the input. Based on the STGU, in this work, we propose the first\ntopology-aware MLP-based model, Ta-MLP, for skeleton-based action recognition.\nIn comparison with existing previous methods on three large-scale datasets,\nTa-MLP achieves competitive performance. In addition, Ta-MLP reduces the\nparameters by up to 62.5% with favorable results. Compared with previous\nstate-of-the-art (SOAT) approaches, Ta-MLP pushes the frontier of real-time\naction recognition. The code will be available at\nhttps://github.com/BUPTSJZhang/Ta-MLP.\n","authors":["Shaojie Zhang","Jianqin Yin","Yonghao Dang","Jiajun Fu"],"pdf_url":"https://arxiv.org/pdf/2308.16018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06289v2","updated":"2023-08-30T13:01:39Z","published":"2023-06-09T22:29:56Z","title":"SegViTv2: Exploring Efficient and Continual Semantic Segmentation with\n  Plain Vision Transformers","summary":"  This paper investigates the capability of plain Vision Transformers (ViTs)\nfor semantic segmentation using the encoder-decoder framework and introduces\n\\textbf{SegViTv2}. In this study, we introduce a novel Attention-to-Mask (\\atm)\nmodule to design a lightweight decoder effective for plain ViT. The proposed\nATM converts the global attention map into semantic masks for high-quality\nsegmentation results. Our decoder outperforms the popular decoder UPerNet using\nvarious ViT backbones while consuming only about $5\\%$ of the computational\ncost. For the encoder, we address the concern of the relatively high\ncomputational cost in the ViT-based encoders and propose a \\emph{Shrunk++}\nstructure that incorporates edge-aware query-based down-sampling (EQD) and\nquery-based upsampling (QU) modules. The Shrunk++ structure reduces the\ncomputational cost of the encoder by up to $50\\%$ while maintaining competitive\nperformance. Furthermore, we propose to adapt SegViT for continual semantic\nsegmentation, demonstrating nearly zero forgetting of previously learned\nknowledge. Experiments show that our proposed SegViTv2 surpasses recent\nsegmentation methods on three popular benchmarks including ADE20k,\nCOCO-Stuff-10k and PASCAL-Context datasets. The code is available through the\nfollowing link: \\url{https://github.com/zbwxp/SegVit}.\n","authors":["Bowen Zhang","Liyang Liu","Minh Hieu Phan","Zhi Tian","Chunhua Shen","Yifan Liu"],"pdf_url":"https://arxiv.org/pdf/2306.06289v2.pdf","comment":"IJCV 2023 accepted, 21 pages, 8 figures, 12 tables"},{"id":"http://arxiv.org/abs/2305.14730v2","updated":"2023-08-30T13:00:09Z","published":"2023-05-24T05:06:59Z","title":"BinaryViT: Towards Efficient and Accurate Binary Vision Transformers","summary":"  Vision Transformers (ViTs) have emerged as the fundamental architecture for\nmost computer vision fields, but the considerable memory and computation costs\nhinders their application on resource-limited devices. As one of the most\npowerful compression methods, binarization reduces the computation of the\nneural network by quantizing the weights and activation values as $\\pm$1.\nAlthough existing binarization methods have demonstrated excellent performance\non Convolutional Neural Networks (CNNs), the full binarization of ViTs is still\nunder-studied and suffering a significant performance drop. In this paper, we\nfirst argue empirically that the severe performance degradation is mainly\ncaused by the weight oscillation in the binarization training and the\ninformation distortion in the activation of ViTs. Based on these analyses, we\npropose $\\textbf{BinaryViT}$, an accurate full binarization scheme for ViTs,\nwhich pushes the quantization of ViTs to the limit. Specifically, we propose a\nnovel gradient regularization scheme (GRS) for driving a bimodal distribution\nof the weights to reduce oscillation in binarization training. Moreover, we\ndesign an activation shift module (ASM) to adaptively tune the activation\ndistribution to reduce the information distortion caused by binarization.\nExtensive experiments on ImageNet dataset show that our BinaryViT consistently\nsurpasses the strong baseline by 2.05% and improve the accuracy of fully\nbinarized ViTs to a usable level. Furthermore, our method achieves impressive\nsavings of 16.2$\\times$ and 17.7$\\times$ in model size and OPs compared to the\nfull-precision DeiT-S.\n","authors":["Junrui Xiao","Zhikai Li","Lianwei Yang","Qingyi Gu"],"pdf_url":"https://arxiv.org/pdf/2305.14730v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.05593v3","updated":"2023-08-30T12:39:29Z","published":"2022-08-10T23:50:01Z","title":"Evaluating the Quality and Diversity of DCGAN-based Generatively\n  Synthesized Diabetic Retinopathy Imagery","summary":"  Publicly available diabetic retinopathy (DR) datasets are imbalanced,\ncontaining limited numbers of images with DR. This imbalance contributes to\noverfitting when training machine learning classifiers. The impact of this\nimbalance is exacerbated as the severity of the DR stage increases, affecting\nthe classifiers' diagnostic capacity. The imbalance can be addressed using\nGenerative Adversarial Networks (GANs) to augment the datasets with synthetic\nimages. Generating synthetic images is advantageous if high-quality and\ndiversified images are produced. To evaluate the quality and diversity of\nsynthetic images, several evaluation metrics, such as Multi-Scale Structural\nSimilarity Index (MS-SSIM), Cosine Distance (CD), and Fr\\'echet Inception\nDistance (FID) are used. Understanding the effectiveness of each metric in\nevaluating the quality and diversity of GAN-based synthetic images is critical\nto select images for augmentation. To date, there has been limited analysis of\nthe appropriateness of these metrics in the context of biomedical imagery. This\nwork contributes an empirical assessment of these evaluation metrics as applied\nto synthetic Proliferative DR imagery generated by a Deep Convolutional GAN\n(DCGAN). Furthermore, the metrics' capacity to indicate the quality and\ndiversity of synthetic images and a correlation with classifier performance is\nundertaken. This enables a quantitative selection of synthetic imagery and an\ninformed augmentation strategy. Results indicate that FID is suitable for\nevaluating the quality, while MS-SSIM and CD are suitable for evaluating the\ndiversity of synthetic imagery. Furthermore, the superior performance of\nConvolutional Neural Network (CNN) and EfficientNet classifiers, as indicated\nby the F1 and AUC scores, for the augmented datasets demonstrates the efficacy\nof synthetic imagery to augment the imbalanced dataset.\n","authors":["Cristina-Madalina Dragan","Muhammad Muneeb Saad","Mubashir Husain Rehmani","Ruairi O'Reilly"],"pdf_url":"https://arxiv.org/pdf/2208.05593v3.pdf","comment":"29 Pages, 8 Figures, submitted to MEDAL23: Advances in Deep\n  Generative Models for Medical Artificial Intelligence (Springer Nature\n  series)"},{"id":"http://arxiv.org/abs/2308.15996v1","updated":"2023-08-30T12:37:03Z","published":"2023-08-30T12:37:03Z","title":"DTrOCR: Decoder-only Transformer for Optical Character Recognition","summary":"  Typical text recognition methods rely on an encoder-decoder structure, in\nwhich the encoder extracts features from an image, and the decoder produces\nrecognized text from these features. In this study, we propose a simpler and\nmore effective method for text recognition, known as the Decoder-only\nTransformer for Optical Character Recognition (DTrOCR). This method uses a\ndecoder-only Transformer to take advantage of a generative language model that\nis pre-trained on a large corpus. We examined whether a generative language\nmodel that has been successful in natural language processing can also be\neffective for text recognition in computer vision. Our experiments demonstrated\nthat DTrOCR outperforms current state-of-the-art methods by a large margin in\nthe recognition of printed, handwritten, and scene text in both English and\nChinese.\n","authors":["Masato Fujitake"],"pdf_url":"https://arxiv.org/pdf/2308.15996v1.pdf","comment":"Accepted to WACV2024"},{"id":"http://arxiv.org/abs/2308.15989v1","updated":"2023-08-30T12:19:35Z","published":"2023-08-30T12:19:35Z","title":"DiffuVolume: Diffusion Model for Volume based Stereo Matching","summary":"  Stereo matching is a significant part in many computer vision tasks and\ndriving-based applications. Recently cost volume-based methods have achieved\ngreat success benefiting from the rich geometry information in paired images.\nHowever, the redundancy of cost volume also interferes with the model training\nand limits the performance. To construct a more precise cost volume, we\npioneeringly apply the diffusion model to stereo matching. Our method, termed\nDiffuVolume, considers the diffusion model as a cost volume filter, which will\nrecurrently remove the redundant information from the cost volume. Two main\ndesigns make our method not trivial. Firstly, to make the diffusion model more\nadaptive to stereo matching, we eschew the traditional manner of directly\nadding noise into the image but embed the diffusion model into a task-specific\nmodule. In this way, we outperform the traditional diffusion stereo matching\nmethod by 22% EPE improvement and 240 times inference acceleration. Secondly,\nDiffuVolume can be easily embedded into any volume-based stereo matching\nnetwork with boost performance but slight parameters rise (only 2%). By adding\nthe DiffuVolume into well-performed methods, we outperform all the published\nmethods on Scene Flow, KITTI2012, KITTI2015 benchmarks and zero-shot\ngeneralization setting. It is worth mentioning that the proposed model ranks\n1st on KITTI 2012 leader board, 2nd on KITTI 2015 leader board since 15, July\n2023.\n","authors":["Dian Zheng","Xiao-Ming Wu","Zuhao Liu","Jingke Meng","Wei-shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2308.15989v1.pdf","comment":"17 pages, 11 figures"},{"id":"http://arxiv.org/abs/2308.15984v1","updated":"2023-08-30T12:13:13Z","published":"2023-08-30T12:13:13Z","title":"Learning Structure-from-Motion with Graph Attention Networks","summary":"  In this paper we tackle the problem of learning Structure-from-Motion (SfM)\nthrough the use of graph attention networks. SfM is a classic computer vision\nproblem that is solved though iterative minimization of reprojection errors,\nreferred to as Bundle Adjustment (BA), starting from a good initialization. In\norder to obtain a good enough initialization to BA, conventional methods rely\non a sequence of sub-problems (such as pairwise pose estimation, pose averaging\nor triangulation) which provides an initial solution that can then be refined\nusing BA. In this work we replace these sub-problems by learning a model that\ntakes as input the 2D keypoints detected across multiple views, and outputs the\ncorresponding camera poses and 3D keypoint coordinates. Our model takes\nadvantage of graph neural networks to learn SfM-specific primitives, and we\nshow that it can be used for fast inference of the reconstruction for new and\nunseen sequences. The experimental results show that the proposed model\noutperforms competing learning-based methods, and challenges COLMAP while\nhaving lower runtime.\n","authors":["Lucas Brynte","José Pedro Iglesias","Carl Olsson","Fredrik Kahl"],"pdf_url":"https://arxiv.org/pdf/2308.15984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15975v1","updated":"2023-08-30T11:57:04Z","published":"2023-08-30T11:57:04Z","title":"RoboTAP: Tracking Arbitrary Points for Few-Shot Visual Imitation","summary":"  For robots to be useful outside labs and specialized factories we need a way\nto teach them new useful behaviors quickly. Current approaches lack either the\ngenerality to onboard new tasks without task-specific engineering, or else lack\nthe data-efficiency to do so in an amount of time that enables practical use.\nIn this work we explore dense tracking as a representational vehicle to allow\nfaster and more general learning from demonstration. Our approach utilizes\nTrack-Any-Point (TAP) models to isolate the relevant motion in a demonstration,\nand parameterize a low-level controller to reproduce this motion across changes\nin the scene configuration. We show this results in robust robot policies that\ncan solve complex object-arrangement tasks such as shape-matching, stacking,\nand even full path-following tasks such as applying glue and sticking objects\ntogether, all from demonstrations that can be collected in minutes.\n","authors":["Mel Vecerik","Carl Doersch","Yi Yang","Todor Davchev","Yusuf Aytar","Guangyao Zhou","Raia Hadsell","Lourdes Agapito","Jon Scholz"],"pdf_url":"https://arxiv.org/pdf/2308.15975v1.pdf","comment":"Project website: https://robotap.github.io"},{"id":"http://arxiv.org/abs/2308.02562v2","updated":"2023-08-30T11:47:05Z","published":"2023-08-03T04:03:46Z","title":"Food Classification using Joint Representation of Visual and Textual\n  Data","summary":"  Food classification is an important task in health care. In this work, we\npropose a multimodal classification framework that uses the modified version of\nEfficientNet with the Mish activation function for image classification, and\nthe traditional BERT transformer-based network is used for text classification.\nThe proposed network and the other state-of-the-art methods are evaluated on a\nlarge open-source dataset, UPMC Food-101. The experimental results show that\nthe proposed network outperforms the other methods, a significant difference of\n11.57% and 6.34% in accuracy is observed for image and text classification,\nrespectively, when compared with the second-best performing method. We also\ncompared the performance in terms of accuracy, precision, and recall for text\nclassification using both machine learning and deep learning-based models. The\ncomparative analysis from the prediction results of both images and text\ndemonstrated the efficiency and robustness of the proposed approach.\n","authors":["Prateek Mittal","Puneet Goyal","Joohi Chauhan"],"pdf_url":"https://arxiv.org/pdf/2308.02562v2.pdf","comment":"Updated results and discussions to be posted and some sections needed\n  to be expanded"},{"id":"http://arxiv.org/abs/2308.15966v1","updated":"2023-08-30T11:42:54Z","published":"2023-08-30T11:42:54Z","title":"SHARP Challenge 2023: Solving CAD History and pArameters Recovery from\n  Point clouds and 3D scans. Overview, Datasets, Metrics, and Baselines","summary":"  Recent breakthroughs in geometric Deep Learning (DL) and the availability of\nlarge Computer-Aided Design (CAD) datasets have advanced the research on\nlearning CAD modeling processes and relating them to real objects. In this\ncontext, 3D reverse engineering of CAD models from 3D scans is considered to be\none of the most sought-after goals for the CAD industry. However, recent\nefforts assume multiple simplifications limiting the applications in real-world\nsettings. The SHARP Challenge 2023 aims at pushing the research a step closer\nto the real-world scenario of CAD reverse engineering through dedicated\ndatasets and tracks. In this paper, we define the proposed SHARP 2023 tracks,\ndescribe the provided datasets, and propose a set of baseline methods along\nwith suitable evaluation metrics to assess the performance of the track\nsolutions. All proposed datasets along with useful routines and the evaluation\nmetrics are publicly available.\n","authors":["Dimitrios Mallis","Sk Aziz Ali","Elona Dupont","Kseniya Cherenkova","Ahmet Serdar Karadeniz","Mohammad Sadil Khan","Anis Kacem","Gleb Gusev","Djamila Aouada"],"pdf_url":"https://arxiv.org/pdf/2308.15966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15961v1","updated":"2023-08-30T11:35:21Z","published":"2023-08-30T11:35:21Z","title":"Finding-Aware Anatomical Tokens for Chest X-Ray Automated Reporting","summary":"  The task of radiology reporting comprises describing and interpreting the\nmedical findings in radiographic images, including description of their\nlocation and appearance. Automated approaches to radiology reporting require\nthe image to be encoded into a suitable token representation for input to the\nlanguage model. Previous methods commonly use convolutional neural networks to\nencode an image into a series of image-level feature map representations.\nHowever, the generated reports often exhibit realistic style but imperfect\naccuracy. Inspired by recent works for image captioning in the general domain\nin which each visual token corresponds to an object detected in an image, we\ninvestigate whether using local tokens corresponding to anatomical structures\ncan improve the quality of the generated reports. We introduce a novel\nadaptation of Faster R-CNN in which finding detection is performed for the\ncandidate bounding boxes extracted during anatomical structure localisation. We\nuse the resulting bounding box feature representations as our set of\nfinding-aware anatomical tokens. This encourages the extracted anatomical\ntokens to be informative about the findings they contain (required for the\nfinal task of radiology reporting). Evaluating on the MIMIC-CXR dataset of\nchest X-Ray images, we show that task-aware anatomical tokens give\nstate-of-the-art performance when integrated into an automated reporting\npipeline, yielding generated reports with improved clinical accuracy.\n","authors":["Francesco Dalla Serra","Chaoyang Wang","Fani Deligianni","Jeffrey Dalton","Alison Q. O'Neil"],"pdf_url":"https://arxiv.org/pdf/2308.15961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15960v1","updated":"2023-08-30T11:33:07Z","published":"2023-08-30T11:33:07Z","title":"Fusing Pseudo Labels with Weak Supervision for Dynamic Traffic Scenarios","summary":"  Advanced Driver Assistance Systems (ADAS) have made significant strides,\ncapitalizing on computer vision to enhance perception and decision-making\ncapabilities. Nonetheless, the adaptation of these systems to diverse traffic\nscenarios poses challenges due to shifts in data distribution stemming from\nfactors such as location, weather, and road infrastructure. To tackle this, we\nintroduce a weakly-supervised label unification pipeline that amalgamates\npseudo labels from a multitude of object detection models trained on\nheterogeneous datasets. Our pipeline engenders a unified label space through\nthe amalgamation of labels from disparate datasets, rectifying bias and\nenhancing generalization. We fine-tune multiple object detection models on\nindividual datasets, subsequently crafting a unified dataset featuring pseudo\nlabels, meticulously validated for precision. Following this, we retrain a\nsolitary object detection model using the merged label space, culminating in a\nresilient model proficient in dynamic traffic scenarios. We put forth a\ncomprehensive evaluation of our approach, employing diverse datasets\noriginating from varied Asian countries, effectively demonstrating its efficacy\nin challenging road conditions. Notably, our method yields substantial\nenhancements in object detection performance, culminating in a model with\nheightened resistance against domain shifts.\n","authors":["Harshith Mohan Kumar","Sean Lawrence"],"pdf_url":"https://arxiv.org/pdf/2308.15960v1.pdf","comment":"This work was accepted as an extended abstract at the International\n  Conference on Computer Vision (ICCV) 2023 BRAVO Workshop, Paris, France"},{"id":"http://arxiv.org/abs/2209.15376v3","updated":"2023-08-30T11:04:14Z","published":"2022-09-30T11:09:54Z","title":"NBV-SC: Next Best View Planning based on Shape Completion for Fruit\n  Mapping and Reconstruction","summary":"  Active perception for fruit mapping and harvesting is a difficult task since\nocclusions occur frequently and the location as well as size of fruits change\nover time. State-of-the-art viewpoint planning approaches utilize\ncomputationally expensive ray casting operations to find good viewpoints aiming\nat maximizing information gain and covering the fruits in the scene. In this\npaper, we present a novel viewpoint planning approach that explicitly uses\ninformation about the predicted fruit shapes to compute targeted viewpoints\nthat observe as yet unobserved parts of the fruits. Furthermore, we formulate\nthe concept of viewpoint dissimilarity to reduce the sampling space for more\nefficient selection of useful, dissimilar viewpoints. Our simulation\nexperiments with a UR5e arm equipped with an RGB-D sensor provide a\nquantitative demonstration of the efficacy of our iterative next best view\nplanning method based on shape completion. In comparative experiments with a\nstate-of-the-art viewpoint planner, we demonstrate improvement not only in the\nestimation of the fruit sizes, but also in their reconstruction, while\nsignificantly reducing the planning time. Finally, we show the viability of our\napproach for mapping sweet peppers plants with a real robotic system in a\ncommercial glasshouse.\n","authors":["Rohit Menon","Tobias Zaenker","Nils Dengler","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2209.15376v3.pdf","comment":"Agricultural Automation, Viewpoint Planning, Active Perception, Shape\n  Completion"},{"id":"http://arxiv.org/abs/2308.15949v1","updated":"2023-08-30T10:57:41Z","published":"2023-08-30T10:57:41Z","title":"Latency-aware Unified Dynamic Networks for Efficient Image Recognition","summary":"  Dynamic computation has emerged as a promising avenue to enhance the\ninference efficiency of deep networks. It allows selective activation of\ncomputational units, leading to a reduction in unnecessary computations for\neach input sample. However, the actual efficiency of these dynamic models can\ndeviate from theoretical predictions. This mismatch arises from: 1) the lack of\na unified approach due to fragmented research; 2) the focus on algorithm design\nover critical scheduling strategies, especially in CUDA-enabled GPU contexts;\nand 3) challenges in measuring practical latency, given that most libraries\ncater to static operations. Addressing these issues, we unveil the\nLatency-Aware Unified Dynamic Networks (LAUDNet), a framework that integrates\nthree primary dynamic paradigms-spatially adaptive computation, dynamic layer\nskipping, and dynamic channel skipping. To bridge the theoretical and practical\nefficiency gap, LAUDNet merges algorithmic design with scheduling optimization,\nguided by a latency predictor that accurately gauges dynamic operator latency.\nWe've tested LAUDNet across multiple vision tasks, demonstrating its capacity\nto notably reduce the latency of models like ResNet-101 by over 50% on\nplatforms such as V100, RTX3090, and TX2 GPUs. Notably, LAUDNet stands out in\nbalancing accuracy and efficiency. Code is available at:\nhttps://www.github.com/LeapLabTHU/LAUDNet.\n","authors":["Yizeng Han","Zeyu Liu","Zhihang Yuan","Yifan Pu","Chaofei Wang","Shiji Song","Gao Huang"],"pdf_url":"https://arxiv.org/pdf/2308.15949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15942v1","updated":"2023-08-30T10:48:53Z","published":"2023-08-30T10:48:53Z","title":"Stage-by-stage Wavelet Optimization Refinement Diffusion Model for\n  Sparse-View CT Reconstruction","summary":"  Diffusion models have emerged as potential tools to tackle the challenge of\nsparse-view CT reconstruction, displaying superior performance compared to\nconventional methods. Nevertheless, these prevailing diffusion models\npredominantly focus on the sinogram or image domains, which can lead to\ninstability during model training, potentially culminating in convergence\ntowards local minimal solutions. The wavelet trans-form serves to disentangle\nimage contents and features into distinct frequency-component bands at varying\nscales, adeptly capturing diverse directional structures. Employing the Wavelet\ntransform as a guiding sparsity prior significantly enhances the robustness of\ndiffusion models. In this study, we present an innovative approach named the\nStage-by-stage Wavelet Optimization Refinement Diffusion (SWORD) model for\nsparse-view CT reconstruction. Specifically, we establish a unified\nmathematical model integrating low-frequency and high-frequency generative\nmodels, achieving the solution with optimization procedure. Furthermore, we\nperform the low-frequency and high-frequency generative models on wavelet's\ndecomposed components rather than sinogram or image domains, ensuring the\nstability of model training. Our method rooted in established optimization\ntheory, comprising three distinct stages, including low-frequency generation,\nhigh-frequency refinement and domain transform. Our experimental results\ndemonstrate that the proposed method outperforms existing state-of-the-art\nmethods both quantitatively and qualitatively.\n","authors":["Kai Xu","Shiyu Lu","Bin Huang","Weiwen Wu","Qiegen Liu"],"pdf_url":"https://arxiv.org/pdf/2308.15942v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16098v5","updated":"2023-08-30T10:48:12Z","published":"2022-11-29T11:17:34Z","title":"Three-stage binarization of color document images based on discrete\n  wavelet transform and generative adversarial networks","summary":"  The efficient segmentation of foreground text information from the background\nin degraded color document images is a critical challenge in the preservation\nof ancient manuscripts. The imperfect preservation of ancient manuscripts over\ntime has led to various types of degradation, such as staining, yellowing, and\nink seepage, significantly affecting image binarization results. This work\nproposes a three-stage method using Generative Adversarial Networks (GAN) for\nenhancing and binarizing degraded color document images through Discrete\nWavelet Transform (DWT). Stage-1 involves applying DWT and retaining the\nLow-Low (LL) subband images for image enhancement. In Stage-2, the original\ninput image is divided into four single-channel images (Red, Green, Blue, and\nGray), and each is trained with independent adversarial networks to extract\ncolor foreground information. In Stage-3, the output image from Stage-2 and the\noriginal input image are used to train independent adversarial networks for\ndocument binarization, enabling the integration of global and local features.\nThe experimental results demonstrate that our proposed method outperforms other\nclassic and state-of-the-art (SOTA) methods on the Document Image Binarization\nContest (DIBCO) datasets. We have released our implementation code at\nhttps://github.com/abcpp12383/ThreeStageBinarization.\n","authors":["Rui-Yang Ju","Yu-Shian Lin","Chih-Chia Chen","Chun-Tse Chien","Jen-Shiun Chiang"],"pdf_url":"https://arxiv.org/pdf/2211.16098v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15428v2","updated":"2023-08-30T10:38:41Z","published":"2023-07-28T09:26:00Z","title":"Implicit neural representation for change detection","summary":"  Identifying changes in a pair of 3D aerial LiDAR point clouds, obtained\nduring two distinct time periods over the same geographic region presents a\nsignificant challenge due to the disparities in spatial coverage and the\npresence of noise in the acquisition system. The most commonly used approaches\nto detecting changes in point clouds are based on supervised methods which\nnecessitate extensive labelled data often unavailable in real-world\napplications. To address these issues, we propose an unsupervised approach that\ncomprises two components: Implicit Neural Representation (INR) for continuous\nshape reconstruction and a Gaussian Mixture Model for categorising changes. INR\noffers a grid-agnostic representation for encoding bi-temporal point clouds,\nwith unmatched spatial support that can be regularised to enhance\nhigh-frequency details and reduce noise. The reconstructions at each timestamp\nare compared at arbitrary spatial scales, leading to a significant increase in\ndetection capabilities. We apply our method to a benchmark dataset comprising\nsimulated LiDAR point clouds for urban sprawling. This dataset encompasses\ndiverse challenging scenarios, varying in resolutions, input modalities and\nnoise levels. This enables a comprehensive multi-scenario evaluation, comparing\nour method with the current state-of-the-art approach. We outperform the\nprevious methods by a margin of 10% in the intersection over union metric. In\naddition, we put our techniques to practical use by applying them in a\nreal-world scenario to identify instances of illicit excavation of\narchaeological sites and validate our results by comparing them with findings\nfrom field experts.\n","authors":["Peter Naylor","Diego Di Carlo","Arianna Traviglia","Makoto Yamada","Marco Fiorucci"],"pdf_url":"https://arxiv.org/pdf/2307.15428v2.pdf","comment":"Main article is 10 pages + 6 pages of supplementary. Conference style\n  paper"},{"id":"http://arxiv.org/abs/2308.15939v1","updated":"2023-08-30T10:35:36Z","published":"2023-08-30T10:35:36Z","title":"AnoVL: Adapting Vision-Language Models for Unified Zero-shot Anomaly\n  Localization","summary":"  Contrastive Language-Image Pre-training (CLIP) models have shown promising\nperformance on zero-shot visual recognition tasks by learning visual\nrepresentations under natural language supervision. Recent studies attempt the\nuse of CLIP to tackle zero-shot anomaly detection by matching images with\nnormal and abnormal state prompts. However, since CLIP focuses on building\ncorrespondence between paired text prompts and global image-level\nrepresentations, the lack of patch-level vision to text alignment limits its\ncapability on precise visual anomaly localization. In this work, we introduce a\ntraining-free adaptation (TFA) framework of CLIP for zero-shot anomaly\nlocalization. In the visual encoder, we innovate a training-free value-wise\nattention mechanism to extract intrinsic local tokens of CLIP for patch-level\nlocal description. From the perspective of text supervision, we particularly\ndesign a unified domain-aware contrastive state prompting template. On top of\nthe proposed TFA, we further introduce a test-time adaptation (TTA) mechanism\nto refine anomaly localization results, where a layer of trainable parameters\nin the adapter is optimized using TFA's pseudo-labels and synthetic\nnoise-corrupted tokens. With both TFA and TTA adaptation, we significantly\nexploit the potential of CLIP for zero-shot anomaly localization and\ndemonstrate the effectiveness of our proposed methods on various datasets.\n","authors":["Hanqiu Deng","Zhaoxiang Zhang","Jinan Bao","Xingyu Li"],"pdf_url":"https://arxiv.org/pdf/2308.15939v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15932v1","updated":"2023-08-30T10:21:57Z","published":"2023-08-30T10:21:57Z","title":"Attention-based CT Scan Interpolation for Lesion Segmentation of\n  Colorectal Liver Metastases","summary":"  Small liver lesions common to colorectal liver metastases (CRLMs) are\nchallenging for convolutional neural network (CNN) segmentation models,\nespecially when we have a wide range of slice thicknesses in the computed\ntomography (CT) scans. Slice thickness of CT images may vary by clinical\nindication. For example, thinner slices are used for presurgical planning when\nfine anatomic details of small vessels are required. While keeping the\neffective radiation dose in patients as low as possible, various slice\nthicknesses are employed in CRLMs due to their limitations. However,\ndifferences in slice thickness across CTs lead to significant performance\ndegradation in CT segmentation models based on CNNs. This paper proposes a\nnovel unsupervised attention-based interpolation model to generate intermediate\nslices from consecutive triplet slices in CT scans. We integrate segmentation\nloss during the interpolation model's training to leverage segmentation labels\nin existing slices to generate middle ones. Unlike common interpolation\ntechniques in CT volumes, our model highlights the regions of interest (liver\nand lesions) inside the abdominal CT scans in the interpolated slice. Moreover,\nour model's outputs are consistent with the original input slices while\nincreasing the segmentation performance in two cutting-edge 3D segmentation\npipelines. We tested the proposed model on the CRLM dataset to upsample\nsubjects with thick slices and create isotropic volume for our segmentation\nmodel. The produced isotropic dataset increases the Dice score in the\nsegmentation of lesions and outperforms other interpolation approaches in terms\nof interpolation metrics.\n","authors":["Mohammad Hamghalam","Richard K. G. Do","Amber L. Simpson"],"pdf_url":"https://arxiv.org/pdf/2308.15932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09829v2","updated":"2023-08-30T10:19:02Z","published":"2023-07-19T08:34:25Z","title":"What do neural networks learn in image classification? A frequency\n  shortcut perspective","summary":"  Frequency analysis is useful for understanding the mechanisms of\nrepresentation learning in neural networks (NNs). Most research in this area\nfocuses on the learning dynamics of NNs for regression tasks, while little for\nclassification. This study empirically investigates the latter and expands the\nunderstanding of frequency shortcuts. First, we perform experiments on\nsynthetic datasets, designed to have a bias in different frequency bands. Our\nresults demonstrate that NNs tend to find simple solutions for classification,\nand what they learn first during training depends on the most distinctive\nfrequency characteristics, which can be either low- or high-frequencies.\nSecond, we confirm this phenomenon on natural images. We propose a metric to\nmeasure class-wise frequency characteristics and a method to identify frequency\nshortcuts. The results show that frequency shortcuts can be texture-based or\nshape-based, depending on what best simplifies the objective. Third, we\nvalidate the transferability of frequency shortcuts on out-of-distribution\n(OOD) test sets. Our results suggest that frequency shortcuts can be\ntransferred across datasets and cannot be fully avoided by larger model\ncapacity and data augmentation. We recommend that future research should focus\non effective training schemes mitigating frequency shortcut learning.\n","authors":["Shunxin Wang","Raymond Veldhuis","Christoph Brune","Nicola Strisciuglio"],"pdf_url":"https://arxiv.org/pdf/2307.09829v2.pdf","comment":"Accepted at ICCV2023"},{"id":"http://arxiv.org/abs/2308.15918v1","updated":"2023-08-30T09:45:14Z","published":"2023-08-30T09:45:14Z","title":"Physics-Informed DeepMRI: Bridging the Gap from Heat Diffusion to\n  k-Space Interpolation","summary":"  In the field of parallel imaging (PI), alongside image-domain regularization\nmethods, substantial research has been dedicated to exploring $k$-space\ninterpolation. However, the interpretability of these methods remains an\nunresolved issue. Furthermore, these approaches currently face acceleration\nlimitations that are comparable to those experienced by image-domain methods.\nIn order to enhance interpretability and overcome the acceleration limitations,\nthis paper introduces an interpretable framework that unifies both $k$-space\ninterpolation techniques and image-domain methods, grounded in the physical\nprinciples of heat diffusion equations. Building upon this foundational\nframework, a novel $k$-space interpolation method is proposed. Specifically, we\nmodel the process of high-frequency information attenuation in $k$-space as a\nheat diffusion equation, while the effort to reconstruct high-frequency\ninformation from low-frequency regions can be conceptualized as a reverse heat\nequation. However, solving the reverse heat equation poses a challenging\ninverse problem. To tackle this challenge, we modify the heat equation to align\nwith the principles of magnetic resonance PI physics and employ the score-based\ngenerative method to precisely execute the modified reverse heat diffusion.\nFinally, experimental validation conducted on publicly available datasets\ndemonstrates the superiority of the proposed approach over traditional\n$k$-space interpolation methods, deep learning-based $k$-space interpolation\nmethods, and conventional diffusion models in terms of reconstruction accuracy,\nparticularly in high-frequency regions.\n","authors":["Zhuo-Xu Cui","Congcong Liu","Xiaohong Fan","Chentao Cao","Jing Cheng","Qingyong Zhu","Yuanyuan Liu","Sen Jia","Yihang Zhou","Haifeng Wang","Yanjie Zhu","Jianping Zhang","Qiegen Liu","Dong Liang"],"pdf_url":"https://arxiv.org/pdf/2308.15918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14027v2","updated":"2023-08-30T09:26:11Z","published":"2023-03-24T14:37:07Z","title":"Poincaré ResNet","summary":"  This paper introduces an end-to-end residual network that operates entirely\non the Poincar\\'e ball model of hyperbolic space. Hyperbolic learning has\nrecently shown great potential for visual understanding, but is currently only\nperformed in the penultimate layer(s) of deep networks. All visual\nrepresentations are still learned through standard Euclidean networks. In this\npaper we investigate how to learn hyperbolic representations of visual data\ndirectly from the pixel-level. We propose Poincar\\'e ResNet, a hyperbolic\ncounterpart of the celebrated residual network, starting from Poincar\\'e 2D\nconvolutions up to Poincar\\'e residual connections. We identify three\nroadblocks for training convolutional networks entirely in hyperbolic space and\npropose a solution for each: (i) Current hyperbolic network initializations\ncollapse to the origin, limiting their applicability in deeper networks. We\nprovide an identity-based initialization that preserves norms over many layers.\n(ii) Residual networks rely heavily on batch normalization, which comes with\nexpensive Fr\\'echet mean calculations in hyperbolic space. We introduce\nPoincar\\'e midpoint batch normalization as a faster and equally effective\nalternative. (iii) Due to the many intermediate operations in Poincar\\'e\nlayers, we lastly find that the computation graphs of deep learning libraries\nblow up, limiting our ability to train on deep hyperbolic networks. We provide\nmanual backward derivations of core hyperbolic operations to maintain\nmanageable computation graphs.\n","authors":["Max van Spengler","Erwin Berkhout","Pascal Mettes"],"pdf_url":"https://arxiv.org/pdf/2303.14027v2.pdf","comment":"International Conference on Computer Vision 2023"},{"id":"http://arxiv.org/abs/2308.15887v1","updated":"2023-08-30T09:04:24Z","published":"2023-08-30T09:04:24Z","title":"On the Potential of CLIP for Compositional Logical Reasoning","summary":"  In this paper we explore the possibility of using OpenAI's CLIP to perform\nlogically coherent grounded visual reasoning. To that end, we formalize our\nterms and give a geometric analysis of how embeddings in CLIP's latent space\nwould need to be configured in order for the system to be logically coherent.\nOur main conclusion is that, as usually configured, CLIP cannot perform such\nreasoning.\n","authors":["Justin Brody"],"pdf_url":"https://arxiv.org/pdf/2308.15887v1.pdf","comment":"In Proceedings ICLP 2023, arXiv:2308.14898"},{"id":"http://arxiv.org/abs/2308.15881v1","updated":"2023-08-30T09:03:28Z","published":"2023-08-30T09:03:28Z","title":"Interpretability-guided Data Augmentation for Robust Segmentation in\n  Multi-centre Colonoscopy Data","summary":"  Multi-centre colonoscopy images from various medical centres exhibit distinct\ncomplicating factors and overlays that impact the image content, contingent on\nthe specific acquisition centre. Existing Deep Segmentation networks struggle\nto achieve adequate generalizability in such data sets, and the currently\navailable data augmentation methods do not effectively address these sources of\ndata variability. As a solution, we introduce an innovative data augmentation\napproach centred on interpretability saliency maps, aimed at enhancing the\ngeneralizability of Deep Learning models within the realm of multi-centre\ncolonoscopy image segmentation. The proposed augmentation technique\ndemonstrates increased robustness across different segmentation models and\ndomains. Thorough testing on a publicly available multi-centre dataset for\npolyp detection demonstrates the effectiveness and versatility of our approach,\nwhich is observed both in quantitative and qualitative results. The code is\npublicly available at:\nhttps://github.com/nki-radiology/interpretability_augmentation\n","authors":["Valentina Corbetta","Regina Beets-Tan","Wilson Silva"],"pdf_url":"https://arxiv.org/pdf/2308.15881v1.pdf","comment":"10 pages, 4 figures, 1 table, accepted at MICCAI 2023 Workshop on\n  Machine Learning in Medical Imaging (MLMI)"},{"id":"http://arxiv.org/abs/2308.15868v1","updated":"2023-08-30T08:56:36Z","published":"2023-08-30T08:56:36Z","title":"Feature Attention Network (FA-Net): A Deep-Learning Based Approach for\n  Underwater Single Image Enhancement","summary":"  Underwater image processing and analysis have been a hotspot of study in\nrecent years, as more emphasis has been focused to underwater monitoring and\nusage of marine resources. Compared with the open environment, underwater image\nencountered with more complicated conditions such as light abortion,\nscattering, turbulence, nonuniform illumination and color diffusion. Although\nconsiderable advances and enhancement techniques achieved in resolving these\nissues, they treat low-frequency information equally across the entire channel,\nwhich results in limiting the network's representativeness. We propose a deep\nlearning and feature-attention-based end-to-end network (FA-Net) to solve this\nproblem. In particular, we propose a Residual Feature Attention Block (RFAB),\ncontaining the channel attention, pixel attention, and residual learning\nmechanism with long and short skip connections. RFAB allows the network to\nfocus on learning high-frequency information while skipping low-frequency\ninformation on multi-hop connections. The channel and pixel attention mechanism\nconsiders each channel's different features and the uneven distribution of haze\nover different pixels in the image. The experimental results shows that the\nFA-Net propose by us provides higher accuracy, quantitatively and qualitatively\nand superiority to previous state-of-the-art methods.\n","authors":["Muhammad Hamza","Ammar Hawbani","Sami Ul Rehman","Xingfu Wang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2308.15868v1.pdf","comment":"Fourteenth International Conference on Digital Image Processing\n  (ICDIP 2022), 2022, Wuhan, China, May 20-23, 2022.8 pages.5 Figures.doi:\n  10.1117/12.2644516"},{"id":"http://arxiv.org/abs/2308.15855v1","updated":"2023-08-30T08:44:21Z","published":"2023-08-30T08:44:21Z","title":"Semi-supervised Domain Adaptation with Inter and Intra-domain Mixing for\n  Semantic Segmentation","summary":"  Despite recent advances in semantic segmentation, an inevitable challenge is\nthe performance degradation caused by the domain shift in real application.\nCurrent dominant approach to solve this problem is unsupervised domain\nadaptation (UDA). However, the absence of labeled target data in UDA is overly\nrestrictive and limits performance. To overcome this limitation, a more\npractical scenario called semi-supervised domain adaptation (SSDA) has been\nproposed. Existing SSDA methods are derived from the UDA paradigm and primarily\nfocus on leveraging the unlabeled target data and source data. In this paper,\nwe highlight the significance of exploiting the intra-domain information\nbetween the limited labeled target data and unlabeled target data, as it\ngreatly benefits domain adaptation. Instead of solely using the scarce labeled\ndata for supervision, we propose a novel SSDA framework that incorporates both\ninter-domain mixing and intra-domain mixing, where inter-domain mixing\nmitigates the source-target domain gap and intra-domain mixing enriches the\navailable target domain information. By simultaneously learning from\ninter-domain mixing and intra-domain mixing, the network can capture more\ndomain-invariant features and promote its performance on the target domain. We\nalso explore different domain mixing operations to better exploit the target\ndomain information. Comprehensive experiments conducted on the GTA5toCityscapes\nand SYNTHIA2Cityscapes benchmarks demonstrate the effectiveness of our method,\nsurpassing previous methods by a large margin.\n","authors":["Weifu Fu","Qiang Nie","Jialin Li","Yuhuan Lin","Kai Wu","Yong Liu","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2308.15855v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2211.12436v2","updated":"2023-08-30T08:40:16Z","published":"2022-11-22T17:45:06Z","title":"Dynamic Depth-Supervised NeRF for Multi-View RGB-D Operating Room Images","summary":"  The operating room (OR) is an environment of interest for the development of\nsensing systems, enabling the detection of people, objects, and their semantic\nrelations. Due to frequent occlusions in the OR, these systems often rely on\ninput from multiple cameras. While increasing the number of cameras generally\nincreases algorithm performance, there are hard limitations to the number and\nlocations of cameras in the OR. Neural Radiance Fields (NeRF) can be used to\nrender synthetic views from arbitrary camera positions, virtually enlarging the\nnumber of cameras in the dataset. In this work, we explore the use of NeRF for\nview synthesis of dynamic scenes in the OR, and we show that regularisation\nwith depth supervision from RGB-D sensor data results in higher image quality.\nWe optimise a dynamic depth-supervised NeRF with up to six synchronised cameras\nthat capture the surgical field in five distinct phases before and during a\nknee replacement surgery. We qualitatively inspect views rendered by a virtual\ncamera that moves 180 degrees around the surgical field at differing time\nvalues. Quantitatively, we evaluate view synthesis from an unseen camera\nposition in terms of PSNR, SSIM and LPIPS for the colour channels and in MAE\nand error percentage for the estimated depth. We find that NeRFs can be used to\ngenerate geometrically consistent views, also from interpolated camera\npositions and at interpolated time intervals. Views are generated from an\nunseen camera pose with an average PSNR of 18.2 and a depth estimation error of\n2.0%. Our results show the potential of a dynamic NeRF for view synthesis in\nthe OR and stress the relevance of depth supervision in a clinical setting.\n","authors":["Beerend G. A. Gerats","Jelmer M. Wolterink","Ivo A. M. J. Broeders"],"pdf_url":"https://arxiv.org/pdf/2211.12436v2.pdf","comment":"Accepted to the Workshop on Ambient Intelligence for HealthCare 2023"},{"id":"http://arxiv.org/abs/2308.15854v1","updated":"2023-08-30T08:40:15Z","published":"2023-08-30T08:40:15Z","title":"Zero-shot Inversion Process for Image Attribute Editing with Diffusion\n  Models","summary":"  Denoising diffusion models have shown outstanding performance in image\nediting. Existing works tend to use either image-guided methods, which provide\na visual reference but lack control over semantic coherence, or text-guided\nmethods, which ensure faithfulness to text guidance but lack visual quality. To\naddress the problem, we propose the Zero-shot Inversion Process (ZIP), a\nframework that injects a fusion of generated visual reference and text guidance\ninto the semantic latent space of a \\textit{frozen} pre-trained diffusion\nmodel. Only using a tiny neural network, the proposed ZIP produces diverse\ncontent and attributes under the intuitive control of the text prompt.\nMoreover, ZIP shows remarkable robustness for both in-domain and out-of-domain\nattribute manipulation on real images. We perform detailed experiments on\nvarious benchmark datasets. Compared to state-of-the-art methods, ZIP produces\nimages of equivalent quality while providing a realistic editing effect.\n","authors":["Zhanbo Feng","Zenan Ling","Ci Gong","Feng Zhou","Jie Li","Robert C. Qiu"],"pdf_url":"https://arxiv.org/pdf/2308.15854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14847v2","updated":"2023-08-30T08:34:08Z","published":"2023-08-28T19:08:17Z","title":"NSF: Neural Surface Fields for Human Modeling from Monocular Depth","summary":"  Obtaining personalized 3D animatable avatars from a monocular camera has\nseveral real world applications in gaming, virtual try-on, animation, and\nVR/XR, etc. However, it is very challenging to model dynamic and fine-grained\nclothing deformations from such sparse data. Existing methods for modeling 3D\nhumans from depth data have limitations in terms of computational efficiency,\nmesh coherency, and flexibility in resolution and topology. For instance,\nreconstructing shapes using implicit functions and extracting explicit meshes\nper frame is computationally expensive and cannot ensure coherent meshes across\nframes. Moreover, predicting per-vertex deformations on a pre-designed human\ntemplate with a discrete surface lacks flexibility in resolution and topology.\nTo overcome these limitations, we propose a novel method `\\keyfeature: Neural\nSurface Fields' for modeling 3D clothed humans from monocular depth. NSF\ndefines a neural field solely on the base surface which models a continuous and\nflexible displacement field. NSF can be adapted to the base surface with\ndifferent resolution and topology without retraining at inference time.\nCompared to existing approaches, our method eliminates the expensive per-frame\nsurface extraction while maintaining mesh coherency, and is capable of\nreconstructing meshes with arbitrary resolution without retraining. To foster\nresearch in this direction, we release our code in project page at:\nhttps://yuxuan-xue.com/nsf.\n","authors":["Yuxuan Xue","Bharat Lal Bhatnagar","Riccardo Marin","Nikolaos Sarafianos","Yuanlu Xu","Gerard Pons-Moll","Tony Tung"],"pdf_url":"https://arxiv.org/pdf/2308.14847v2.pdf","comment":"Accpted to ICCV 2023; Homepage at: https://yuxuan-xue.com/nsf"},{"id":"http://arxiv.org/abs/2308.15846v1","updated":"2023-08-30T08:33:13Z","published":"2023-08-30T08:33:13Z","title":"Exploring Multi-Modal Contextual Knowledge for Open-Vocabulary Object\n  Detection","summary":"  In this paper, we for the first time explore helpful multi-modal contextual\nknowledge to understand novel categories for open-vocabulary object detection\n(OVD). The multi-modal contextual knowledge stands for the joint relationship\nacross regions and words. However, it is challenging to incorporate such\nmulti-modal contextual knowledge into OVD. The reason is that previous\ndetection frameworks fail to jointly model multi-modal contextual knowledge, as\nobject detectors only support vision inputs and no caption description is\nprovided at test time. To this end, we propose a multi-modal contextual\nknowledge distillation framework, MMC-Det, to transfer the learned contextual\nknowledge from a teacher fusion transformer with diverse multi-modal masked\nlanguage modeling (D-MLM) to a student detector. The diverse multi-modal masked\nlanguage modeling is realized by an object divergence constraint upon\ntraditional multi-modal masked language modeling (MLM), in order to extract\nfine-grained region-level visual contexts, which are vital to object detection.\nExtensive experiments performed upon various detection datasets show the\neffectiveness of our multi-modal context learning strategy, where our approach\nwell outperforms the recent state-of-the-art methods.\n","authors":["Yifan Xu","Mengdan Zhang","Xiaoshan Yang","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2308.15846v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15844v1","updated":"2023-08-30T08:31:55Z","published":"2023-08-30T08:31:55Z","title":"Reconstructing Groups of People with Hypergraph Relational Reasoning","summary":"  Due to the mutual occlusion, severe scale variation, and complex spatial\ndistribution, the current multi-person mesh recovery methods cannot produce\naccurate absolute body poses and shapes in large-scale crowded scenes. To\naddress the obstacles, we fully exploit crowd features for reconstructing\ngroups of people from a monocular image. A novel hypergraph relational\nreasoning network is proposed to formulate the complex and high-order relation\ncorrelations among individuals and groups in the crowd. We first extract\ncompact human features and location information from the original\nhigh-resolution image. By conducting the relational reasoning on the extracted\nindividual features, the underlying crowd collectiveness and interaction\nrelationship can provide additional group information for the reconstruction.\nFinally, the updated individual features and the localization information are\nused to regress human meshes in camera coordinates. To facilitate the network\ntraining, we further build pseudo ground-truth on two crowd datasets, which may\nalso promote future research on pose estimation and human behavior\nunderstanding in crowded scenes. The experimental results show that our\napproach outperforms other baseline methods both in crowded and common\nscenarios. The code and datasets are publicly available at\nhttps://github.com/boycehbz/GroupRec.\n","authors":["Buzhen Huang","Jingyi Ju","Zhihao Li","Yangang Wang"],"pdf_url":"https://arxiv.org/pdf/2308.15844v1.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2308.15839v1","updated":"2023-08-30T08:21:52Z","published":"2023-08-30T08:21:52Z","title":"Utilizing Task-Generic Motion Prior to Recover Full-Body Motion from\n  Very Sparse Signals","summary":"  The most popular type of devices used to track a user's posture in a virtual\nreality experience consists of a head-mounted display and two controllers held\nin both hands. However, due to the limited number of tracking sensors (three in\ntotal), faithfully recovering the user in full-body is challenging, limiting\nthe potential for interactions among simulated user avatars within the virtual\nworld. Therefore, recent studies have attempted to reconstruct full-body poses\nusing neural networks that utilize previously learned human poses or accept a\nseries of past poses over a short period. In this paper, we propose a method\nthat utilizes information from a neural motion prior to improve the accuracy of\nreconstructed user's motions. Our approach aims to reconstruct user's full-body\nposes by predicting the latent representation of the user's overall motion from\nlimited input signals and integrating this information with tracking sensor\ninputs. This is based on the premise that the ultimate goal of pose\nreconstruction is to reconstruct the motion, which is a series of poses. Our\nresults show that this integration enables more accurate reconstruction of the\nuser's full-body motion, particularly enhancing the robustness of lower body\nmotion reconstruction from impoverished signals. Web:\nhttps://https://mjsh34.github.io/mp-sspe/\n","authors":["Myungjin Shin","Dohae Lee","In-Kwon Lee"],"pdf_url":"https://arxiv.org/pdf/2308.15839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06262v4","updated":"2023-08-30T08:21:13Z","published":"2023-01-16T05:08:50Z","title":"Collaborative Perception in Autonomous Driving: Methods, Datasets and\n  Challenges","summary":"  Collaborative perception is essential to address occlusion and sensor failure\nissues in autonomous driving. In recent years, theoretical and experimental\ninvestigations of novel works for collaborative perception have increased\ntremendously. So far, however, few reviews have focused on systematical\ncollaboration modules and large-scale collaborative perception datasets. This\nwork reviews recent achievements in this field to bridge this gap and motivate\nfuture research. We start with a brief overview of collaboration schemes. After\nthat, we systematically summarize the collaborative perception methods for\nideal scenarios and real-world issues. The former focuses on collaboration\nmodules and efficiency, and the latter is devoted to addressing the problems in\nactual application. Furthermore, we present large-scale public datasets and\nsummarize quantitative results on these benchmarks. Finally, we highlight gaps\nand overlook challenges between current academic research and real-world\napplications. The project page is\nhttps://github.com/CatOneTwo/Collaborative-Perception-in-Autonomous-Driving\n","authors":["Yushan Han","Hui Zhang","Huifang Li","Yi Jin","Congyan Lang","Yidong Li"],"pdf_url":"https://arxiv.org/pdf/2301.06262v4.pdf","comment":"18 pages, 6 figures. Accepted by IEEE Intelligent Transportation\n  Systems Magazine. URL:\n  https://github.com/CatOneTwo/Collaborative-Perception-in-Autonomous-Driving"},{"id":"http://arxiv.org/abs/2308.15321v2","updated":"2023-08-30T08:20:30Z","published":"2023-08-29T14:16:09Z","title":"Elucidating the Exposure Bias in Diffusion Models","summary":"  Diffusion models have demonstrated impressive generative capabilities, but\ntheir 'exposure bias' problem, described as the input mismatch between training\nand sampling, lacks in-depth exploration. In this paper, we systematically\ninvestigate the exposure bias problem in diffusion models by first analytically\nmodelling the sampling distribution, based on which we then attribute the\nprediction error at each sampling step as the root cause of the exposure bias\nissue. Furthermore, we discuss potential solutions to this issue and propose an\nintuitive metric for it. Along with the elucidation of exposure bias, we\npropose a simple, yet effective, training-free method called Epsilon Scaling to\nalleviate the exposure bias. We show that Epsilon Scaling explicitly moves the\nsampling trajectory closer to the vector field learned in the training phase by\nscaling down the network output (Epsilon), mitigating the input mismatch\nbetween training and sampling. Experiments on various diffusion frameworks\n(ADM, DDPM/DDIM, LDM), unconditional and conditional settings, and\ndeterministic vs. stochastic sampling verify the effectiveness of our method.\n","authors":["Mang Ning","Mingxiao Li","Jianlin Su","Albert Ali Salah","Itir Onal Ertugrul"],"pdf_url":"https://arxiv.org/pdf/2308.15321v2.pdf","comment":"7 pages, code available soon"},{"id":"http://arxiv.org/abs/2204.09398v2","updated":"2023-08-30T08:18:15Z","published":"2022-04-20T11:43:58Z","title":"Case-Aware Adversarial Training","summary":"  The neural network (NN) becomes one of the most heated type of models in\nvarious signal processing applications. However, NNs are extremely vulnerable\nto adversarial examples (AEs). To defend AEs, adversarial training (AT) is\nbelieved to be the most effective method while due to the intensive\ncomputation, AT is limited to be applied in most applications. In this paper,\nto resolve the problem, we design a generic and efficient AT improvement\nscheme, namely case-aware adversarial training (CAT). Specifically, the\nintuition stems from the fact that a very limited part of informative samples\ncan contribute to most of model performance. Alternatively, if only the most\ninformative AEs are used in AT, we can lower the computation complexity of AT\nsignificantly as maintaining the defense effect. To achieve this, CAT achieves\ntwo breakthroughs. First, a method to estimate the information degree of\nadversarial examples is proposed for AE filtering. Second, to further enrich\nthe information that the NN can obtain from AEs, CAT involves a weight\nestimation and class-level balancing based sampling strategy to increase the\ndiversity of AT at each iteration. Extensive experiments show that CAT is\nfaster than vanilla AT by up to 3x while achieving competitive defense effect.\n","authors":["Mingyuan Fan","Yang Liu","Cen Chen"],"pdf_url":"https://arxiv.org/pdf/2204.09398v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03512v3","updated":"2023-08-30T08:10:20Z","published":"2023-07-07T11:00:44Z","title":"Tranfer Learning of Semantic Segmentation Methods for Identifying Buried\n  Archaeological Structures on LiDAR Data","summary":"  When applying deep learning to remote sensing data in archaeological\nresearch, a notable obstacle is the limited availability of suitable datasets\nfor training models. The application of transfer learning is frequently\nemployed to mitigate this drawback. However, there is still a need to explore\nits effectiveness when applied across different archaeological datasets. This\npaper compares the performance of various transfer learning configurations\nusing two semantic segmentation deep neural networks on two LiDAR datasets. The\nexperimental results indicate that transfer learning-based approaches in\narchaeology can lead to performance improvements, although a systematic\nenhancement has not yet been observed. We provide specific insights about the\nvalidity of such techniques that can serve as a baseline for future works.\n","authors":["Gregory Sech","Paolo Soleni","Wouter B. Verschoof-van der Vaart","Žiga Kokalj","Arianna Traviglia","Marco Fiorucci"],"pdf_url":"https://arxiv.org/pdf/2307.03512v3.pdf","comment":"Accepted to IEEE International Geoscience and Remote Sensing\n  Symposium 2023 (IGARSS 2023) @IEEE copyright"},{"id":"http://arxiv.org/abs/2308.15829v1","updated":"2023-08-30T08:09:40Z","published":"2023-08-30T08:09:40Z","title":"Early Detection of Red Palm Weevil Infestations using Deep Learning\n  Classification of Acoustic Signals","summary":"  The Red Palm Weevil (RPW), also known as the palm weevil, is considered among\nthe world's most damaging insect pests of palms. Current detection techniques\ninclude the detection of symptoms of RPW using visual or sound inspection and\nchemical detection of volatile signatures generated by infested palm trees.\nHowever, efficient detection of RPW diseases at an early stage is considered\none of the most challenging issues for cultivating date palms. In this paper,\nan efficient approach to the early detection of RPW is proposed. The proposed\napproach is based on RPW sound activities being recorded and analyzed. The\nfirst step involves the conversion of sound data into images based on a\nselected set of features. The second step involves the combination of images\nfrom the same sound file but computed by different features into a single\nimage. The third step involves the application of different Deep Learning (DL)\ntechniques to classify resulting images into two classes: infested and not\ninfested. Experimental results show good performances of the proposed approach\nfor RPW detection using different DL techniques, namely MobileNetV2,\nResNet50V2, ResNet152V2, VGG16, VGG19, DenseNet121, DenseNet201, Xception, and\nInceptionV3. The proposed approach outperformed existing techniques for public\ndatasets.\n","authors":["Wadii Boulila","Ayyub Alzahem","Anis Koubaa","Bilel Benjdira","Adel Ammar"],"pdf_url":"https://arxiv.org/pdf/2308.15829v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15827v1","updated":"2023-08-30T08:03:49Z","published":"2023-08-30T08:03:49Z","title":"Introducing Language Guidance in Prompt-based Continual Learning","summary":"  Continual Learning aims to learn a single model on a sequence of tasks\nwithout having access to data from previous tasks. The biggest challenge in the\ndomain still remains catastrophic forgetting: a loss in performance on seen\nclasses of earlier tasks. Some existing methods rely on an expensive replay\nbuffer to store a chunk of data from previous tasks. This, while promising,\nbecomes expensive when the number of tasks becomes large or data can not be\nstored for privacy reasons. As an alternative, prompt-based methods have been\nproposed that store the task information in a learnable prompt pool. This\nprompt pool instructs a frozen image encoder on how to solve each task. While\nthe model faces a disjoint set of classes in each task in this setting, we\nargue that these classes can be encoded to the same embedding space of a\npre-trained language encoder. In this work, we propose Language Guidance for\nPrompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.\nLGCL is model agnostic and introduces language guidance at the task level in\nthe prompt pool and at the class level on the output feature of the vision\nencoder. We show with extensive experimentation that LGCL consistently improves\nthe performance of prompt-based continual learning methods to set a new\nstate-of-the art. LGCL achieves these performance improvements without needing\nany additional learnable parameters.\n","authors":["Muhammad Gul Zain Ali Khan","Muhammad Ferjad Naeem","Luc Van Gool","Didier Stricker","Federico Tombari","Muhammad Zeshan Afzal"],"pdf_url":"https://arxiv.org/pdf/2308.15827v1.pdf","comment":"Accepted at ICCV 2023"},{"id":"http://arxiv.org/abs/2209.14624v2","updated":"2023-08-30T08:01:22Z","published":"2022-09-29T08:38:30Z","title":"Is Complexity Required for Neural Network Pruning? A Case Study on\n  Global Magnitude Pruning","summary":"  Pruning neural networks has become popular in the last decade when it was\nshown that a large number of weights can be safely removed from modern neural\nnetworks without compromising accuracy. Numerous pruning methods have been\nproposed since then, each claiming to be better than the previous. Many\nstate-of-the-art (SOTA) techniques today rely on complex pruning methodologies\nutilizing importance scores, getting feedback through back-propagation or\nhaving heuristics-based pruning rules amongst others. In this work, we question\nwhether this pattern of introducing complexity is really necessary to achieve\nbetter pruning results. We benchmark these SOTA techniques against a naive\npruning baseline, namely, Global Magnitude Pruning (Global MP). Global MP ranks\nweights in order of their magnitudes and prunes the smallest ones. Hence, in\nits vanilla form, it is one of the simplest pruning techniques. Surprisingly,\nwe find that vanilla Global MP outperforms all the other SOTA techniques and\nachieves a new SOTA result. It also achieves promising performance on FLOPs\nsparsification, which we find is enhanced, when pruning is conducted in a\ngradual fashion. We also find that Global MP is generalizable across tasks,\ndatasets, and models with superior performance. Moreover, a common issue that\nmany pruning algorithms run into at high sparsity rates, namely,\nlayer-collapse, can be easily fixed in Global MP by setting a minimum threshold\nof weights to be retained in each layer. Lastly, unlike many other SOTA\ntechniques, Global MP does not require any additional algorithm specific\nhyper-parameters and is very straightforward to tune and implement. We showcase\nour findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1\nand FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is\navailable at https://github.com/manasgupta-1/GlobalMP.\n","authors":["Manas Gupta","Efe Camci","Vishandi Rudy Keneta","Abhishek Vaidyanathan","Ritwik Kanodia","Chuan-Sheng Foo","Wu Min","Lin Jie"],"pdf_url":"https://arxiv.org/pdf/2209.14624v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15822v1","updated":"2023-08-30T07:48:32Z","published":"2023-08-30T07:48:32Z","title":"AMDNet23: A combined deep Contour-based Convolutional Neural Network and\n  Long Short Term Memory system to diagnose Age-related Macular Degeneration","summary":"  In light of the expanding population, an automated framework of disease\ndetection can assist doctors in the diagnosis of ocular diseases, yields\naccurate, stable, rapid outcomes, and improves the success rate of early\ndetection. The work initially intended the enhancing the quality of fundus\nimages by employing an adaptive contrast enhancement algorithm (CLAHE) and\nGamma correction. In the preprocessing techniques, CLAHE elevates the local\ncontrast of the fundus image and gamma correction increases the intensity of\nrelevant features. This study operates on a AMDNet23 system of deep learning\nthat combined the neural networks made up of convolutions (CNN) and short-term\nand long-term memory (LSTM) to automatically detect aged macular degeneration\n(AMD) disease from fundus ophthalmology. In this mechanism, CNN is utilized for\nextracting features and LSTM is utilized to detect the extracted features. The\ndataset of this research is collected from multiple sources and afterward\napplied quality assessment techniques, 2000 experimental fundus images\nencompass four distinct classes equitably. The proposed hybrid deep AMDNet23\nmodel demonstrates to detection of AMD ocular disease and the experimental\nresult achieved an accuracy 96.50%, specificity 99.32%, sensitivity 96.5%, and\nF1-score 96.49.0%. The system achieves state-of-the-art findings on fundus\nimagery datasets to diagnose AMD ocular disease and findings effectively\npotential of our method.\n","authors":["Md. Aiyub Ali","Md. Shakhawat Hossain","Md. Kawar Hossain","Subhadra Soumi Sikder","Sharun Akter Khushbu","Mirajul Islam"],"pdf_url":"https://arxiv.org/pdf/2308.15822v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15816v1","updated":"2023-08-30T07:41:26Z","published":"2023-08-30T07:41:26Z","title":"Improving Underwater Visual Tracking With a Large Scale Dataset and\n  Image Enhancement","summary":"  This paper presents a new dataset and general tracker enhancement method for\nUnderwater Visual Object Tracking (UVOT). Despite its significance, underwater\ntracking has remained unexplored due to data inaccessibility. It poses distinct\nchallenges; the underwater environment exhibits non-uniform lighting\nconditions, low visibility, lack of sharpness, low contrast, camouflage, and\nreflections from suspended particles. Performance of traditional tracking\nmethods designed primarily for terrestrial or open-air scenarios drops in such\nconditions. We address the problem by proposing a novel underwater image\nenhancement algorithm designed specifically to boost tracking quality. The\nmethod has resulted in a significant performance improvement, of up to 5.0%\nAUC, of state-of-the-art (SOTA) visual trackers. To develop robust and accurate\nUVOT methods, large-scale datasets are required. To this end, we introduce a\nlarge-scale UVOT benchmark dataset consisting of 400 video segments and 275,000\nmanually annotated frames enabling underwater training and evaluation of deep\ntrackers. The videos are labelled with several underwater-specific tracking\nattributes including watercolor variation, target distractors, camouflage,\ntarget relative size, and low visibility conditions. The UVOT400 dataset,\ntracking results, and the code are publicly available on:\nhttps://github.com/BasitAlawode/UWVOT400.\n","authors":["Basit Alawode","Fayaz Ali Dharejo","Mehnaz Ummar","Yuhang Guo","Arif Mahmood","Naoufel Werghi","Fahad Shahbaz Khan","Sajid Javed"],"pdf_url":"https://arxiv.org/pdf/2308.15816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15807v1","updated":"2023-08-30T07:23:32Z","published":"2023-08-30T07:23:32Z","title":"ACNPU: A 4.75TOPS/W 1080P@30FPS Super Resolution Accelerator with\n  Decoupled Asymmetric Convolution","summary":"  Deep learning-driven superresolution (SR) outperforms traditional techniques\nbut also faces the challenge of high complexity and memory bandwidth. This\nchallenge leads many accelerators to opt for simpler and shallow models like\nFSRCNN, compromising performance for real-time needs, especially for\nresource-limited edge devices. This paper proposes an energy-efficient SR\naccelerator, ACNPU, to tackle this challenge. The ACNPU enhances image quality\nby 0.34dB with a 27-layer model, but needs 36\\% less complexity than FSRCNN,\nwhile maintaining a similar model size, with the \\textit{decoupled asymmetric\nconvolution and split-bypass structure}. The hardware-friendly 17K-parameter\nmodel enables \\textit{holistic model fusion} instead of localized layer fusion\nto remove external DRAM access of intermediate feature maps. The on-chip memory\nbandwidth is further reduced with the \\textit{input stationary flow} and\n\\textit{parallel-layer execution} to reduce power consumption. Hardware is\nregular and easy to control to support different layers by \\textit{processing\nelements (PEs) clusters with reconfigurable input and uniform data flow}. The\nimplementation in the 40 nm CMOS process consumes 2333 K gate counts and 198KB\nSRAMs. The ACNPU achieves 31.7 FPS and 124.4 FPS for x2 and x4 scales Full-HD\ngeneration, respectively, which attains 4.75 TOPS/W energy efficiency.\n","authors":["Tun-Hao Yang","Tian-Sheuan Chang"],"pdf_url":"https://arxiv.org/pdf/2308.15807v1.pdf","comment":"9 pages, 14 figures"},{"id":"http://arxiv.org/abs/2308.07016v2","updated":"2023-08-30T07:01:42Z","published":"2023-08-14T09:04:06Z","title":"HHTrack: Hyperspectral Object Tracking Using Hybrid Attention","summary":"  Hyperspectral imagery provides abundant spectral information beyond the\nvisible RGB bands, offering rich discriminative details about objects in a\nscene. Leveraging such data has the potential to enhance visual tracking\nperformance. In this paper, we propose a hyperspectral object tracker based on\nhybrid attention (HHTrack). The core of HHTrack is a hyperspectral hybrid\nattention (HHA) module that unifies feature extraction and fusion within one\ncomponent through token interactions. A hyperspectral bands fusion (HBF) module\nis also introduced to selectively aggregate spatial and spectral signatures\nfrom the full hyperspectral input. Extensive experiments demonstrate the\nstate-of-the-art performance of HHTrack on benchmark Near Infrared (NIR), Red\nNear Infrared (Red-NIR), and Visible (VIS) hyperspectral tracking datasets. Our\nwork provides new insights into harnessing the strengths of transformers and\nhyperspectral fusion to advance robust object tracking.\n","authors":["Yuedong Tan"],"pdf_url":"https://arxiv.org/pdf/2308.07016v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.11499v3","updated":"2023-08-30T06:57:57Z","published":"2022-08-24T12:47:58Z","title":"Semi-supervised Semantic Segmentation with Mutual Knowledge Distillation","summary":"  Consistency regularization has been widely studied in recent semisupervised\nsemantic segmentation methods, and promising performance has been achieved. In\nthis work, we propose a new consistency regularization framework, termed mutual\nknowledge distillation (MKD), combined with data and feature augmentation. We\nintroduce two auxiliary mean-teacher models based on consistency\nregularization. More specifically, we use the pseudo-labels generated by a mean\nteacher to supervise the student network to achieve a mutual knowledge\ndistillation between the two branches. In addition to using image-level strong\nand weak augmentation, we also discuss feature augmentation. This involves\nconsidering various sources of knowledge to distill the student network. Thus,\nwe can significantly increase the diversity of the training samples.\nExperiments on public benchmarks show that our framework outperforms previous\nstate-of-the-art (SOTA) methods under various semi-supervised settings. Code is\navailable at semi-mmseg.\n","authors":["Jianlong Yuan","Jinchao Ge","Zhibin Wang","Yifan Liu"],"pdf_url":"https://arxiv.org/pdf/2208.11499v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15795v1","updated":"2023-08-30T06:56:53Z","published":"2023-08-30T06:56:53Z","title":"Occlusion-Aware Detection and Re-ID Calibrated Network for Multi-Object\n  Tracking","summary":"  Multi-Object Tracking (MOT) is a crucial computer vision task that aims to\npredict the bounding boxes and identities of objects simultaneously. While\nstate-of-the-art methods have made remarkable progress by jointly optimizing\nthe multi-task problems of detection and Re-ID feature learning, yet, few\napproaches explore to tackle the occlusion issue, which is a long-standing\nchallenge in the MOT field. Generally, occluded objects may hinder the detector\nfrom estimating the bounding boxes, resulting in fragmented trajectories. And\nthe learned occluded Re-ID embeddings are less distinct since they contain\ninterferer. To this end, we propose an occlusion-aware detection and Re-ID\ncalibrated network for multi-object tracking, termed as ORCTrack. Specifically,\nwe propose an Occlusion-Aware Attention (OAA) module in the detector that\nhighlights the object features while suppressing the occluded background\nregions. OAA can serve as a modulator that enhances the detector for some\npotentially occluded objects. Furthermore, we design a Re-ID embedding matching\nblock based on the optimal transport problem, which focuses on enhancing and\ncalibrating the Re-ID representations through different adjacent frames\ncomplementarily. To validate the effectiveness of the proposed method,\nextensive experiments are conducted on two challenging VisDrone2021-MOT and\nKITTI benchmarks. Experimental evaluations demonstrate the superiority of our\napproach, which can achieve new state-of-the-art performance and enjoy high\nrun-time efficiency.\n","authors":["Yukun Su","Ruizhou Sun","Xin Shu","Yu Zhang","Qingyao Wu"],"pdf_url":"https://arxiv.org/pdf/2308.15795v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15791v1","updated":"2023-08-30T06:49:34Z","published":"2023-08-30T06:49:34Z","title":"Neural Video Compression with Temporal Layer-Adaptive Hierarchical\n  B-frame Coding","summary":"  Neural video compression (NVC) is a rapidly evolving video coding research\narea, with some models achieving superior coding efficiency compared to the\nlatest video coding standard Versatile Video Coding (VVC). In conventional\nvideo coding standards, the hierarchical B-frame coding, which utilizes a\nbidirectional prediction structure for higher compression, had been\nwell-studied and exploited. In NVC, however, limited research has investigated\nthe hierarchical B scheme. In this paper, we propose an NVC model exploiting\nhierarchical B-frame coding with temporal layer-adaptive optimization. We first\nextend an existing unidirectional NVC model to a bidirectional model, which\nachieves -21.13% BD-rate gain over the unidirectional baseline model. However,\nthis model faces challenges when applied to sequences with complex or large\nmotions, leading to performance degradation. To address this, we introduce\ntemporal layer-adaptive optimization, incorporating methods such as temporal\nlayer-adaptive quality scaling (TAQS) and temporal layer-adaptive latent\nscaling (TALS). The final model with the proposed methods achieves an\nimpressive BD-rate gain of -39.86% against the baseline. It also resolves the\nchallenges in sequences with large or complex motions with up to -49.13% more\nBD-rate gains than the simple bidirectional extension. This improvement is\nattributed to the allocation of more bits to lower temporal layers, thereby\nenhancing overall reconstruction quality with smaller bits. Since our method\nhas little dependency on a specific NVC model architecture, it can serve as a\ngeneral tool for extending unidirectional NVC models to the ones with\nhierarchical B-frame coding.\n","authors":["Yeongwoong Kim","Suyong Bahk","Seungeon Kim","Won Hee Lee","Dokwan Oh","Hui Yong Kim"],"pdf_url":"https://arxiv.org/pdf/2308.15791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.07394v3","updated":"2023-08-30T06:36:08Z","published":"2022-06-15T08:55:47Z","title":"Efficient Adaptive Ensembling for Image Classification","summary":"  In recent times, with the exception of sporadic cases, the trend in Computer\nVision is to achieve minor improvements compared to considerable increases in\ncomplexity.\n  To reverse this trend, we propose a novel method to boost image\nclassification performances without increasing complexity.\n  To this end, we revisited ensembling, a powerful approach, often not used\nproperly due to its more complex nature and the training time, so as to make it\nfeasible through a specific design choice. First, we trained two\nEfficientNet-b0 end-to-end models (known to be the architecture with the best\noverall accuracy/complexity trade-off for image classification) on disjoint\nsubsets of data (i.e. bagging). Then, we made an efficient adaptive ensemble by\nperforming fine-tuning of a trainable combination layer. In this way, we were\nable to outperform the state-of-the-art by an average of 0.5$\\%$ on the\naccuracy, with restrained complexity both in terms of the number of parameters\n(by 5-60 times), and the FLoating point Operations Per Second (FLOPS) by 10-100\ntimes on several major benchmark datasets.\n","authors":["Antonio Bruno","Davide Moroni","Massimo Martinelli"],"pdf_url":"https://arxiv.org/pdf/2206.07394v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04557v2","updated":"2023-08-30T06:30:43Z","published":"2023-03-08T13:15:19Z","title":"Scene Matters: Model-based Deep Video Compression","summary":"  Video compression has always been a popular research area, where many\ntraditional and deep video compression methods have been proposed. These\nmethods typically rely on signal prediction theory to enhance compression\nperformance by designing high efficient intra and inter prediction strategies\nand compressing video frames one by one. In this paper, we propose a novel\nmodel-based video compression (MVC) framework that regards scenes as the\nfundamental units for video sequences. Our proposed MVC directly models the\nintensity variation of the entire video sequence in one scene, seeking\nnon-redundant representations instead of reducing redundancy through\nspatio-temporal predictions. To achieve this, we employ implicit neural\nrepresentation as our basic modeling architecture. To improve the efficiency of\nvideo modeling, we first propose context-related spatial positional embedding\nand frequency domain supervision in spatial context enhancement. For temporal\ncorrelation capturing, we design the scene flow constrain mechanism and\ntemporal contrastive loss. Extensive experimental results demonstrate that our\nmethod achieves up to a 20\\% bitrate reduction compared to the latest video\ncoding standard H.266 and is more efficient in decoding than existing video\ncoding strategies.\n","authors":["Lv Tang","Xinfeng Zhang","Gai Zhang","Xiaoqi Ma"],"pdf_url":"https://arxiv.org/pdf/2303.04557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10799v2","updated":"2023-08-30T05:01:31Z","published":"2023-06-19T09:39:10Z","title":"SelfTalk: A Self-Supervised Commutative Training Diagram to Comprehend\n  3D Talking Faces","summary":"  Speech-driven 3D face animation technique, extending its applications to\nvarious multimedia fields. Previous research has generated promising realistic\nlip movements and facial expressions from audio signals. However, traditional\nregression models solely driven by data face several essential problems, such\nas difficulties in accessing precise labels and domain gaps between different\nmodalities, leading to unsatisfactory results lacking precision and coherence.\nTo enhance the visual accuracy of generated lip movement while reducing the\ndependence on labeled data, we propose a novel framework SelfTalk, by involving\nself-supervision in a cross-modals network system to learn 3D talking faces.\nThe framework constructs a network system consisting of three modules: facial\nanimator, speech recognizer, and lip-reading interpreter. The core of SelfTalk\nis a commutative training diagram that facilitates compatible features exchange\namong audio, text, and lip shape, enabling our models to learn the intricate\nconnection between these factors. The proposed framework leverages the\nknowledge learned from the lip-reading interpreter to generate more plausible\nlip shapes. Extensive experiments and user studies demonstrate that our\nproposed approach achieves state-of-the-art performance both qualitatively and\nquantitatively. We recommend watching the supplementary video.\n","authors":["Ziqiao Peng","Yihao Luo","Yue Shi","Hao Xu","Xiangyu Zhu","Jun He","Hongyan Liu","Zhaoxin Fan"],"pdf_url":"https://arxiv.org/pdf/2306.10799v2.pdf","comment":"Accepted by ACM MM 2023"},{"id":"http://arxiv.org/abs/2304.04027v3","updated":"2023-08-30T04:55:04Z","published":"2023-04-08T14:40:35Z","title":"Estimating 3D Dental Structures using Simulated Panoramic Radiographs\n  and Neural Ray Tracing","summary":"  Panoramic radiography (Panoramic X-ray, PX) is a widely used imaging modality\nfor dental examination. However, PX only provides a flattened 2D image, lacking\nin a 3D view of the oral structure. In this paper, we propose a framework to\nestimate 3D oral structures from real-world PX. Our framework tackles full 3D\nreconstruction for varying subjects (patients) where each reconstruction is\nbased only on a single panoramic image. We create an intermediate\nrepresentation called simulated PX (SimPX) from 3D Cone-beam computed\ntomography (CBCT) data based on the Beer-Lambert law of X-ray rendering and\nrotational principles of PX imaging. SimPX aims at not only truthfully\nsimulating PX, but also facilitates the reverting process back to 3D data. We\npropose a novel neural model based on ray tracing which exploits both global\nand local input features to convert SimPX to 3D output. At inference, a real PX\nimage is translated to a SimPX-style image with semantic regularization, and\nthe translated image is processed by generation module to produce high-quality\noutputs. Experiments show that our method outperforms prior state-of-the-art in\nreconstruction tasks both quantitatively and qualitatively. Unlike prior\nmethods, Our method does not require any prior information such as the shape of\ndental arches, nor the matched PX-CBCT dataset for training, which is difficult\nto obtain in clinical practice.\n","authors":["Sihwa Park","Seongjun Kim","Doeyoung Kwon","Yohan Jang","In-Seok Song","Seungjun Baek"],"pdf_url":"https://arxiv.org/pdf/2304.04027v3.pdf","comment":"20 pages, 16 figures"},{"id":"http://arxiv.org/abs/2306.01762v2","updated":"2023-08-30T04:53:15Z","published":"2023-05-27T06:00:51Z","title":"Pre-trained transformer for adversarial purification","summary":"  With more and more deep neural networks being deployed as various daily\nservices, their reliability is essential. It's frightening that deep neural\nnetworks are vulnerable and sensitive to adversarial attacks, the most common\none of which for the services is evasion-based. Recent works usually strengthen\nthe robustness by adversarial training or leveraging the knowledge of an amount\nof clean data. However, in practical terms, retraining and redeploying the\nmodel need a large computational budget, leading to heavy losses to the online\nservice. In addition, when adversarial examples of a certain attack are\ndetected, only limited adversarial examples are available for the service\nprovider, while much clean data may not be accessible. Given the mentioned\nproblems, we propose a new scenario, RaPiD (Rapid Plug-in Defender), which is\nto rapidly defend against a certain attack for the frozen original service\nmodel with limitations of few clean and adversarial examples. Motivated by the\ngeneralization and the universal computation ability of pre-trained transformer\nmodels, we come up with a new defender method, CeTaD, which stands for\nConsidering Pre-trained Transformers as Defenders. In particular, we evaluate\nthe effectiveness and the transferability of CeTaD in the case of one-shot\nadversarial examples and explore the impact of different parts of CeTaD as well\nas training data conditions. CeTaD is flexible, able to be embedded into an\narbitrary differentiable model, and suitable for various types of attacks.\n","authors":["Kai Wu","Yujian Betterest Li","Xiaoyu Zhang","Handing Wang","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2306.01762v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.01418v3","updated":"2023-08-30T04:41:10Z","published":"2023-03-02T17:09:27Z","title":"Human Motion Diffusion as a Generative Prior","summary":"  Recent work has demonstrated the significant potential of denoising diffusion\nmodels for generating human motion, including text-to-motion capabilities.\nHowever, these methods are restricted by the paucity of annotated motion data,\na focus on single-person motions, and a lack of detailed control. In this\npaper, we introduce three forms of composition based on diffusion priors:\nsequential, parallel, and model composition. Using sequential composition, we\ntackle the challenge of long sequence generation. We introduce DoubleTake, an\ninference-time method with which we generate long animations consisting of\nsequences of prompted intervals and their transitions, using a prior trained\nonly for short clips. Using parallel composition, we show promising steps\ntoward two-person generation. Beginning with two fixed priors as well as a few\ntwo-person training examples, we learn a slim communication block, ComMDM, to\ncoordinate interaction between the two resulting motions. Lastly, using model\ncomposition, we first train individual priors to complete motions that realize\na prescribed motion for a given joint. We then introduce DiffusionBlending, an\ninterpolation mechanism to effectively blend several such models to enable\nflexible and efficient fine-grained joint and trajectory-level control and\nediting. We evaluate the composition methods using an off-the-shelf motion\ndiffusion model, and further compare the results to dedicated models trained\nfor these specific tasks.\n","authors":["Yonatan Shafir","Guy Tevet","Roy Kapon","Amit H. Bermano"],"pdf_url":"https://arxiv.org/pdf/2303.01418v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15752v1","updated":"2023-08-30T04:29:48Z","published":"2023-08-30T04:29:48Z","title":"Large-scale data extraction from the UNOS organ donor documents","summary":"  The scope of our study is all UNOS data of the USA organ donors since 2008.\nThe data is not analyzable in a large scale in the past because it was captured\nin PDF documents known as \"Attachments\", whereby every donor is represented by\ndozens of PDF documents in heterogenous formats. To make the data analyzable,\none needs to convert the content inside these PDFs to an analyzable data\nformat, such as a standard SQL database. In this paper we will focus on 2022\nUNOS data comprised of $\\approx 400,000$ PDF documents spanning millions of\npages. The totality of UNOS data covers 15 years (2008--20022) and our results\nwill be quickly extended to the entire data. Our method captures a portion of\nthe data in DCD flowsheets, kidney perfusion data, and data captured during\npatient hospital stay (e.g. vital signs, ventilator settings, etc.). The\ncurrent paper assumes that the reader is familiar with the content of the UNOS\ndata. The overview of the types of data and challenges they present is a\nsubject of another paper. Here we focus on demonstrating that the goal of\nbuilding a comprehensive, analyzable database from UNOS documents is an\nattainable task, and we provide an overview of our methodology. The project\nresulted in datasets by far larger than previously available even in this\npreliminary phase.\n","authors":["Marek Rychlik","Bekir Tanriover","Yan Han"],"pdf_url":"https://arxiv.org/pdf/2308.15752v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03741v4","updated":"2023-08-30T04:18:50Z","published":"2022-12-07T16:10:08Z","title":"FineDance: A Fine-grained Choreography Dataset for 3D Full Body Dance\n  Generation","summary":"  Generating full-body and multi-genre dance sequences from given music is a\nchallenging task, due to the limitations of existing datasets and the inherent\ncomplexity of the fine-grained hand motion and dance genres. To address these\nproblems, we propose FineDance, which contains 14.6 hours of music-dance paired\ndata, with fine-grained hand motions, fine-grained genres (22 dance genres),\nand accurate posture. To the best of our knowledge, FineDance is the largest\nmusic-dance paired dataset with the most dance genres. Additionally, to address\nmonotonous and unnatural hand movements existing in previous methods, we\npropose a full-body dance generation network, which utilizes the diverse\ngeneration capabilities of the diffusion model to solve monotonous problems,\nand use expert nets to solve unreal problems. To further enhance the\ngenre-matching and long-term stability of generated dances, we propose a\nGenre&Coherent aware Retrieval Module. Besides, we propose a novel metric named\nGenre Matching Score to evaluate the genre-matching degree between dance and\nmusic. Quantitative and qualitative experiments demonstrate the quality of\nFineDance, and the state-of-the-art performance of FineNet. The FineDance\nDataset and more qualitative samples can be found at our website.\n","authors":["Ronghui Li","Junfan Zhao","Yachao Zhang","Mingyang Su","Zeping Ren","Han Zhang","Yansong Tang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2212.03741v4.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2305.12596v2","updated":"2023-08-30T03:55:54Z","published":"2023-05-21T23:10:14Z","title":"iWarpGAN: Disentangling Identity and Style to Generate Synthetic Iris\n  Images","summary":"  Generative Adversarial Networks (GANs) have shown success in approximating\ncomplex distributions for synthetic image generation. However, current\nGAN-based methods for generating biometric images, such as iris, have certain\nlimitations: (a) the synthetic images often closely resemble images in the\ntraining dataset; (b) the generated images lack diversity in terms of the\nnumber of unique identities represented in them; and (c) it is difficult to\ngenerate multiple images pertaining to the same identity. To overcome these\nissues, we propose iWarpGAN that disentangles identity and style in the context\nof the iris modality by using two transformation pathways: Identity\nTransformation Pathway to generate unique identities from the training set, and\nStyle Transformation Pathway to extract the style code from a reference image\nand output an iris image using this style. By concatenating the transformed\nidentity code and reference style code, iWarpGAN generates iris images with\nboth inter- and intra-class variations. The efficacy of the proposed method in\ngenerating such iris DeepFakes is evaluated both qualitatively and\nquantitatively using ISO/IEC 29794-6 Standard Quality Metrics and the VeriEye\niris matcher. Further, the utility of the synthetically generated images is\ndemonstrated by improving the performance of deep learning based iris matchers\nthat augment synthetic data with real data during the training process.\n","authors":["Shivangi Yadav","Arun Ross"],"pdf_url":"https://arxiv.org/pdf/2305.12596v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15740v1","updated":"2023-08-30T03:35:55Z","published":"2023-08-30T03:35:55Z","title":"Beard Segmentation and Recognition Bias","summary":"  A person's facial hairstyle, such as presence and size of beard, can\nsignificantly impact face recognition accuracy. There are publicly-available\ndeep networks that achieve reasonable accuracy at binary attribute\nclassification, such as beard / no beard, but few if any that segment the\nfacial hair region. To investigate the effect of facial hair in a rigorous\nmanner, we first created a set of fine-grained facial hair annotations to train\na segmentation model and evaluate its accuracy across African-American and\nCaucasian face images. We then use our facial hair segmentations to categorize\nimage pairs according to the degree of difference or similarity in the facial\nhairstyle. We find that the False Match Rate (FMR) for image pairs with\ndifferent categories of facial hairstyle varies by a factor of over 10 for\nAfrican-American males and over 25 for Caucasian males. To reduce the bias\nacross image pairs with different facial hairstyles, we propose a scheme for\nadaptive thresholding based on facial hairstyle similarity. Evaluation on a\nsubject-disjoint set of images shows that adaptive similarity thresholding\nbased on facial hairstyles of the image pair reduces the ratio between the\nhighest and lowest FMR across facial hairstyle categories for African-American\nfrom 10.7 to 1.8 and for Caucasians from 25.9 to 1.3. Facial hair annotations\nand facial hair segmentation model will be publicly available.\n","authors":["Kagan Ozturk","Grace Bezold","Aman Bhatta","Haiyu Wu","Kevin Bowyer"],"pdf_url":"https://arxiv.org/pdf/2308.15740v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02862v2","updated":"2023-08-30T03:21:29Z","published":"2023-03-06T03:27:17Z","title":"EvHandPose: Event-based 3D Hand Pose Estimation with Sparse Supervision","summary":"  Event camera shows great potential in 3D hand pose estimation, especially\naddressing the challenges of fast motion and high dynamic range in a low-power\nway. However, due to the asynchronous differential imaging mechanism, it is\nchallenging to design event representation to encode hand motion information\nespecially when the hands are not moving (causing motion ambiguity), and it is\ninfeasible to fully annotate the temporally dense event stream. In this paper,\nwe propose EvHandPose with novel hand flow representations in Event-to-Pose\nmodule for accurate hand pose estimation and alleviating the motion ambiguity\nissue. To solve the problem under sparse annotation, we design contrast\nmaximization and hand-edge constraints in Pose-to-IWE (Image with Warped\nEvents) module and formulate EvHandPose in a weakly-supervision framework. We\nfurther build EvRealHands, the first large-scale real-world event-based hand\npose dataset on several challenging scenes to bridge the real-synthetic domain\ngap. Experiments on EvRealHands demonstrate that EvHandPose outperforms\nprevious event-based methods under all evaluation scenes, achieves accurate and\nstable hand pose estimation with high temporal resolution in fast motion and\nstrong light scenes compared with RGB-based methods, generalizes well to\noutdoor scenes and another type of event camera, and shows the potential for\nthe hand gesture recognition task.\n","authors":["Jianping Jiang","Jiahe Li","Baowen Zhang","Xiaoming Deng","Boxin Shi"],"pdf_url":"https://arxiv.org/pdf/2303.02862v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15733v1","updated":"2023-08-30T03:17:57Z","published":"2023-08-30T03:17:57Z","title":"Drone-NeRF: Efficient NeRF Based 3D Scene Reconstruction for Large-Scale\n  Drone Survey","summary":"  Neural rendering has garnered substantial attention owing to its capacity for\ncreating realistic 3D scenes. However, its applicability to extensive scenes\nremains challenging, with limitations in effectiveness. In this work, we\npropose the Drone-NeRF framework to enhance the efficient reconstruction of\nunbounded large-scale scenes suited for drone oblique photography using Neural\nRadiance Fields (NeRF). Our approach involves dividing the scene into uniform\nsub-blocks based on camera position and depth visibility. Sub-scenes are\ntrained in parallel using NeRF, then merged for a complete scene. We refine the\nmodel by optimizing camera poses and guiding NeRF with a uniform sampler.\nIntegrating chosen samples enhances accuracy. A hash-coded fusion MLP\naccelerates density representation, yielding RGB and Depth outputs. Our\nframework accounts for sub-scene constraints, reduces parallel-training noise,\nhandles shadow occlusion, and merges sub-regions for a polished rendering\nresult. This Drone-NeRF framework demonstrates promising capabilities in\naddressing challenges related to scene complexity, rendering efficiency, and\naccuracy in drone-obtained imagery.\n","authors":["Zhihao Jia","Bing Wang","Changhao Chen"],"pdf_url":"https://arxiv.org/pdf/2308.15733v1.pdf","comment":"15 pages, 7 figures, in submission"},{"id":"http://arxiv.org/abs/2303.07543v4","updated":"2023-08-30T03:12:34Z","published":"2023-03-14T00:13:57Z","title":"WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant\n  Analysis","summary":"  Deep neural networks are susceptible to generating overconfident yet\nerroneous predictions when presented with data beyond known concepts. This\nchallenge underscores the importance of detecting out-of-distribution (OOD)\nsamples in the open world. In this work, we propose a novel feature-space OOD\ndetection score based on class-specific and class-agnostic information.\nSpecifically, the approach utilizes Whitened Linear Discriminant Analysis to\nproject features into two subspaces - the discriminative and residual subspaces\n- for which the in-distribution (ID) classes are maximally separated and\nclosely clustered, respectively. The OOD score is then determined by combining\nthe deviation from the input data to the ID pattern in both subspaces. The\nefficacy of our method, named WDiscOOD, is verified on the large-scale\nImageNet-1k benchmark, with six OOD datasets that cover a variety of\ndistribution shifts. WDiscOOD demonstrates superior performance on deep\nclassifiers with diverse backbone architectures, including CNN and vision\ntransformer. Furthermore, we also show that WDiscOOD more effectively detects\nnovel concepts in representation spaces trained with contrastive objectives,\nincluding supervised contrastive loss and multi-modality contrastive loss.\n","authors":["Yiye Chen","Yunzhi Lin","Ruinian Xu","Patricio A. Vela"],"pdf_url":"https://arxiv.org/pdf/2303.07543v4.pdf","comment":"Accepted by ICCV 2023. Code is available at:\n  https://github.com/ivalab/WDiscOOD.git"},{"id":"http://arxiv.org/abs/2305.04466v2","updated":"2023-08-30T03:10:19Z","published":"2023-05-08T05:34:15Z","title":"Generalized Universal Domain Adaptation with Generative Flow Networks","summary":"  We introduce a new problem in unsupervised domain adaptation, termed as\nGeneralized Universal Domain Adaptation (GUDA), which aims to achieve precise\nprediction of all target labels including unknown categories. GUDA bridges the\ngap between label distribution shift-based and label space mismatch-based\nvariants, essentially categorizing them as a unified problem, guiding to a\ncomprehensive framework for thoroughly solving all the variants. The key\nchallenge of GUDA is developing and identifying novel target categories while\nestimating the target label distribution. To address this problem, we take\nadvantage of the powerful exploration capability of generative flow networks\nand propose an active domain adaptation algorithm named GFlowDA, which selects\ndiverse samples with probabilities proportional to a reward function. To\nenhance the exploration capability and effectively perceive the target label\ndistribution, we tailor the states and rewards, and introduce an efficient\nsolution for parent exploration and state transition. We also propose a\ntraining paradigm for GUDA called Generalized Universal Adversarial Network\n(GUAN), which involves collaborative optimization between GUAN and GFlowNet.\nTheoretical analysis highlights the importance of exploration, and extensive\nexperiments on benchmark datasets demonstrate the superiority of GFlowDA.\n","authors":["Didi Zhu","Yinchuan Li","Yunfeng Shao","Jianye Hao","Fei Wu","Kun Kuang","Jun Xiao","Chao Wu"],"pdf_url":"https://arxiv.org/pdf/2305.04466v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15724v1","updated":"2023-08-30T02:56:55Z","published":"2023-08-30T02:56:55Z","title":"Background Debiased SAR Target Recognition via Causal Interventional\n  Regularizer","summary":"  Recent studies have utilized deep learning (DL) techniques to automatically\nextract features from synthetic aperture radar (SAR) images, which shows great\npromise for enhancing the performance of SAR automatic target recognition\n(ATR). However, our research reveals a previously overlooked issue: SAR images\nto be recognized include not only the foreground (i.e., the target), but also a\ncertain size of the background area. When a DL-model is trained exclusively on\nforeground data, its recognition performance is significantly superior to a\nmodel trained on original data that includes both foreground and background.\nThis suggests that the presence of background impedes the ability of the\nDL-model to learn additional semantic information about the target. To address\nthis issue, we construct a structural causal model (SCM) that incorporates the\nbackground as a confounder. Based on the constructed SCM, we propose a causal\nintervention based regularization method to eliminate the negative impact of\nbackground on feature semantic learning and achieve background debiased\nSAR-ATR. The proposed causal interventional regularizer can be integrated into\nany existing DL-based SAR-ATR models to mitigate the impact of background\ninterference on the feature extraction and recognition accuracy. Experimental\nresults on the Moving and Stationary Target Acquisition and Recognition (MSTAR)\ndataset indicate that the proposed method can enhance the efficiency of\nexisting DL-based methods in a plug-and-play manner.\n","authors":["Hongwei Dong","Fangzhou Han","Lingyu Si","Wenwen Qiang","Lamei Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.15724v1.pdf","comment":"38 pages, 8 figures"},{"id":"http://arxiv.org/abs/2304.11862v4","updated":"2023-08-30T02:55:09Z","published":"2023-04-24T07:16:54Z","title":"Universal Domain Adaptation via Compressive Attention Matching","summary":"  Universal domain adaptation (UniDA) aims to transfer knowledge from the\nsource domain to the target domain without any prior knowledge about the label\nset. The challenge lies in how to determine whether the target samples belong\nto common categories. The mainstream methods make judgments based on the sample\nfeatures, which overemphasizes global information while ignoring the most\ncrucial local objects in the image, resulting in limited accuracy. To address\nthis issue, we propose a Universal Attention Matching (UniAM) framework by\nexploiting the self-attention mechanism in vision transformer to capture the\ncrucial object information. The proposed framework introduces a novel\nCompressive Attention Matching (CAM) approach to explore the core information\nby compressively representing attentions. Furthermore, CAM incorporates a\nresidual-based measurement to determine the sample commonness. By utilizing the\nmeasurement, UniAM achieves domain-wise and category-wise Common Feature\nAlignment (CFA) and Target Class Separation (TCS). Notably, UniAM is the first\nmethod utilizing the attention in vision transformer directly to perform\nclassification tasks. Extensive experiments show that UniAM outperforms the\ncurrent state-of-the-art methods on various benchmark datasets.\n","authors":["Didi Zhu","Yincuan Li","Junkun Yuan","Zexi Li","Kun Kuang","Chao Wu"],"pdf_url":"https://arxiv.org/pdf/2304.11862v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10720v3","updated":"2023-08-30T02:47:27Z","published":"2023-06-19T06:41:19Z","title":"Exploring the Relationship between Samples and Masks for Robust Defect\n  Localization","summary":"  Defect detection aims to detect and localize regions out of the normal\ndistribution.Previous approaches model normality and compare it with the input\nto identify defective regions, potentially limiting their generalizability.This\npaper proposes a one-stage framework that detects defective patterns directly\nwithout the modeling process.This ability is adopted through the joint efforts\nof three parties: a generative adversarial network (GAN), a newly proposed\nscaled pattern loss, and a dynamic masked cycle-consistent auxiliary network.\nExplicit information that could indicate the position of defects is\nintentionally excluded to avoid learning any direct mapping.Experimental\nresults on the texture class of the challenging MVTec AD dataset show that the\nproposed method is 2.9\\% higher than the SOTA methods in F1-Score, while\nsubstantially outperforming SOTA methods in generalizability.\n","authors":["Jiang Lin","Yaping Yan"],"pdf_url":"https://arxiv.org/pdf/2306.10720v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10421v2","updated":"2023-08-30T02:32:08Z","published":"2023-08-21T02:13:40Z","title":"UniM$^2$AE: Multi-modal Masked Autoencoders with Unified 3D\n  Representation for 3D Perception in Autonomous Driving","summary":"  Masked Autoencoders (MAE) play a pivotal role in learning potent\nrepresentations, delivering outstanding results across various 3D perception\ntasks essential for autonomous driving. In real-world driving scenarios, it's\ncommonplace to deploy multiple sensors for comprehensive environment\nperception. While integrating multi-modal features from these sensors can\nproduce rich and powerful features, there is a noticeable gap in MAE methods\naddressing this integration. This research delves into multi-modal Masked\nAutoencoders tailored for a unified representation space in autonomous driving,\naiming to pioneer a more efficient fusion of two distinct modalities. To\nintricately marry the semantics inherent in images with the geometric\nintricacies of LiDAR point clouds, the UniM$^2$AE is proposed. This model\nstands as a potent yet straightforward, multi-modal self-supervised\npre-training framework, mainly consisting of two designs. First, it projects\nthe features from both modalities into a cohesive 3D volume space, ingeniously\nexpanded from the bird's eye view (BEV) to include the height dimension. The\nextension makes it possible to back-project the informative features, obtained\nby fusing features from both modalities, into their native modalities to\nreconstruct the multiple masked inputs. Second, the Multi-modal 3D Interactive\nModule (MMIM) is invoked to facilitate the efficient inter-modal interaction\nduring the interaction process. Extensive experiments conducted on the nuScenes\nDataset attest to the efficacy of UniM$^2$AE, indicating enhancements in 3D\nobject detection and BEV map segmentation by 1.2\\%(NDS) and 6.5\\% (mIoU),\nrespectively. Code is available at https://github.com/hollow-503/UniM2AE.\n","authors":["Jian Zou","Tianyu Huang","Guanglei Yang","Zhenhua Guo","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2308.10421v2.pdf","comment":"Code available at https://github.com/hollow-503/UniM2AE"},{"id":"http://arxiv.org/abs/2303.17895v4","updated":"2023-08-30T02:10:53Z","published":"2023-03-31T08:56:29Z","title":"EA-LSS: Edge-aware Lift-splat-shot Framework for 3D BEV Object Detection","summary":"  In recent years, great progress has been made in the Lift-Splat-Shot-based\n(LSS-based) 3D object detection method. However, inaccurate depth estimation\nremains an important constraint to the accuracy of camera-only and multi-model\n3D object detection models, especially in regions where the depth changes\nsignificantly (i.e., the \"depth jump\" problem). In this paper, we proposed a\nnovel Edge-aware Lift-splat-shot (EA-LSS) framework. Specifically, edge-aware\ndepth fusion (EADF) module is proposed to alleviate the \"depth jump\" problem\nand fine-grained depth (FGD) module to further enforce refined supervision on\ndepth. Our EA-LSS framework is compatible for any LSS-based 3D object detection\nmodels, and effectively boosts their performances with negligible increment of\ninference time. Experiments on nuScenes benchmarks demonstrate that EA-LSS is\neffective in either camera-only or multi-model models. It is worth mentioning\nthat EA-LSS achieved the state-of-the-art performance on nuScenes test\nbenchmarks with mAP and NDS of 76.5% and 77.6%, respectively.\n","authors":["Haotian Hu","Fanyi Wang","Jingwen Su","Yaonong Wang","Laifeng Hu","Weiye Fang","Jingwei Xu","Zhiwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.17895v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15705v1","updated":"2023-08-30T02:01:19Z","published":"2023-08-30T02:01:19Z","title":"Towards Earlier Detection of Oral Diseases On Smartphones Using Oral and\n  Dental RGB Images","summary":"  Oral diseases such as periodontal (gum) diseases and dental caries (cavities)\naffect billions of people across the world today. However, previous\nstate-of-the-art models have relied on X-ray images to detect oral diseases,\nmaking them inaccessible to remote monitoring, developing countries, and\ntelemedicine. To combat this overuse of X-ray imagery, we propose a lightweight\nmachine learning model capable of detecting calculus (also known as hardened\nplaque or tartar) in RGB images while running efficiently on low-end devices.\nThe model, a modified MobileNetV3-Small neural network transfer learned from\nImageNet, achieved an accuracy of 72.73% (which is comparable to\nstate-of-the-art solutions) while still being able to run on mobile devices due\nto its reduced memory requirements and processing times. A ResNet34-based model\nwas also constructed and achieved an accuracy of 81.82%. Both of these models\nwere tested on a mobile app, demonstrating their potential to limit the number\nof serious oral disease cases as their predictions can help patients schedule\nappointments earlier without the need to go to the clinic.\n","authors":["Ayush Garg","Julia Lu","Anika Maji"],"pdf_url":"https://arxiv.org/pdf/2308.15705v1.pdf","comment":"10 pages, 6 figures, 1 formula. This research was conducted as a\n  mentored project performed for a college course and research program at the\n  University of California Santa Barbara's Summer Research Academies program"},{"id":"http://arxiv.org/abs/2308.15005v2","updated":"2023-08-30T01:54:27Z","published":"2023-08-29T03:54:26Z","title":"Few-Shot Object Detection via Synthetic Features with Optimal Transport","summary":"  Few-shot object detection aims to simultaneously localize and classify the\nobjects in an image with limited training samples. However, most existing\nfew-shot object detection methods focus on extracting the features of a few\nsamples of novel classes that lack diversity. Hence, they may not be sufficient\nto capture the data distribution. To address that limitation, in this paper, we\npropose a novel approach in which we train a generator to generate synthetic\ndata for novel classes. Still, directly training a generator on the novel class\nis not effective due to the lack of novel data. To overcome that issue, we\nleverage the large-scale dataset of base classes. Our overarching goal is to\ntrain a generator that captures the data variations of the base dataset. We\nthen transform the captured variations into novel classes by generating\nsynthetic data with the trained generator. To encourage the generator to\ncapture data variations on base classes, we propose to train the generator with\nan optimal transport loss that minimizes the optimal transport distance between\nthe distributions of real and synthetic data. Extensive experiments on two\nbenchmark datasets demonstrate that the proposed method outperforms the state\nof the art. Source code will be available.\n","authors":["Anh-Khoa Nguyen Vu","Thanh-Toan Do","Vinh-Tiep Nguyen","Tam Le","Minh-Triet Tran","Tam V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2308.15005v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06767v3","updated":"2023-08-30T01:25:29Z","published":"2023-04-13T18:22:40Z","title":"RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment","summary":"  Generative foundation models are susceptible to implicit biases that can\narise from extensive unsupervised training data. Such biases can produce\nsuboptimal samples, skewed outcomes, and unfairness, with potentially serious\nconsequences. Consequently, aligning these models with human ethics and\npreferences is an essential step toward ensuring their responsible and\neffective deployment in real-world applications. Prior research has primarily\nemployed Reinforcement Learning from Human Feedback (RLHF) to address this\nproblem, where generative models are fine-tuned with RL algorithms guided by a\nhuman-feedback-informed reward model. However, the inefficiencies and\ninstabilities associated with RL algorithms frequently present substantial\nobstacles to the successful alignment, necessitating the development of a more\nrobust and streamlined approach. To this end, we introduce a new framework,\nReward rAnked FineTuning (RAFT), designed to align generative models\neffectively. Utilizing a reward model and a sufficient number of samples, our\napproach selects the high-quality samples, discarding those that exhibit\nundesired behavior, and subsequently enhancing the model by fine-tuning on\nthese filtered samples. Our studies show that RAFT can effectively improve the\nmodel performance in both reward learning and other automated metrics in both\nlarge language models and diffusion models.\n","authors":["Hanze Dong","Wei Xiong","Deepanshu Goyal","Yihan Zhang","Winnie Chow","Rui Pan","Shizhe Diao","Jipeng Zhang","Kashun Shum","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.06767v3.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2308.15692v1","updated":"2023-08-30T01:21:11Z","published":"2023-08-30T01:21:11Z","title":"Intriguing Properties of Diffusion Models: A Large-Scale Dataset for\n  Evaluating Natural Attack Capability in Text-to-Image Generative Models","summary":"  Denoising probabilistic diffusion models have shown breakthrough performance\nthat can generate more photo-realistic images or human-level illustrations than\nthe prior models such as GANs. This high image-generation capability has\nstimulated the creation of many downstream applications in various areas.\nHowever, we find that this technology is indeed a double-edged sword: We\nidentify a new type of attack, called the Natural Denoising Diffusion (NDD)\nattack based on the finding that state-of-the-art deep neural network (DNN)\nmodels still hold their prediction even if we intentionally remove their robust\nfeatures, which are essential to the human visual system (HVS), by text\nprompts. The NDD attack can generate low-cost, model-agnostic, and\ntransferrable adversarial attacks by exploiting the natural attack capability\nin diffusion models. Motivated by the finding, we construct a large-scale\ndataset, Natural Denoising Diffusion Attack (NDDA) dataset, to systematically\nevaluate the risk of the natural attack capability of diffusion models with\nstate-of-the-art text-to-image diffusion models. We evaluate the natural attack\ncapability by answering 6 research questions. Through a user study to confirm\nthe validity of the NDD attack, we find that the NDD attack can achieve an 88%\ndetection rate while being stealthy to 93% of human subjects. We also find that\nthe non-robust features embedded by diffusion models contribute to the natural\nattack capability. To confirm the model-agnostic and transferrable attack\ncapability, we perform the NDD attack against an AD vehicle and find that 73%\nof the physically printed attacks can be detected as a stop sign. We hope that\nour study and dataset can help our community to be aware of the risk of\ndiffusion models and facilitate further research toward robust DNN models.\n","authors":["Takami Sato","Justin Yue","Nanze Chen","Ningfei Wang","Qi Alfred Chen"],"pdf_url":"https://arxiv.org/pdf/2308.15692v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15690v1","updated":"2023-08-30T01:14:32Z","published":"2023-08-30T01:14:32Z","title":"CongNaMul: A Dataset for Advanced Image Processing of Soybean Sprouts","summary":"  We present 'CongNaMul', a comprehensive dataset designed for various tasks in\nsoybean sprouts image analysis. The CongNaMul dataset is curated to facilitate\ntasks such as image classification, semantic segmentation, decomposition, and\nmeasurement of length and weight. The classification task provides four classes\nto determine the quality of soybean sprouts: normal, broken, spotted, and\nbroken and spotted, for the development of AI-aided automatic quality\ninspection technology. For semantic segmentation, images with varying\ncomplexity, from single sprout images to images with multiple sprouts, along\nwith human-labelled mask images, are included. The label has 4 different\nclasses: background, head, body, tail. The dataset also provides images and\nmasks for the image decomposition task, including two separate sprout images\nand their combined form. Lastly, 5 physical features of sprouts (head length,\nbody length, body thickness, tail length, weight) are provided for image-based\nmeasurement tasks. This dataset is expected to be a valuable resource for a\nwide range of research and applications in the advanced analysis of images of\nsoybean sprouts. Also, we hope that this dataset can assist researchers\nstudying classification, semantic segmentation, decomposition, and physical\nfeature measurement in other industrial fields, in evaluating their models. The\ndataset is available at the authors' repository. (https://bhban.kr/data)\n","authors":["Byunghyun Ban","Donghun Ryu","Su-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2308.15690v1.pdf","comment":"Accepted to International Conference on ICT Convergence 2023"}]},"2023-08-29T00:00:00Z":{"Graphics":[{"id":"http://arxiv.org/abs/2308.15638v1","updated":"2023-08-29T21:13:43Z","published":"2023-08-29T21:13:43Z","title":"A Task-Parallel Approach for Localized Topological Data Structures","summary":"  Unstructured meshes are characterized by data points irregularly distributed\nin the Euclidian space. Due to the irregular nature of these data, computing\nconnectivity information between the mesh elements requires much more time and\nmemory than on uniformly distributed data. To lower storage costs, dynamic data\nstructures have been proposed. These data structures compute connectivity\ninformation on the fly and discard them when no longer needed. However,\non-the-fly computation slows down algorithms and results in a negative impact\non the time performance. To address this issue, we propose a new task-parallel\napproach to proactively compute mesh connectivity. Unlike previous approaches\nimplementing data-parallel models, where all threads run the same type of\ninstructions, our task-parallel approach allows threads to run different\nfunctions. Specifically, some threads run the algorithm of choice while other\nthreads compute connectivity information before they are actually needed. The\napproach was implemented in the new Accelerated Clustered TOPOlogical (ACTOPO)\ndata structure, which can support any processing algorithm requiring mesh\nconnectivity information. Our experiments show that ACTOPO combines the\nbenefits of state-of-the-art memory-efficient (TTK CompactTriangulation) and\ntime-efficient (TTK ExplicitTriangulation) topological data structures. It\noccupies a similar amount of memory as TTK CompactTriangulation while providing\nup to 5x speedup. Moreover, it achieves comparable time performance as TTK\nExplicitTriangulation while using only half of the memory space.\n","authors":["Guoxi Liu","Federico Iuricich"],"pdf_url":"https://arxiv.org/pdf/2308.15638v1.pdf","comment":"11pages, 13 figures, accepted at 2023 IEEE Visualization Conference\n  (VIS)"},{"id":"http://arxiv.org/abs/2308.15547v1","updated":"2023-08-29T18:11:32Z","published":"2023-08-29T18:11:32Z","title":"Efficient Ray Sampling for Radiance Fields Reconstruction","summary":"  Accelerating neural radiance fields training is of substantial practical\nvalue, as the ray sampling strategy profoundly impacts network convergence.\nMore efficient ray sampling can thus directly enhance existing NeRF models'\ntraining efficiency. We therefore propose a novel ray sampling approach for\nneural radiance fields that improves training efficiency while retaining\nphotorealistic rendering results. First, we analyze the relationship between\nthe pixel loss distribution of sampled rays and rendering quality. This reveals\nredundancy in the original NeRF's uniform ray sampling. Guided by this finding,\nwe develop a sampling method leveraging pixel regions and depth boundaries. Our\nmain idea is to sample fewer rays in training views, yet with each ray more\ninformative for scene fitting. Sampling probability increases in pixel areas\nexhibiting significant color and depth variation, greatly reducing wasteful\nrays from other regions without sacrificing precision. Through this method, not\nonly can the convergence of the network be accelerated, but the spatial\ngeometry of a scene can also be perceived more accurately. Rendering outputs\nare enhanced, especially for texture-complex regions. Experiments demonstrate\nthat our method significantly outperforms state-of-the-art techniques on public\nbenchmark datasets.\n","authors":["Shilei Sun","Ming Liu","Zhongyi Fan","Yuxue Liu","Chengwei Lv","Liquan Dong","Lingqin Kong"],"pdf_url":"https://arxiv.org/pdf/2308.15547v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2210.00647v3","updated":"2023-08-29T08:34:40Z","published":"2022-10-02T22:45:11Z","title":"IntrinsicNeRF: Learning Intrinsic Neural Radiance Fields for Editable\n  Novel View Synthesis","summary":"  Existing inverse rendering combined with neural rendering methods can only\nperform editable novel view synthesis on object-specific scenes, while we\npresent intrinsic neural radiance fields, dubbed IntrinsicNeRF, which introduce\nintrinsic decomposition into the NeRF-based neural rendering method and can\nextend its application to room-scale scenes. Since intrinsic decomposition is a\nfundamentally under-constrained inverse problem, we propose a novel\ndistance-aware point sampling and adaptive reflectance iterative clustering\noptimization method, which enables IntrinsicNeRF with traditional intrinsic\ndecomposition constraints to be trained in an unsupervised manner, resulting in\nmulti-view consistent intrinsic decomposition results. To cope with the problem\nthat different adjacent instances of similar reflectance in a scene are\nincorrectly clustered together, we further propose a hierarchical clustering\nmethod with coarse-to-fine optimization to obtain a fast hierarchical indexing\nrepresentation. It supports compelling real-time augmented applications such as\nrecoloring and illumination variation. Extensive experiments and editing\nsamples on both object-specific/room-scale scenes and synthetic/real-word data\ndemonstrate that we can obtain consistent intrinsic decomposition results and\nhigh-fidelity novel view synthesis even for challenging sequences.\n","authors":["Weicai Ye","Shuo Chen","Chong Bao","Hujun Bao","Marc Pollefeys","Zhaopeng Cui","Guofeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2210.00647v3.pdf","comment":"Accepted to ICCV2023, Project webpage:\n  https://zju3dv.github.io/intrinsic_nerf/, code:\n  https://github.com/zju3dv/IntrinsicNeRF"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.17209v2","updated":"2023-08-29T23:47:49Z","published":"2023-03-30T08:05:59Z","title":"Human from Blur: Human Pose Tracking from Blurry Images","summary":"  We propose a method to estimate 3D human poses from substantially blurred\nimages. The key idea is to tackle the inverse problem of image deblurring by\nmodeling the forward problem with a 3D human model, a texture map, and a\nsequence of poses to describe human motion. The blurring process is then\nmodeled by a temporal image aggregation step. Using a differentiable renderer,\nwe can solve the inverse problem by backpropagating the pixel-wise reprojection\nerror to recover the best human motion representation that explains a single or\nmultiple input images. Since the image reconstruction loss alone is\ninsufficient, we present additional regularization terms. To the best of our\nknowledge, we present the first method to tackle this problem. Our method\nconsistently outperforms other methods on significantly blurry inputs since\nthey lack one or multiple key functionalities that our method unifies, i.e.\nimage deblurring with sub-frame accuracy and explicit 3D modeling of non-rigid\nhuman motion.\n","authors":["Yiming Zhao","Denys Rozumnyi","Jie Song","Otmar Hilliges","Marc Pollefeys","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2303.17209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15670v1","updated":"2023-08-29T23:45:54Z","published":"2023-08-29T23:45:54Z","title":"Multimodal Foundation Models For Echocardiogram Interpretation","summary":"  Multimodal deep learning foundation models can learn the relationship between\nimages and text. In the context of medical imaging, mapping images to language\nconcepts reflects the clinical task of diagnostic image interpretation, however\ncurrent general-purpose foundation models do not perform well in this context\nbecause their training corpus have limited medical text and images. To address\nthis challenge and account for the range of cardiac physiology, we leverage\n1,032,975 cardiac ultrasound videos and corresponding expert interpretations to\ndevelop EchoCLIP, a multimodal foundation model for echocardiography. EchoCLIP\ndisplays strong zero-shot (not explicitly trained) performance in cardiac\nfunction assessment (external validation left ventricular ejection fraction\nmean absolute error (MAE) of 7.1%) and identification of implanted intracardiac\ndevices (areas under the curve (AUC) between 0.84 and 0.98 for pacemakers and\nartificial heart valves). We also developed a long-context variant (EchoCLIP-R)\nwith a custom echocardiography report text tokenizer which can accurately\nidentify unique patients across multiple videos (AUC of 0.86), identify\nclinical changes such as orthotopic heart transplants (AUC of 0.79) or cardiac\nsurgery (AUC 0.77), and enable robust image-to-text search (mean cross-modal\nretrieval rank in the top 1% of candidate text reports). These emergent\ncapabilities can be used for preliminary assessment and summarization of\nechocardiographic findings.\n","authors":["Matthew Christensen","Milos Vukadinovic","Neal Yuan","David Ouyang"],"pdf_url":"https://arxiv.org/pdf/2308.15670v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15667v1","updated":"2023-08-29T23:35:36Z","published":"2023-08-29T23:35:36Z","title":"Bridging Distribution Learning and Image Clustering in High-dimensional\n  Space","summary":"  Distribution learning focuses on learning the probability density function\nfrom a set of data samples. In contrast, clustering aims to group similar\nobjects together in an unsupervised manner. Usually, these two tasks are\nconsidered unrelated. However, the relationship between the two may be\nindirectly correlated, with Gaussian Mixture Models (GMM) acting as a bridge.\nIn this paper, we focus on exploring the correlation between distribution\nlearning and clustering, with the motivation to fill the gap between these two\nfields, utilizing an autoencoder (AE) to encode images into a high-dimensional\nlatent space. Then, Monte-Carlo Marginalization (MCMarg) and Kullback-Leibler\n(KL) divergence loss are used to fit the Gaussian components of the GMM and\nlearn the data distribution. Finally, image clustering is achieved through each\nGaussian component of GMM. Yet, the \"curse of dimensionality\" poses severe\nchallenges for most clustering algorithms. Compared with the classic\nExpectation-Maximization (EM) Algorithm, experimental results show that MCMarg\nand KL divergence can greatly alleviate the difficulty. Based on the\nexperimental results, we believe distribution learning can exploit the\npotential of GMM in image clustering within high-dimensional space.\n","authors":["Guanfang Dong","Chenqiu Zhao","Anup Basu"],"pdf_url":"https://arxiv.org/pdf/2308.15667v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15660v1","updated":"2023-08-29T22:43:46Z","published":"2023-08-29T22:43:46Z","title":"Unveiling Camouflage: A Learnable Fourier-based Augmentation for\n  Camouflaged Object Detection and Instance Segmentation","summary":"  Camouflaged object detection (COD) and camouflaged instance segmentation\n(CIS) aim to recognize and segment objects that are blended into their\nsurroundings, respectively. While several deep neural network models have been\nproposed to tackle those tasks, augmentation methods for COD and CIS have not\nbeen thoroughly explored. Augmentation strategies can help improve the\nperformance of models by increasing the size and diversity of the training data\nand exposing the model to a wider range of variations in the data. Besides, we\naim to automatically learn transformations that help to reveal the underlying\nstructure of camouflaged objects and allow the model to learn to better\nidentify and segment camouflaged objects. To achieve this, we propose a\nlearnable augmentation method in the frequency domain for COD and CIS via\nFourier transform approach, dubbed CamoFourier. Our method leverages a\nconditional generative adversarial network and cross-attention mechanism to\ngenerate a reference image and an adaptive hybrid swapping with parameters to\nmix the low-frequency component of the reference image and the high-frequency\ncomponent of the input image. This approach aims to make camouflaged objects\nmore visible for detection and segmentation models. Without bells and whistles,\nour proposed augmentation method boosts the performance of camouflaged object\ndetectors and camouflaged instance segmenters by large margins.\n","authors":["Minh-Quan Le","Minh-Triet Tran","Trung-Nghia Le","Tam V. Nguyen","Thanh-Toan Do"],"pdf_url":"https://arxiv.org/pdf/2308.15660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11418v2","updated":"2023-08-29T22:36:22Z","published":"2023-01-26T21:09:45Z","title":"Parkinson gait modelling from an anomaly deep representation","summary":"  Parkinson's Disease (PD) is associated with gait movement disorders, such as\nbradykinesia, stiffness, tremors and postural instability, caused by\nprogressive dopamine deficiency. Today, some approaches have implemented\nlearning representations to quantify kinematic patterns during locomotion,\nsupporting clinical procedures such as diagnosis and treatment planning. These\napproaches assumes a large amount of stratified and labeled data to optimize\ndiscriminative representations. Nonetheless these considerations may restrict\nthe approaches to be operable in real scenarios during clinical practice. This\nwork introduces a self-supervised generative representation to learn\ngait-motion-related patterns, under the pretext of video reconstruction and an\nanomaly detection framework. This architecture is trained following a one-class\nweakly supervised learning to avoid inter-class variance and approach the\nmultiple relationships that represent locomotion. The proposed approach was\nvalidated with 14 PD patients and 23 control subjects, and trained with the\ncontrol population only, achieving an AUC of 95%, homocedasticity level of 70%\nand shapeness level of 70% in the classification task considering its\ngeneralization.\n","authors":["Edgar Rangel","Fabio Martinez"],"pdf_url":"https://arxiv.org/pdf/2301.11418v2.pdf","comment":"Journal not submitted to any editorial"},{"id":"http://arxiv.org/abs/2305.02422v3","updated":"2023-08-29T22:12:04Z","published":"2023-05-03T20:29:04Z","title":"GAMIVAL: Video Quality Prediction on Mobile Cloud Gaming Content","summary":"  The mobile cloud gaming industry has been rapidly growing over the last\ndecade. When streaming gaming videos are transmitted to customers' client\ndevices from cloud servers, algorithms that can monitor distorted video quality\nwithout having any reference video available are desirable tools. However,\ncreating No-Reference Video Quality Assessment (NR VQA) models that can\naccurately predict the quality of streaming gaming videos rendered by computer\ngraphics engines is a challenging problem, since gaming content generally\ndiffers statistically from naturalistic videos, often lacks detail, and\ncontains many smooth regions. Until recently, the problem has been further\ncomplicated by the lack of adequate subjective quality databases of mobile\ngaming content. We have created a new gaming-specific NR VQA model called the\nGaming Video Quality Evaluator (GAMIVAL), which combines and leverages the\nadvantages of spatial and temporal gaming distorted scene statistics models, a\nneural noise model, and deep semantic features. Using a support vector\nregression (SVR) as a regressor, GAMIVAL achieves superior performance on the\nnew LIVE-Meta Mobile Cloud Gaming (LIVE-Meta MCG) video quality database.\n","authors":["Yu-Chih Chen","Avinab Saha","Chase Davis","Bo Qiu","Xiaoming Wang","Rahul Gowda","Ioannis Katsavounidis","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2305.02422v3.pdf","comment":"Accepted to IEEE SPL 2023. The implementation of GAMIVAL has been\n  made available online: https://github.com/lskdream/GAMIVAL"},{"id":"http://arxiv.org/abs/2303.02698v3","updated":"2023-08-29T20:57:07Z","published":"2023-03-05T15:27:24Z","title":"Robust affine point matching via quadratic assignment on Grassmannians","summary":"  Robust Affine matching with Grassmannians (RAG) is a new algorithm to perform\naffine registration of point clouds. The algorithm is based on minimizing the\nFrobenius distance between two elements of the Grassmannian. For this purpose,\nan indefinite relaxation of the Quadratic Assignment Problem (QAP) is used, and\nseveral approaches to affine feature matching are studied and compared.\nExperiments demonstrate that RAG is more robust to noise and point discrepancy\nthan previous methods.\n","authors":["Alexander Kolpakov","Michael Werman"],"pdf_url":"https://arxiv.org/pdf/2303.02698v3.pdf","comment":"8 pages, 23 figures; GitHub repository at\n  (https://github.com/sashakolpakov/rag)"},{"id":"http://arxiv.org/abs/2305.18221v3","updated":"2023-08-29T20:52:57Z","published":"2023-05-29T17:01:54Z","title":"GazeGNN: A Gaze-Guided Graph Neural Network for Chest X-ray\n  Classification","summary":"  Eye tracking research is important in computer vision because it can help us\nunderstand how humans interact with the visual world. Specifically for\nhigh-risk applications, such as in medical imaging, eye tracking can help us to\ncomprehend how radiologists and other medical professionals search, analyze,\nand interpret images for diagnostic and clinical purposes. Hence, the\napplication of eye tracking techniques in disease classification has become\nincreasingly popular in recent years. Contemporary works usually transform gaze\ninformation collected by eye tracking devices into visual attention maps (VAMs)\nto supervise the learning process. However, this is a time-consuming\npreprocessing step, which stops us from applying eye tracking to radiologists'\ndaily work. To solve this problem, we propose a novel gaze-guided graph neural\nnetwork (GNN), GazeGNN, to leverage raw eye-gaze data without being converted\ninto VAMs. In GazeGNN, to directly integrate eye gaze into image\nclassification, we create a unified representation graph that models both\nimages and gaze pattern information. With this benefit, we develop a real-time,\nreal-world, end-to-end disease classification algorithm for the first time in\nthe literature. This achievement demonstrates the practicality and feasibility\nof integrating real-time eye tracking techniques into the daily work of\nradiologists. To our best knowledge, GazeGNN is the first work that adopts GNN\nto integrate image and eye-gaze data. Our experiments on the public chest X-ray\ndataset show that our proposed method exhibits the best classification\nperformance compared to existing methods. The code is available at\nhttps://github.com/ukaukaaaa/GazeGNN.\n","authors":["Bin Wang","Hongyi Pan","Armstrong Aboah","Zheyuan Zhang","Elif Keles","Drew Torigian","Baris Turkbey","Elizabeth Krupinski","Jayaram Udupa","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2305.18221v3.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2308.15624v1","updated":"2023-08-29T20:45:41Z","published":"2023-08-29T20:45:41Z","title":"Detection of Mild Cognitive Impairment Using Facial Features in Video\n  Conversations","summary":"  Early detection of Mild Cognitive Impairment (MCI) leads to early\ninterventions to slow the progression from MCI into dementia. Deep Learning\n(DL) algorithms could help achieve early non-invasive, low-cost detection of\nMCI. This paper presents the detection of MCI in older adults using DL models\nbased only on facial features extracted from video-recorded conversations at\nhome. We used the data collected from the I-CONECT behavioral intervention\nstudy (NCT02871921), where several sessions of semi-structured interviews\nbetween socially isolated older individuals and interviewers were video\nrecorded. We develop a framework that extracts spatial holistic facial features\nusing a convolutional autoencoder and temporal information using transformers.\nOur proposed DL model was able to detect the I-CONECT study participants'\ncognitive conditions (MCI vs. those with normal cognition (NC)) using facial\nfeatures. The segments and sequence information of the facial features improved\nthe prediction performance compared with the non-temporal features. The\ndetection accuracy using this combined method reached 88% whereas 84% is the\naccuracy without applying the segments and sequences information of the facial\nfeatures within a video on a certain theme.\n","authors":["Muath Alsuhaibani","Hiroko H. Dodge","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2308.15624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15618v1","updated":"2023-08-29T20:25:49Z","published":"2023-08-29T20:25:49Z","title":"RACR-MIL: Weakly Supervised Skin Cancer Grading using Rank-Aware\n  Contextual Reasoning on Whole Slide Images","summary":"  Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer\nin the US. It is diagnosed by manual multi-class tumor grading using a tissue\nwhole slide image (WSI), which is subjective and suffers from inter-pathologist\nvariability. We propose an automated weakly-supervised grading approach for\ncSCC WSIs that is trained using WSI-level grade and does not require\nfine-grained tumor annotations. The proposed model, RACR-MIL, transforms each\nWSI into a bag of tiled patches and leverages attention-based multiple-instance\nlearning to assign a WSI-level grade. We propose three key innovations to\naddress general as well as cSCC-specific challenges in tumor grading. First, we\nleverage spatial and semantic proximity to define a WSI graph that encodes both\nlocal and non-local dependencies between tumor regions and leverage graph\nattention convolution to derive contextual patch features. Second, we introduce\na novel ordinal ranking constraint on the patch attention network to ensure\nthat higher-grade tumor regions are assigned higher attention. Third, we use\ntumor depth as an auxiliary task to improve grade classification in a multitask\nlearning framework. RACR-MIL achieves 2-9% improvement in grade classification\nover existing weakly-supervised approaches on a dataset of 718 cSCC tissue\nimages and localizes the tumor better. The model achieves 5-20% higher accuracy\nin difficult-to-classify high-risk grade classes and is robust to class\nimbalance.\n","authors":["Anirudh Choudhary","Angelina Hwang","Jacob Kechter","Krishnakant Saboo","Blake Bordeaux","Puneet Bhullar","Nneka Comfere","David DiCaudo","Steven Nelson","Emma Johnson","Leah Swanson","Dennis Murphree","Aaron Mangold","Ravishankar K. Iyer"],"pdf_url":"https://arxiv.org/pdf/2308.15618v1.pdf","comment":"7 pages main text, 2 page references, 3 page appendix; submitted to\n  AAAI"},{"id":"http://arxiv.org/abs/2308.15575v1","updated":"2023-08-29T19:04:42Z","published":"2023-08-29T19:04:42Z","title":"Prototype Fission: Closing Set for Robust Open-set Semi-supervised\n  Learning","summary":"  Semi-supervised Learning (SSL) has been proven vulnerable to\nout-of-distribution (OOD) samples in realistic large-scale unsupervised\ndatasets due to over-confident pseudo-labeling OODs as in-distribution (ID). A\nkey underlying problem is class-wise latent space spreading from closed seen\nspace to open unseen space, and the bias is further magnified in SSL's\nself-training loops. To close the ID distribution set so that OODs are better\nrejected for safe SSL, we propose Prototype Fission(PF) to divide class-wise\nlatent spaces into compact sub-spaces by automatic fine-grained latent space\nmining, driven by coarse-grained labels only. Specifically, we form multiple\nunique learnable sub-class prototypes for each class, optimized towards both\ndiversity and consistency. The Diversity Modeling term encourages samples to be\nclustered by one of the multiple sub-class prototypes, while the Consistency\nModeling term clusters all samples of the same class to a global prototype.\nInstead of \"opening set\", i.e., modeling OOD distribution, Prototype Fission\n\"closes set\" and makes it hard for OOD samples to fit in sub-class latent\nspace. Therefore, PF is compatible with existing methods for further\nperformance gains. Extensive experiments validate the effectiveness of our\nmethod in open-set SSL settings in terms of successfully forming sub-classes,\ndiscriminating OODs from IDs and improving overall accuracy. Codes will be\nreleased.\n","authors":["Xuwei Tan","Yi-Jie Huang","Yaqian Li"],"pdf_url":"https://arxiv.org/pdf/2308.15575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12542v2","updated":"2023-08-29T18:40:19Z","published":"2022-11-22T19:18:30Z","title":"CASSPR: Cross Attention Single Scan Place Recognition","summary":"  Place recognition based on point clouds (LiDAR) is an important component for\nautonomous robots or self-driving vehicles. Current SOTA performance is\nachieved on accumulated LiDAR submaps using either point-based or voxel-based\nstructures. While voxel-based approaches nicely integrate spatial context\nacross multiple scales, they do not exhibit the local precision of point-based\nmethods. As a result, existing methods struggle with fine-grained matching of\nsubtle geometric features in sparse single-shot Li- DAR scans. To overcome\nthese limitations, we propose CASSPR as a method to fuse point-based and\nvoxel-based approaches using cross attention transformers. CASSPR leverages a\nsparse voxel branch for extracting and aggregating information at lower\nresolution and a point-wise branch for obtaining fine-grained local\ninformation. CASSPR uses queries from one branch to try to match structures in\nthe other branch, ensuring that both extract self-contained descriptors of the\npoint cloud (rather than one branch dominating), but using both to inform the\noutput global descriptor of the point cloud. Extensive experiments show that\nCASSPR surpasses the state-of-the-art by a large margin on several datasets\n(Oxford RobotCar, TUM, USyd). For instance, it achieves AR@1 of 85.6% on the\nTUM dataset, surpassing the strongest prior model by ~15%. Our code is publicly\navailable.\n","authors":["Yan Xia","Mariia Gladkova","Rui Wang","Qianyun Li","Uwe Stilla","João F. Henriques","Daniel Cremers"],"pdf_url":"https://arxiv.org/pdf/2211.12542v2.pdf","comment":"Accepted by ICCV2023"},{"id":"http://arxiv.org/abs/2308.15564v1","updated":"2023-08-29T18:36:21Z","published":"2023-08-29T18:36:21Z","title":"Learning Sequential Information in Task-based fMRI for Synthetic Data\n  Augmentation","summary":"  Insufficiency of training data is a persistent issue in medical image\nanalysis, especially for task-based functional magnetic resonance images (fMRI)\nwith spatio-temporal imaging data acquired using specific cognitive tasks. In\nthis paper, we propose an approach for generating synthetic fMRI sequences that\ncan then be used to create augmented training datasets in downstream learning\ntasks. To synthesize high-resolution task-specific fMRI, we adapt the\n$\\alpha$-GAN structure, leveraging advantages of both GAN and variational\nautoencoder models, and propose different alternatives in aggregating temporal\ninformation. The synthetic images are evaluated from multiple perspectives\nincluding visualizations and an autism spectrum disorder (ASD) classification\ntask. The results show that the synthetic task-based fMRI can provide effective\ndata augmentation in learning the ASD classification task.\n","authors":["Jiyao Wang","Nicha C. Dvornek","Lawrence H. Staib","James S. Duncan"],"pdf_url":"https://arxiv.org/pdf/2308.15564v1.pdf","comment":"Accepted by Machine Learning in Clinical Neuroimaging 2023 (MICCAI\n  workshop), preprint version"},{"id":"http://arxiv.org/abs/2308.15557v1","updated":"2023-08-29T18:24:28Z","published":"2023-08-29T18:24:28Z","title":"A Pseudo-Boolean Polynomials Approach for Image Edge Detection","summary":"  We introduce a novel approach for image edge detection based on\npseudo-Boolean polynomials for image patches. We show that patches covering\nedge regions in the image result in pseudo-Boolean polynomials with higher\ndegrees compared to patches that cover blob regions. The proposed approach is\nbased on reduction of polynomial degree and equivalence properties of\npenalty-based pseudo-Boolean polynomials.\n","authors":["Tendai Mapungwana Chikake","Boris Goldengorin"],"pdf_url":"https://arxiv.org/pdf/2308.15557v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2110.03105v3","updated":"2023-08-29T18:15:10Z","published":"2021-10-06T23:37:21Z","title":"MetaCOG: Learning a Metacognition to Recover What Objects Are Actually\n  There","summary":"  Humans not only form representations about the world based on what we see,\nbut also learn meta-cognitive representations about how our own vision works.\nThis enables us to recognize when our vision is unreliable (e.g., when we\nrealize that we are experiencing a visual illusion) and enables us to question\nwhat we see. Inspired by this human capacity, we present MetaCOG: a model that\nincreases the robustness of object detectors by learning representations of\ntheir reliability, and does so without feedback. Specifically, MetaCOG is a\nhierarchical probabilistic model that expresses a joint distribution over the\nobjects in a 3D scene and the outputs produced by a detector. When paired with\nan off-the-shelf object detector, MetaCOG takes detections as input and infers\nthe detector's tendencies to miss objects of certain categories and to\nhallucinate objects that are not actually present, all without access to\nground-truth object labels. When paired with three modern neural object\ndetectors, MetaCOG learns useful and accurate meta-cognitive representations,\nresulting in improved performance on the detection task. Additionally, we show\nthat MetaCOG is robust to varying levels of error in the detections. Our\nresults are a proof-of-concept for a novel approach to the problem of\ncorrecting a faulty vision system's errors. The model code, datasets, results,\nand demos are available:\nhttps://osf.io/8b9qt/?view_only=8c1b1c412c6b4e1697e3c7859be2fce6\n","authors":["Marlene Berke","Zhangir Azerbayev","Mario Belledonne","Zenna Tavares","Julian Jara-Ettinger"],"pdf_url":"https://arxiv.org/pdf/2110.03105v3.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.15547v1","updated":"2023-08-29T18:11:32Z","published":"2023-08-29T18:11:32Z","title":"Efficient Ray Sampling for Radiance Fields Reconstruction","summary":"  Accelerating neural radiance fields training is of substantial practical\nvalue, as the ray sampling strategy profoundly impacts network convergence.\nMore efficient ray sampling can thus directly enhance existing NeRF models'\ntraining efficiency. We therefore propose a novel ray sampling approach for\nneural radiance fields that improves training efficiency while retaining\nphotorealistic rendering results. First, we analyze the relationship between\nthe pixel loss distribution of sampled rays and rendering quality. This reveals\nredundancy in the original NeRF's uniform ray sampling. Guided by this finding,\nwe develop a sampling method leveraging pixel regions and depth boundaries. Our\nmain idea is to sample fewer rays in training views, yet with each ray more\ninformative for scene fitting. Sampling probability increases in pixel areas\nexhibiting significant color and depth variation, greatly reducing wasteful\nrays from other regions without sacrificing precision. Through this method, not\nonly can the convergence of the network be accelerated, but the spatial\ngeometry of a scene can also be perceived more accurately. Rendering outputs\nare enhanced, especially for texture-complex regions. Experiments demonstrate\nthat our method significantly outperforms state-of-the-art techniques on public\nbenchmark datasets.\n","authors":["Shilei Sun","Ming Liu","Zhongyi Fan","Yuxue Liu","Chengwei Lv","Liquan Dong","Lingqin Kong"],"pdf_url":"https://arxiv.org/pdf/2308.15547v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2308.15536v1","updated":"2023-08-29T18:00:22Z","published":"2023-08-29T18:00:22Z","title":"DebSDF: Delving into the Details and Bias of Neural Indoor Scene\n  Reconstruction","summary":"  In recent years, the neural implicit surface has emerged as a powerful\nrepresentation for multi-view surface reconstruction due to its simplicity and\nstate-of-the-art performance. However, reconstructing smooth and detailed\nsurfaces in indoor scenes from multi-view images presents unique challenges.\nIndoor scenes typically contain large texture-less regions, making the\nphotometric loss unreliable for optimizing the implicit surface. Previous work\nutilizes monocular geometry priors to improve the reconstruction in indoor\nscenes. However, monocular priors often contain substantial errors in thin\nstructure regions due to domain gaps and the inherent inconsistencies when\nderived independently from different views. This paper presents \\textbf{DebSDF}\nto address these challenges, focusing on the utilization of uncertainty in\nmonocular priors and the bias in SDF-based volume rendering. We propose an\nuncertainty modeling technique that associates larger uncertainties with larger\nerrors in the monocular priors. High-uncertainty priors are then excluded from\noptimization to prevent bias. This uncertainty measure also informs an\nimportance-guided ray sampling and adaptive smoothness regularization,\nenhancing the learning of fine structures. We further introduce a bias-aware\nsigned distance function to density transformation that takes into account the\ncurvature and the angle between the view direction and the SDF normals to\nreconstruct fine details better. Our approach has been validated through\nextensive experiments on several challenging datasets, demonstrating improved\nqualitative and quantitative results in reconstructing thin structures in\nindoor scenes, thereby outperforming previous work.\n","authors":["Yuting Xiao","Jingwei Xu","Zehao Yu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2308.15536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15479v1","updated":"2023-08-29T17:58:55Z","published":"2023-08-29T17:58:55Z","title":"3D Adversarial Augmentations for Robust Out-of-Domain Predictions","summary":"  Since real-world training datasets cannot properly sample the long tail of\nthe underlying data distribution, corner cases and rare out-of-domain samples\ncan severely hinder the performance of state-of-the-art models. This problem\nbecomes even more severe for dense tasks, such as 3D semantic segmentation,\nwhere points of non-standard objects can be confidently associated to the wrong\nclass. In this work, we focus on improving the generalization to out-of-domain\ndata. We achieve this by augmenting the training set with adversarial examples.\nFirst, we learn a set of vectors that deform the objects in an adversarial\nfashion. To prevent the adversarial examples from being too far from the\nexisting data distribution, we preserve their plausibility through a series of\nconstraints, ensuring sensor-awareness and shapes smoothness. Then, we perform\nadversarial augmentation by applying the learned sample-independent vectors to\nthe available objects when training a model. We conduct extensive experiments\nacross a variety of scenarios on data from KITTI, Waymo, and CrashD for 3D\nobject detection, and on data from SemanticKITTI, Waymo, and nuScenes for 3D\nsemantic segmentation. Despite training on a standard single dataset, our\napproach substantially improves the robustness and generalization of both 3D\nobject detection and 3D semantic segmentation methods to out-of-domain data.\n","authors":["Alexander Lehner","Stefano Gasperini","Alvaro Marcos-Ramiro","Michael Schmidt","Nassir Navab","Benjamin Busam","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2308.15479v1.pdf","comment":"37 pages, 12 figures"},{"id":"http://arxiv.org/abs/2308.15478v1","updated":"2023-08-29T17:57:20Z","published":"2023-08-29T17:57:20Z","title":"An Adaptive Tangent Feature Perspective of Neural Networks","summary":"  In order to better understand feature learning in neural networks, we propose\na framework for understanding linear models in tangent feature space where the\nfeatures are allowed to be transformed during training. We consider linear\ntransformations of features, resulting in a joint optimization over parameters\nand transformations with a bilinear interpolation constraint. We show that this\noptimization problem has an equivalent linearly constrained optimization with\nstructured regularization that encourages approximately low rank solutions.\nSpecializing to neural network structure, we gain insights into how the\nfeatures and thus the kernel function change, providing additional nuance to\nthe phenomenon of kernel alignment when the target function is poorly\nrepresented using tangent features. In addition to verifying our theoretical\nobservations in real neural networks on a simple regression problem, we\nempirically show that an adaptive feature implementation of tangent feature\nclassification has an order of magnitude lower sample complexity than the fixed\ntangent feature model on MNIST and CIFAR-10.\n","authors":["Daniel LeJeune","Sina Alemohammad"],"pdf_url":"https://arxiv.org/pdf/2308.15478v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2308.15474v1","updated":"2023-08-29T17:52:10Z","published":"2023-08-29T17:52:10Z","title":"A General-Purpose Self-Supervised Model for Computational Pathology","summary":"  Tissue phenotyping is a fundamental computational pathology (CPath) task in\nlearning objective characterizations of histopathologic biomarkers in anatomic\npathology. However, whole-slide imaging (WSI) poses a complex computer vision\nproblem in which the large-scale image resolutions of WSIs and the enormous\ndiversity of morphological phenotypes preclude large-scale data annotation.\nCurrent efforts have proposed using pretrained image encoders with either\ntransfer learning from natural image datasets or self-supervised pretraining on\npublicly-available histopathology datasets, but have not been extensively\ndeveloped and evaluated across diverse tissue types at scale. We introduce UNI,\na general-purpose self-supervised model for pathology, pretrained using over\n100 million tissue patches from over 100,000 diagnostic haematoxylin and\neosin-stained WSIs across 20 major tissue types, and evaluated on 33\nrepresentative CPath clinical tasks in CPath of varying diagnostic\ndifficulties. In addition to outperforming previous state-of-the-art models, we\ndemonstrate new modeling capabilities in CPath such as resolution-agnostic\ntissue classification, slide classification using few-shot class prototypes,\nand disease subtyping generalization in classifying up to 108 cancer types in\nthe OncoTree code classification system. UNI advances unsupervised\nrepresentation learning at scale in CPath in terms of both pretraining data and\ndownstream evaluation, enabling data-efficient AI models that can generalize\nand transfer to a gamut of diagnostically-challenging tasks and clinical\nworkflows in anatomic pathology.\n","authors":["Richard J. Chen","Tong Ding","Ming Y. Lu","Drew F. K. Williamson","Guillaume Jaume","Bowen Chen","Andrew Zhang","Daniel Shao","Andrew H. Song","Muhammad Shaban","Mane Williams","Anurag Vaidya","Sharifa Sahai","Lukas Oldenburg","Luca L. Weishaupt","Judy J. Wang","Walt Williams","Long Phi Le","Georg Gerber","Faisal Mahmood"],"pdf_url":"https://arxiv.org/pdf/2308.15474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15472v1","updated":"2023-08-29T17:51:22Z","published":"2023-08-29T17:51:22Z","title":"Learning Modulated Transformation in GANs","summary":"  The success of style-based generators largely benefits from style modulation,\nwhich helps take care of the cross-instance variation within data. However, the\ninstance-wise stochasticity is typically introduced via regular convolution,\nwhere kernels interact with features at some fixed locations, limiting its\ncapacity for modeling geometric variation. To alleviate this problem, we equip\nthe generator in generative adversarial networks (GANs) with a plug-and-play\nmodule, termed as modulated transformation module (MTM). This module predicts\nspatial offsets under the control of latent codes, based on which the\nconvolution operation can be applied at variable locations for different\ninstances, and hence offers the model an additional degree of freedom to handle\ngeometry deformation. Extensive experiments suggest that our approach can be\nfaithfully generalized to various generative tasks, including image generation,\n3D-aware image synthesis, and video generation, and get compatible with\nstate-of-the-art frameworks without any hyper-parameter tuning. It is\nnoteworthy that, towards human generation on the challenging TaiChi dataset, we\nimprove the FID of StyleGAN3 from 21.36 to 13.60, demonstrating the efficacy of\nlearning modulated geometry transformation.\n","authors":["Ceyuan Yang","Qihang Zhang","Yinghao Xu","Jiapeng Zhu","Yujun Shen","Bo Dai"],"pdf_url":"https://arxiv.org/pdf/2308.15472v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2308.15469v1","updated":"2023-08-29T17:48:33Z","published":"2023-08-29T17:48:33Z","title":"Multimodal Contrastive Learning and Tabular Attention for Automated\n  Alzheimer's Disease Prediction","summary":"  Alongside neuroimaging such as MRI scans and PET, Alzheimer's disease (AD)\ndatasets contain valuable tabular data including AD biomarkers and clinical\nassessments. Existing computer vision approaches struggle to utilize this\nadditional information. To address these needs, we propose a generalizable\nframework for multimodal contrastive learning of image data and tabular data, a\nnovel tabular attention module for amplifying and ranking salient features in\ntables, and the application of these techniques onto Alzheimer's disease\nprediction. Experimental evaulations demonstrate the strength of our framework\nby detecting Alzheimer's disease (AD) from over 882 MR image slices from the\nADNI database. We take advantage of the high interpretability of tabular data\nand our novel tabular attention approach and through attribution of the\nattention scores for each row of the table, we note and rank the most\npredominant features. Results show that the model is capable of an accuracy of\nover 83.8%, almost a 10% increase from previous state of the art.\n","authors":["Weichen Huang"],"pdf_url":"https://arxiv.org/pdf/2308.15469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15466v1","updated":"2023-08-29T17:47:42Z","published":"2023-08-29T17:47:42Z","title":"Input margins can predict generalization too","summary":"  Understanding generalization in deep neural networks is an active area of\nresearch. A promising avenue of exploration has been that of margin\nmeasurements: the shortest distance to the decision boundary for a given sample\nor its representation internal to the network. While margins have been shown to\nbe correlated with the generalization ability of a model when measured at its\nhidden representations (hidden margins), no such link between large margins and\ngeneralization has been established for input margins. We show that while input\nmargins are not generally predictive of generalization, they can be if the\nsearch space is appropriately constrained. We develop such a measure based on\ninput margins, which we refer to as `constrained margins'. The predictive power\nof this new measure is demonstrated on the 'Predicting Generalization in Deep\nLearning' (PGDL) dataset and contrasted with hidden representation margins. We\nfind that constrained margins achieve highly competitive scores and outperform\nother margin measurements in general. This provides a novel insight on the\nrelationship between generalization and classification margins, and highlights\nthe importance of considering the data manifold for investigations of\ngeneralization in DNNs.\n","authors":["Coenraad Mouton","Marthinus W. Theunissen","Marelie H. Davel"],"pdf_url":"https://arxiv.org/pdf/2308.15466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15462v1","updated":"2023-08-29T17:40:57Z","published":"2023-08-29T17:40:57Z","title":"Online Overexposed Pixels Hallucination in Videos with Adaptive\n  Reference Frame Selection","summary":"  Low dynamic range (LDR) cameras cannot deal with wide dynamic range inputs,\nfrequently leading to local overexposure issues. We present a learning-based\nsystem to reduce these artifacts without resorting to complex acquisition\nmechanisms like alternating exposures or costly processing that are typical of\nhigh dynamic range (HDR) imaging. We propose a transformer-based deep neural\nnetwork (DNN) to infer the missing HDR details. In an ablation study, we show\nthe importance of using a multiscale DNN and train it with the proper cost\nfunction to achieve state-of-the-art quality. To aid the reconstruction of the\noverexposed areas, our DNN takes a reference frame from the past as an\nadditional input. This leverages the commonly occurring temporal instabilities\nof autoexposure to our advantage: since well-exposed details in the current\nframe may be overexposed in the future, we use reinforcement learning to train\na reference frame selection DNN that decides whether to adopt the current frame\nas a future reference. Without resorting to alternating exposures, we obtain\ntherefore a causal, HDR hallucination algorithm with potential application in\ncommon video acquisition settings. Our demo video can be found at\nhttps://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view\n","authors":["Yazhou Xing","Amrita Mazumdar","Anjul Patney","Chao Liu","Hongxu Yin","Qifeng Chen","Jan Kautz","Iuri Frosio"],"pdf_url":"https://arxiv.org/pdf/2308.15462v1.pdf","comment":"The demo video can be found at\n  https://drive.google.com/file/d/1-r12BKImLOYCLUoPzdebnMyNjJ4Rk360/view"},{"id":"http://arxiv.org/abs/2301.13803v2","updated":"2023-08-29T17:38:45Z","published":"2023-01-31T17:44:59Z","title":"Fairness-aware Vision Transformer via Debiased Self-Attention","summary":"  Vision Transformer (ViT) has recently gained significant interest in solving\ncomputer vision (CV) problems due to its capability of extracting informative\nfeatures and modeling long-range dependencies through the self-attention\nmechanism. To fully realize the advantages of ViT in real-world applications,\nrecent works have explored the trustworthiness of ViT, including its robustness\nand explainability. However, another desiderata, fairness has not yet been\nadequately addressed in the literature. We establish that the existing\nfairness-aware algorithms (primarily designed for CNNs) do not perform well on\nViT. This necessitates the need for developing our novel framework via Debiased\nSelf-Attention (DSA). DSA is a fairness-through-blindness approach that\nenforces ViT to eliminate spurious features correlated with the sensitive\nattributes for bias mitigation. Notably, adversarial examples are leveraged to\nlocate and mask the spurious features in the input image patches. In addition,\nDSA utilizes an attention weights alignment regularizer in the training\nobjective to encourage learning informative features for target prediction.\nImportantly, our DSA framework leads to improved fairness guarantees over prior\nworks on multiple prediction tasks without compromising target prediction\nperformance.\n","authors":["Yao Qiang","Chengyin Li","Prashant Khanduri","Dongxiao Zhu"],"pdf_url":"https://arxiv.org/pdf/2301.13803v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15461v1","updated":"2023-08-29T17:38:33Z","published":"2023-08-29T17:38:33Z","title":"Canonical Factors for Hybrid Neural Fields","summary":"  Factored feature volumes offer a simple way to build more compact, efficient,\nand intepretable neural fields, but also introduce biases that are not\nnecessarily beneficial for real-world data. In this work, we (1) characterize\nthe undesirable biases that these architectures have for axis-aligned signals\n-- they can lead to radiance field reconstruction differences of as high as 2\nPSNR -- and (2) explore how learning a set of canonicalizing transformations\ncan improve representations by removing these biases. We prove in a\ntwo-dimensional model problem that simultaneously learning these\ntransformations together with scene appearance succeeds with drastically\nimproved efficiency. We validate the resulting architectures, which we call\nTILTED, using image, signed distance, and radiance field reconstruction tasks,\nwhere we observe improvements across quality, robustness, compactness, and\nruntime. Results demonstrate that TILTED can enable capabilities comparable to\nbaselines that are 2x larger, while highlighting weaknesses of neural field\nevaluation procedures.\n","authors":["Brent Yi","Weijia Zeng","Sam Buchanan","Yi Ma"],"pdf_url":"https://arxiv.org/pdf/2308.15461v1.pdf","comment":"ICCV 2023. Project webpage: https://brentyi.github.io/tilted/"},{"id":"http://arxiv.org/abs/2308.15453v1","updated":"2023-08-29T17:23:33Z","published":"2023-08-29T17:23:33Z","title":"Pseudo-Boolean Polynomials Approach To Edge Detection And Image\n  Segmentation","summary":"  We introduce a deterministic approach to edge detection and image\nsegmentation by formulating pseudo-Boolean polynomials on image patches. The\napproach works by applying a binary classification of blob and edge regions in\nan image based on the degrees of pseudo-Boolean polynomials calculated on\npatches extracted from the provided image. We test our method on simple images\ncontaining primitive shapes of constant and contrasting colour and establish\nthe feasibility before applying it to complex instances like aerial landscape\nimages. The proposed method is based on the exploitation of the reduction,\npolynomial degree, and equivalence properties of penalty-based pseudo-Boolean\npolynomials.\n","authors":["Tendai Mapungwana Chikake","Boris Goldengorin","Alexey Samosyuk"],"pdf_url":"https://arxiv.org/pdf/2308.15453v1.pdf","comment":"14 pages, 8 figures, submitted to the International Conference Data\n  Analysis, Optimization and Their Applications on the Occasion of Boris\n  Mirkin's 80th Birthday January 30-31, 2023, Dolgoprudny, Moscow Region,\n  Moscow Institute of Physics and Technology\n  https://mipt.ru/education/chairs/dm/conferences/data-analysis-optimization-and-their-applications-2023.php"},{"id":"http://arxiv.org/abs/2112.09153v2","updated":"2023-08-29T17:04:19Z","published":"2021-12-16T19:00:55Z","title":"An Empirical Investigation of the Role of Pre-training in Lifelong\n  Learning","summary":"  The lifelong learning paradigm in machine learning is an attractive\nalternative to the more prominent isolated learning scheme not only due to its\nresemblance to biological learning but also its potential to reduce energy\nwaste by obviating excessive model re-training. A key challenge to this\nparadigm is the phenomenon of catastrophic forgetting. With the increasing\npopularity and success of pre-trained models in machine learning, we pose the\nquestion: What role does pre-training play in lifelong learning, specifically\nwith respect to catastrophic forgetting? We investigate existing methods in the\ncontext of large, pre-trained models and evaluate their performance on a\nvariety of text and image classification tasks, including a large-scale study\nusing a novel data set of 15 diverse NLP tasks. Across all settings, we observe\nthat generic pre-training implicitly alleviates the effects of catastrophic\nforgetting when learning multiple tasks sequentially compared to randomly\ninitialized models. We then further investigate why pre-training alleviates\nforgetting in this setting. We study this phenomenon by analyzing the loss\nlandscape, finding that pre-trained weights appear to ease forgetting by\nleading to wider minima. Based on this insight, we propose jointly optimizing\nfor current task loss and loss basin sharpness to explicitly encourage wider\nbasins during sequential fine-tuning. We show that this optimization approach\noutperforms several state-of-the-art task-sequential continual learning\nalgorithms across multiple settings, occasionally even without retaining a\nmemory that scales in size with the number of tasks.\n","authors":["Sanket Vaibhav Mehta","Darshan Patil","Sarath Chandar","Emma Strubell"],"pdf_url":"https://arxiv.org/pdf/2112.09153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15517v1","updated":"2023-08-29T16:58:03Z","published":"2023-08-29T16:58:03Z","title":"Document AI: A Comparative Study of Transformer-Based, Graph-Based\n  Models, and Convolutional Neural Networks For Document Layout Analysis","summary":"  Document AI aims to automatically analyze documents by leveraging natural\nlanguage processing and computer vision techniques. One of the major tasks of\nDocument AI is document layout analysis, which structures document pages by\ninterpreting the content and spatial relationships of layout, image, and text.\nThis task can be image-centric, wherein the aim is to identify and label\nvarious regions such as authors and paragraphs, or text-centric, where the\nfocus is on classifying individual words in a document. Although there are\nincreasingly sophisticated methods for improving layout analysis, doubts remain\nabout the extent to which their findings can be generalized to a broader\ncontext. Specifically, prior work developed systems based on very different\narchitectures, such as transformer-based, graph-based, and CNNs. However, no\nwork has mentioned the effectiveness of these models in a comparative analysis.\nMoreover, while language-independent Document AI models capable of knowledge\ntransfer have been developed, it remains to be investigated to what degree they\ncan effectively transfer knowledge. In this study, we aim to fill these gaps by\nconducting a comparative evaluation of state-of-the-art models in document\nlayout analysis and investigating the potential of cross-lingual layout\nanalysis by utilizing machine translation techniques.\n","authors":["Sotirios Kastanas","Shaomu Tan","Yi He"],"pdf_url":"https://arxiv.org/pdf/2308.15517v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15427v1","updated":"2023-08-29T16:33:16Z","published":"2023-08-29T16:33:16Z","title":"Complementing Onboard Sensors with Satellite Map: A New Perspective for\n  HD Map Construction","summary":"  High-Definition (HD) maps play a crucial role in autonomous driving systems.\nRecent methods have attempted to construct HD maps in real-time based on\ninformation obtained from vehicle onboard sensors. However, the performance of\nthese methods is significantly susceptible to the environment surrounding the\nvehicle due to the inherent limitation of onboard sensors, such as weak\ncapacity for long-range detection. In this study, we demonstrate that\nsupplementing onboard sensors with satellite maps can enhance the performance\nof HD map construction methods, leveraging the broad coverage capability of\nsatellite maps. For the purpose of further research, we release the satellite\nmap tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose\na hierarchical fusion module that enables better fusion of satellite maps\ninformation with existing methods. Specifically, we design an attention mask\nbased on segmentation and distance, applying the cross-attention mechanism to\nfuse onboard Bird's Eye View (BEV) features and satellite features in\nfeature-level fusion. An alignment module is introduced before concatenation in\nBEV-level fusion to mitigate the impact of misalignment between the two\nfeatures. The experimental results on the augmented nuScenes dataset showcase\nthe seamless integration of our module into three existing HD map construction\nmethods. It notably enhances their performance in both HD map semantic\nsegmentation and instance detection tasks.\n","authors":["Wenjie Gao","Jiawei Fu","Haodong Jing","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2308.15427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15413v1","updated":"2023-08-29T16:13:04Z","published":"2023-08-29T16:13:04Z","title":"WrappingNet: Mesh Autoencoder via Deep Sphere Deformation","summary":"  There have been recent efforts to learn more meaningful representations via\nfixed length codewords from mesh data, since a mesh serves as a complete model\nof underlying 3D shape compared to a point cloud. However, the mesh\nconnectivity presents new difficulties when constructing a deep learning\npipeline for meshes. Previous mesh unsupervised learning approaches typically\nassume category-specific templates, e.g., human face/body templates. It\nrestricts the learned latent codes to only be meaningful for objects in a\nspecific category, so the learned latent spaces are unable to be used across\ndifferent types of objects. In this work, we present WrappingNet, the first\nmesh autoencoder enabling general mesh unsupervised learning over heterogeneous\nobjects. It introduces a novel base graph in the bottleneck dedicated to\nrepresenting mesh connectivity, which is shown to facilitate learning a shared\nlatent space representing object shape. The superiority of WrappingNet mesh\nlearning is further demonstrated via improved reconstruction quality and\ncompetitive classification compared to point cloud learning, as well as latent\ninterpolation between meshes of different categories.\n","authors":["Eric Lei","Muhammad Asad Lodhi","Jiahao Pang","Junghyun Ahn","Dong Tian"],"pdf_url":"https://arxiv.org/pdf/2308.15413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15405v1","updated":"2023-08-29T16:07:18Z","published":"2023-08-29T16:07:18Z","title":"Robust Long-Tailed Learning via Label-Aware Bounded CVaR","summary":"  Data in the real-world classification problems are always imbalanced or\nlong-tailed, wherein the majority classes have the most of the samples that\ndominate the model training. In such setting, the naive model tends to have\npoor performance on the minority classes. Previously, a variety of loss\nmodifications have been proposed to address the long-tailed leaning problem,\nwhile these methods either treat the samples in the same class\nindiscriminatingly or lack a theoretical guarantee. In this paper, we propose\ntwo novel approaches based on CVaR (Conditional Value at Risk) to improve the\nperformance of long-tailed learning with a solid theoretical ground.\nSpecifically, we firstly introduce a Label-Aware Bounded CVaR (LAB-CVaR) loss\nto overcome the pessimistic result of the original CVaR, and further design the\noptimal weight bounds for LAB-CVaR theoretically. Based on LAB-CVaR, we\nadditionally propose a LAB-CVaR with logit adjustment (LAB-CVaR-logit) loss to\nstabilize the optimization process, where we also offer the theoretical\nsupport. Extensive experiments on real-world datasets with long-tailed label\ndistributions verify the superiority of our proposed methods.\n","authors":["Hong Zhu","Runpeng Yu","Xing Tang","Yifei Wang","Yuan Fang","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2308.15405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12896v2","updated":"2023-08-29T15:57:02Z","published":"2023-08-24T16:16:47Z","title":"Beyond Document Page Classification: Design, Datasets, and Challenges","summary":"  This paper highlights the need to bring document classification benchmarking\ncloser to real-world applications, both in the nature of data tested ($X$:\nmulti-channel, multi-paged, multi-industry; $Y$: class distributions and label\nset variety) and in classification tasks considered ($f$: multi-page document,\npage stream, and document bundle classification, ...). We identify the lack of\npublic multi-page document classification datasets, formalize different\nclassification tasks arising in application scenarios, and motivate the value\nof targeting efficient multi-page document representations. An experimental\nstudy on proposed multi-page document classification datasets demonstrates that\ncurrent benchmarks have become irrelevant and need to be updated to evaluate\ncomplete documents, as they naturally occur in practice. This reality check\nalso calls for more mature evaluation methodologies, covering calibration\nevaluation, inference complexity (time-memory), and a range of realistic\ndistribution shifts (e.g., born-digital vs. scanning noise, shifting page\norder). Our study ends on a hopeful note by recommending concrete avenues for\nfuture improvements.}\n","authors":["Jordy Van Landeghem","Sanket Biswas","Matthew B. Blaschko","Marie-Francine Moens"],"pdf_url":"https://arxiv.org/pdf/2308.12896v2.pdf","comment":"8 pages, under review"},{"id":"http://arxiv.org/abs/2308.15397v1","updated":"2023-08-29T15:56:38Z","published":"2023-08-29T15:56:38Z","title":"Color Aesthetics: Fuzzy based User-driven Method for Harmony and\n  Preference Prediction","summary":"  Color is the most important intrinsic sensory feature that has a powerful\nimpact on product sales. Color is even responsible for raising the aesthetic\nsenses in our brains. Account for individual differences is crucial in color\naesthetics. It requires user-driven mechanisms for various e-commerce\napplications. We propose a method for quantitative evaluation of all types of\nperceptual responses to color(s): distinct color preference, color harmony, and\ncolor combination preference. Preference for color schemes can be predicted by\ncombining preferences for the basic colors and ratings of color harmony.\nHarmonious pallets are extracted from big data set using comparison algorithms\nbased on fuzzy similarity and grouping. The proposed model results in useful\npredictions of harmony and preference of multicolored images. For example, in\nthe context of apparel coordination, it allows predicting a preference for a\nlook based on clothing colors. Our approach differs from standard aesthetic\nmodels, since in accounts for a personal variation. In addition, it can process\nnot only lower-order color pairs, but also groups of several colors.\n","authors":["Pakizar Shamoi","Atsushi Inoue","Hiroharu Kawanaka"],"pdf_url":"https://arxiv.org/pdf/2308.15397v1.pdf","comment":"It was accepted as a short paper. IFSA-SCIS 2017 Conference held in\n  Otsu, Japan"},{"id":"http://arxiv.org/abs/2308.15512v1","updated":"2023-08-29T15:39:15Z","published":"2023-08-29T15:39:15Z","title":"Shatter and Gather: Learning Referring Image Segmentation with Text\n  Supervision","summary":"  Referring image segmentation, the task of segmenting any arbitrary entities\ndescribed in free-form texts, opens up a variety of vision applications.\nHowever, manual labeling of training data for this task is prohibitively\ncostly, leading to lack of labeled data for training. We address this issue by\na weakly supervised learning approach using text descriptions of training\nimages as the only source of supervision. To this end, we first present a new\nmodel that discovers semantic entities in input image and then combines such\nentities relevant to text query to predict the mask of the referent. We also\npresent a new loss function that allows the model to be trained without any\nfurther supervision. Our method was evaluated on four public benchmarks for\nreferring image segmentation, where it clearly outperformed the existing method\nfor the same task and recent open-vocabulary segmentation models on all the\nbenchmarks.\n","authors":["Dongwon Kim","Namyup Kim","Cuiling Lan","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2308.15512v1.pdf","comment":"Accepted to ICCV 2023"},{"id":"http://arxiv.org/abs/2308.15386v1","updated":"2023-08-29T15:29:06Z","published":"2023-08-29T15:29:06Z","title":"Shape-Margin Knowledge Augmented Network for Thyroid Nodule Segmentation\n  and Diagnosis","summary":"  Thyroid nodule segmentation is a crucial step in the diagnostic procedure of\nphysicians and computer-aided diagnosis systems. Mostly, current studies treat\nsegmentation and diagnosis as independent tasks without considering the\ncorrelation between these tasks. The sequence steps of these independent tasks\nin computer-aided diagnosis systems may lead to the accumulation of errors.\nTherefore, it is worth combining them as a whole through exploring the\nrelationship between thyroid nodule segmentation and diagnosis. According to\nthe thyroid imaging reporting and data system (TI-RADS), the assessment of\nshape and margin characteristics is the prerequisite for the discrimination of\nbenign and malignant thyroid nodules. These characteristics can be observed in\nthe thyroid nodule segmentation masks. Inspired by the diagnostic procedure of\nTI-RADS, this paper proposes a shape-margin knowledge augmented network\n(SkaNet) for simultaneously thyroid nodule segmentation and diagnosis. Due to\nthe similarity in visual features between segmentation and diagnosis, SkaNet\nshares visual features in the feature extraction stage and then utilizes a\ndual-branch architecture to perform thyroid nodule segmentation and diagnosis\ntasks simultaneously. To enhance effective discriminative features, an\nexponential mixture module is devised, which incorporates convolutional feature\nmaps and self-attention maps by exponential weighting. Then, SkaNet is jointly\noptimized by a knowledge augmented multi-task loss function with a constraint\npenalty term. It embeds shape and margin characteristics through numerical\ncomputation and models the relationship between the thyroid nodule diagnosis\nresults and segmentation masks.\n","authors":["Weihua Liu","Chaochao Lin"],"pdf_url":"https://arxiv.org/pdf/2308.15386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00371v2","updated":"2023-08-29T15:25:30Z","published":"2023-07-01T15:48:33Z","title":"Learning Content-enhanced Mask Transformer for Domain Generalized\n  Urban-Scene Segmentation","summary":"  Domain-generalized urban-scene semantic segmentation (USSS) aims to learn\ngeneralized semantic predictions across diverse urban-scene styles. Unlike\ndomain gap challenges, USSS is unique in that the semantic categories are often\nsimilar in different urban scenes, while the styles can vary significantly due\nto changes in urban landscapes, weather conditions, lighting, and other\nfactors. Existing approaches typically rely on convolutional neural networks\n(CNNs) to learn the content of urban scenes.\n  In this paper, we propose a Content-enhanced Mask TransFormer (CMFormer) for\ndomain-generalized USSS. The main idea is to enhance the focus of the\nfundamental component, the mask attention mechanism, in Transformer\nsegmentation models on content information. To achieve this, we introduce a\nnovel content-enhanced mask attention mechanism. It learns mask queries from\nboth the image feature and its down-sampled counterpart, as lower-resolution\nimage features usually contain more robust content information and are less\nsensitive to style variations. These features are fused into a Transformer\ndecoder and integrated into a multi-resolution content-enhanced mask attention\nlearning scheme.\n  Extensive experiments conducted on various domain-generalized urban-scene\nsegmentation datasets demonstrate that the proposed CMFormer significantly\noutperforms existing CNN-based methods for domain-generalized semantic\nsegmentation, achieving improvements of up to 14.00\\% in terms of mIoU (mean\nintersection over union). The source code for CMFormer will be made available\nat this\n\\href{https://github.com/BiQiWHU/domain-generalized-urban-scene-segmentation}{repository}.\n","authors":["Qi Bi","Shaodi You","Theo Gevers"],"pdf_url":"https://arxiv.org/pdf/2307.00371v2.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2308.15378v1","updated":"2023-08-29T15:16:51Z","published":"2023-08-29T15:16:51Z","title":"On the Robustness of Object Detection Models in Aerial Images","summary":"  The robustness of object detection models is a major concern when applied to\nreal-world scenarios. However, the performance of most object detection models\ndegrades when applied to images subjected to corruptions, since they are\nusually trained and evaluated on clean datasets. Enhancing the robustness of\nobject detection models is of utmost importance, especially for those designed\nfor aerial images, which feature complex backgrounds, substantial variations in\nscales and orientations of objects. This paper addresses the challenge of\nassessing the robustness of object detection models in aerial images, with a\nspecific emphasis on scenarios where images are affected by clouds. In this\nstudy, we introduce two novel benchmarks based on DOTA-v1.0. The first\nbenchmark encompasses 19 prevalent corruptions, while the second focuses on\ncloud-corrupted images-a phenomenon uncommon in natural pictures yet frequent\nin aerial photography. We systematically evaluate the robustness of mainstream\nobject detection models and perform numerous ablation experiments. Through our\ninvestigations, we find that enhanced model architectures, larger networks,\nwell-crafted modules, and judicious data augmentation strategies collectively\nenhance the robustness of aerial object detection models. The benchmarks we\npropose and our comprehensive experimental analyses can facilitate research on\nrobust object detection in aerial images. Codes and datasets are available at:\n(https://github.com/hehaodong530/DOTA-C)\n","authors":["Haodong He","Jian Ding","Gui-Song Xia"],"pdf_url":"https://arxiv.org/pdf/2308.15378v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2308.15367v1","updated":"2023-08-29T15:03:05Z","published":"2023-08-29T15:03:05Z","title":"Efficient Model Personalization in Federated Learning via\n  Client-Specific Prompt Generation","summary":"  Federated learning (FL) emerges as a decentralized learning framework which\ntrains models from multiple distributed clients without sharing their data to\npreserve privacy. Recently, large-scale pre-trained models (e.g., Vision\nTransformer) have shown a strong capability of deriving robust representations.\nHowever, the data heterogeneity among clients, the limited computation\nresources, and the communication bandwidth restrict the deployment of\nlarge-scale models in FL frameworks. To leverage robust representations from\nlarge-scale models while enabling efficient model personalization for\nheterogeneous clients, we propose a novel personalized FL framework of\nclient-specific Prompt Generation (pFedPG), which learns to deploy a\npersonalized prompt generator at the server for producing client-specific\nvisual prompts that efficiently adapts frozen backbones to local data\ndistributions. Our proposed framework jointly optimizes the stages of\npersonalized prompt adaptation locally and personalized prompt generation\nglobally. The former aims to train visual prompts that adapt foundation models\nto each client, while the latter observes local optimization directions to\ngenerate personalized prompts for all clients. Through extensive experiments on\nbenchmark datasets, we show that our pFedPG is favorable against\nstate-of-the-art personalized FL methods under various types of data\nheterogeneity, allowing computation and communication efficient model\npersonalization.\n","authors":["Fu-En Yang","Chien-Yi Wang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2308.15367v1.pdf","comment":"Accepted by ICCV 2023"},{"id":"http://arxiv.org/abs/2308.15366v1","updated":"2023-08-29T15:02:53Z","published":"2023-08-29T15:02:53Z","title":"AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language\n  Models","summary":"  Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have\ndemonstrated the capability of understanding images and achieved remarkable\nperformance in various visual tasks. Despite their strong abilities in\nrecognizing common objects due to extensive training datasets, they lack\nspecific domain knowledge and have a weaker understanding of localized details\nwithin objects, which hinders their effectiveness in the Industrial Anomaly\nDetection (IAD) task. On the other hand, most existing IAD methods only provide\nanomaly scores and necessitate the manual setting of thresholds to distinguish\nbetween normal and abnormal samples, which restricts their practical\nimplementation. In this paper, we explore the utilization of LVLM to address\nthe IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. We\ngenerate training data by simulating anomalous images and producing\ncorresponding textual descriptions for each image. We also employ an image\ndecoder to provide fine-grained semantic and design a prompt learner to\nfine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the need\nfor manual threshold adjustments, thus directly assesses the presence and\nlocations of anomalies. Additionally, AnomalyGPT supports multi-turn dialogues\nand exhibits impressive few-shot in-context learning capabilities. With only\none normal shot, AnomalyGPT achieves the state-of-the-art performance with an\naccuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3%\non the MVTec-AD dataset. Code is available at\nhttps://github.com/CASIA-IVA-Lab/AnomalyGPT.\n","authors":["Zhaopeng Gu","Bingke Zhu","Guibo Zhu","Yingying Chen","Ming Tang","Jinqiao Wang"],"pdf_url":"https://arxiv.org/pdf/2308.15366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15357v1","updated":"2023-08-29T14:53:16Z","published":"2023-08-29T14:53:16Z","title":"Ego-Motion Estimation and Dynamic Motion Separation from 3D Point Clouds\n  for Accumulating Data and Improving 3D Object Detection","summary":"  New 3+1D high-resolution radar sensors are gaining importance for 3D object\ndetection in the automotive domain due to their relative affordability and\nimproved detection compared to classic low-resolution radar sensors. One\nlimitation of high-resolution radar sensors, compared to lidar sensors, is the\nsparsity of the generated point cloud. This sparsity could be partially\novercome by accumulating radar point clouds of subsequent time steps. This\ncontribution analyzes limitations of accumulating radar point clouds on the\nView-of-Delft dataset. By employing different ego-motion estimation approaches,\nthe dataset's inherent constraints, and possible solutions are analyzed.\nAdditionally, a learning-based instance motion estimation approach is deployed\nto investigate the influence of dynamic motion on the accumulated point cloud\nfor object detection. Experiments document an improved object detection\nperformance by applying an ego-motion estimation and dynamic motion correction\napproach.\n","authors":["Patrick Palmer","Martin Krueger","Richard Altendorfer","Torsten Bertram"],"pdf_url":"https://arxiv.org/pdf/2308.15357v1.pdf","comment":"Published at: AmE 2023 - Automotive meets Electronics; 14. GMM\n  Symposium (https://ieeexplore.ieee.org/document/10227711)"},{"id":"http://arxiv.org/abs/2307.12676v5","updated":"2023-08-29T14:48:37Z","published":"2023-07-24T10:30:54Z","title":"Few-shot $\\mathbf{1/a}$ Anomalies Feedback : Damage Vision Mining\n  Opportunity and Embedding Feature Imbalance","summary":"  Over the past decade, previous balanced datasets have been used to advance\ndeep learning algorithms for industrial applications. In urban infrastructures\nand living environments, damage data mining cannot avoid imbalanced data issues\nbecause of rare unseen events and the high-quality status of improved\noperations. For visual inspection, the deteriorated class acquired from the\nsurface of concrete and steel components are occasionally imbalanced. From\nnumerous related surveys, we conclude that imbalanced data problems can be\ncategorised into four types: 1) missing range of target and label valuables, 2)\nmajority-minority class imbalance, 3) foreground background of spatial\nimbalance, and 4) long-tailed class of pixel-wise imbalance. Since 2015, many\nimbalanced studies have been conducted using deep-learning approaches,\nincluding regression, image classification, object detection, and semantic\nsegmentation. However, anomaly detection for imbalanced data is not well known.\nIn this study, we highlight a one-class anomaly detection application, whether\nanomalous class or not, and demonstrate clear examples of imbalanced vision\ndatasets: medical disease, hazardous behaviour, material deterioration, plant\ndisease, river sludge, and disaster damage. We provide key results on the\nadvantage of damage-vision mining, hypothesising that the more effective the\nrange of the positive ratio, the higher the accuracy gain of the anomalies\nfeedback. In our imbalanced studies, compared with the balanced case with a\npositive ratio of $1/1$, we find that there is an applicable positive ratio\n$1/a$ where the accuracy is consistently high. However, the extremely\nimbalanced range is from one shot to $1/2a$, the accuracy of which is inferior\nto that of the applicable ratio. In contrast, with a positive ratio ranging\nover $2/a$, it shifts in the over-mining phase without an effective gain in\naccuracy.\n","authors":["Takato Yasuno"],"pdf_url":"https://arxiv.org/pdf/2307.12676v5.pdf","comment":"34 pages, 53 figures, 28 tables"},{"id":"http://arxiv.org/abs/2308.15353v1","updated":"2023-08-29T14:48:29Z","published":"2023-08-29T14:48:29Z","title":"Detect, Augment, Compose, and Adapt: Four Steps for Unsupervised Domain\n  Adaptation in Object Detection","summary":"  Unsupervised domain adaptation (UDA) plays a crucial role in object detection\nwhen adapting a source-trained detector to a target domain without annotated\ndata. In this paper, we propose a novel and effective four-step UDA approach\nthat leverages self-supervision and trains source and target data concurrently.\nWe harness self-supervised learning to mitigate the lack of ground truth in the\ntarget domain. Our method consists of the following steps: (1) identify the\nregion with the highest-confidence set of detections in each target image,\nwhich serve as our pseudo-labels; (2) crop the identified region and generate a\ncollection of its augmented versions; (3) combine these latter into a composite\nimage; (4) adapt the network to the target domain using the composed image.\nThrough extensive experiments under cross-camera, cross-weather, and\nsynthetic-to-real scenarios, our approach achieves state-of-the-art\nperformance, improving upon the nearest competitor by more than 2% in terms of\nmean Average Precision (mAP). The code is available at\nhttps://github.com/MohamedTEV/DACA.\n","authors":["Mohamed L. Mekhalfi","Davide Boscaini","Fabio Poiesi"],"pdf_url":"https://arxiv.org/pdf/2308.15353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15346v1","updated":"2023-08-29T14:41:40Z","published":"2023-08-29T14:41:40Z","title":"Enhancing Mobile Face Anti-Spoofing: A Robust Framework for Diverse\n  Attack Types under Screen Flash","summary":"  Face anti-spoofing (FAS) is crucial for securing face recognition systems.\nHowever, existing FAS methods with handcrafted binary or pixel-wise labels have\nlimitations due to diverse presentation attacks (PAs). In this paper, we\npropose an attack type robust face anti-spoofing framework under light flash,\ncalled ATR-FAS. Due to imaging differences caused by various attack types,\ntraditional FAS methods based on single binary classification network may\nresult in excessive intra-class distance of spoof faces, leading to a challenge\nof decision boundary learning. Therefore, we employed multiple networks to\nreconstruct multi-frame depth maps as auxiliary supervision, and each network\nexperts in one type of attack. A dual gate module (DGM) consisting of a type\ngate and a frame-attention gate is introduced, which perform attack type\nrecognition and multi-frame attention generation, respectively. The outputs of\nDGM are utilized as weight to mix the result of multiple expert networks. The\nmulti-experts mixture enables ATR-FAS to generate spoof-differentiated depth\nmaps, and stably detects spoof faces without being affected by different types\nof PAs. Moreover, we design a differential normalization procedure to convert\noriginal flash frames into differential frames. This simple but effective\nprocessing enhances the details in flash frames, aiding in the generation of\ndepth maps. To verify the effectiveness of our framework, we collected a\nlarge-scale dataset containing 12,660 live and spoof videos with diverse PAs\nunder dynamic flash from the smartphone screen. Extensive experiments\nillustrate that the proposed ATR-FAS significantly outperforms existing\nstate-of-the-art methods. The code and dataset will be available at\nhttps://github.com/Chaochao-Lin/ATR-FAS.\n","authors":["Weihua Liu","Chaochao Lin","Yu Yan"],"pdf_url":"https://arxiv.org/pdf/2308.15346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15345v1","updated":"2023-08-29T14:41:10Z","published":"2023-08-29T14:41:10Z","title":"IndGIC: Supervised Action Recognition under Low Illumination","summary":"  Technologies of human action recognition in the dark are gaining more and\nmore attention as huge demand in surveillance, motion control and\nhuman-computer interaction. However, because of limitation in image enhancement\nmethod and low-lighting video datasets, e.g. labeling cost, existing methods\nmeet some problems. Some video-based approached are effect and efficient in\nspecific datasets but cannot generalize to most cases while others methods\nusing multiple sensors rely heavily to prior knowledge to deal with noisy\nnature from video stream. In this paper, we proposes action recognition method\nusing deep multi-input network. Furthermore, we proposed a Independent Gamma\nIntensity Corretion (Ind-GIC) to enhance poor-illumination video, generating\none gamma for one frame to increase enhancement performance. To prove our\nmethod is effective, there is some evaluation and comparison between our method\nand existing methods. Experimental results show that our model achieves high\naccuracy in on ARID dataset.\n","authors":["Jingbo Zeng"],"pdf_url":"https://arxiv.org/pdf/2308.15345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.15344v1","updated":"2023-08-29T14:41:05Z","published":"2023-08-29T14:41:05Z","title":"Imperceptible Adversarial Attack on Deep Neural Networks from Image\n  Boundary","summary":"  Although Deep Neural Networks (DNNs), such as the convolutional neural\nnetworks (CNN) and Vision Transformers (ViTs), have been successfully applied\nin the field of computer vision, they are demonstrated to be vulnerable to\nwell-sought Adversarial Examples (AEs) that can easily fool the DNNs. The\nresearch in AEs has been active, and many adversarial attacks and explanations\nhave been proposed since they were discovered in 2014. The mystery of the AE's\nexistence is still an open question, and many studies suggest that DNN training\nalgorithms have blind spots. The salient objects usually do not overlap with\nboundaries; hence, the boundaries are not the DNN model's attention.\nNevertheless, recent studies show that the boundaries can dominate the behavior\nof the DNN models. Hence, this study aims to look at the AEs from a different\nperspective and proposes an imperceptible adversarial attack that systemically\nattacks the input image boundary for finding the AEs. The experimental results\nhave shown that the proposed boundary attacking method effectively attacks six\nCNN models and the ViT using only 32% of the input image content (from the\nboundaries) with an average success rate (SR) of 95.2% and an average peak\nsignal-to-noise ratio of 41.37 dB. Correlation analyses are conducted,\nincluding the relation between the adversarial boundary's width and the SR and\nhow the adversarial boundary changes the DNN model's attention. This paper's\ndiscoveries can potentially advance the understanding of AEs and provide a\ndifferent perspective on how AEs can be constructed.\n","authors":["Fahad Alrasheedi","Xin Zhong"],"pdf_url":"https://arxiv.org/pdf/2308.15344v1.pdf","comment":null}]},"2023-08-28T00:00:00Z":{"Graphics":[{"id":"http://arxiv.org/abs/2308.14922v1","updated":"2023-08-28T22:32:15Z","published":"2023-08-28T22:32:15Z","title":"Automated Conversion of Music Videos into Lyric Videos","summary":"  Musicians and fans often produce lyric videos, a form of music videos that\nshowcase the song's lyrics, for their favorite songs. However, making such\nvideos can be challenging and time-consuming as the lyrics need to be added in\nsynchrony and visual harmony with the video. Informed by prior work and close\nexamination of existing lyric videos, we propose a set of design guidelines to\nhelp creators make such videos. Our guidelines ensure the readability of the\nlyric text while maintaining a unified focus of attention. We instantiate these\nguidelines in a fully automated pipeline that converts an input music video\ninto a lyric video. We demonstrate the robustness of our pipeline by generating\nlyric videos from a diverse range of input sources. A user study shows that\nlyric videos generated by our pipeline are effective in maintaining text\nreadability and unifying the focus of attention.\n","authors":["Jiaju Ma","Anyi Rao","Li-Yi Wei","Rubaiat Habib Kazi","Hijung Valentina Shin","Maneesh Agrawala"],"pdf_url":"https://arxiv.org/pdf/2308.14922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14841v1","updated":"2023-08-28T18:58:01Z","published":"2023-08-28T18:58:01Z","title":"Toward Optimized VR/AR Ergonomics: Modeling and Predicting User Neck\n  Muscle Contraction","summary":"  Ergonomic efficiency is essential to the mass and prolonged adoption of VR/AR\nexperiences. While VR/AR head-mounted displays unlock users' natural wide-range\nhead movements during viewing, their neck muscle comfort is inevitably\ncompromised by the added hardware weight. Unfortunately, little quantitative\nknowledge for understanding and addressing such an issue is available so far.\n  Leveraging electromyography devices, we measure, model, and predict VR users'\nneck muscle contraction levels (MCL) while they move their heads to interact\nwith the virtual environment. Specifically, by learning from collected\nphysiological data, we develop a bio-physically inspired computational model to\npredict neck MCL under diverse head kinematic states. Beyond quantifying the\ncumulative MCL of completed head movements, our model can also predict\npotential MCL requirements with target head poses only. A series of objective\nevaluations and user studies demonstrate its prediction accuracy and\ngenerality, as well as its ability in reducing users' neck discomfort by\noptimizing the layout of visual targets. We hope this research will motivate\nnew ergonomic-centered designs for VR/AR and interactive graphics applications.\nSource code is released at:\nhttps://github.com/NYU-ICL/xr-ergonomics-neck-comfort.\n","authors":["Yunxiang Zhang","Kenneth Chen","Qi Sun"],"pdf_url":"https://arxiv.org/pdf/2308.14841v1.pdf","comment":"ACM SIGGRAPH 2023 Conference Proceedings"},{"id":"http://arxiv.org/abs/2308.14748v1","updated":"2023-08-28T17:56:18Z","published":"2023-08-28T17:56:18Z","title":"MagicAvatar: Multimodal Avatar Generation and Animation","summary":"  This report presents MagicAvatar, a framework for multimodal video generation\nand animation of human avatars. Unlike most existing methods that generate\navatar-centric videos directly from multimodal inputs (e.g., text prompts),\nMagicAvatar explicitly disentangles avatar video generation into two stages:\n(1) multimodal-to-motion and (2) motion-to-video generation. The first stage\ntranslates the multimodal inputs into motion/ control signals (e.g., human\npose, depth, DensePose); while the second stage generates avatar-centric video\nguided by these motion signals. Additionally, MagicAvatar supports avatar\nanimation by simply providing a few images of the target person. This\ncapability enables the animation of the provided human identity according to\nthe specific motion derived from the first stage. We demonstrate the\nflexibility of MagicAvatar through various applications, including text-guided\nand video-guided avatar generation, as well as multimodal avatar animation.\n","authors":["Jianfeng Zhang","Hanshu Yan","Zhongcong Xu","Jiashi Feng","Jun Hao Liew"],"pdf_url":"https://arxiv.org/pdf/2308.14748v1.pdf","comment":"Project page: https://magic-avatar.github.io/"},{"id":"http://arxiv.org/abs/2308.14740v1","updated":"2023-08-28T17:41:14Z","published":"2023-08-28T17:41:14Z","title":"Total Selfie: Generating Full-Body Selfies","summary":"  We present a method to generate full-body selfies -- photos that you take of\nyourself, but capturing your whole body as if someone else took the photo of\nyou from a few feet away. Our approach takes as input a pre-captured video of\nyour body, a target pose photo, and a selfie + background pair for each\nlocation. We introduce a novel diffusion-based approach to combine all of this\ninformation into high quality, well-composed photos of you with the desired\npose and background.\n","authors":["Bowei Chen","Brian Curless","Ira Kemelmacher-Shlizerman","Steve Seitz"],"pdf_url":"https://arxiv.org/pdf/2308.14740v1.pdf","comment":"Project page:\n  https://homes.cs.washington.edu/~boweiche/project_page/totalselfie/"},{"id":"http://arxiv.org/abs/2308.14737v1","updated":"2023-08-28T17:38:31Z","published":"2023-08-28T17:38:31Z","title":"Flexible Techniques for Differentiable Rendering with 3D Gaussians","summary":"  Fast, reliable shape reconstruction is an essential ingredient in many\ncomputer vision applications. Neural Radiance Fields demonstrated that\nphotorealistic novel view synthesis is within reach, but was gated by\nperformance requirements for fast reconstruction of real scenes and objects.\nSeveral recent approaches have built on alternative shape representations, in\nparticular, 3D Gaussians. We develop extensions to these renderers, such as\nintegrating differentiable optical flow, exporting watertight meshes and\nrendering per-ray normals. Additionally, we show how two of the recent methods\nare interoperable with each other. These reconstructions are quick, robust, and\neasily performed on GPU or CPU. For code and visual examples, see\nhttps://leonidk.github.io/fmb-plus\n","authors":["Leonid Keselman","Martial Hebert"],"pdf_url":"https://arxiv.org/pdf/2308.14737v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15989v2","updated":"2023-08-28T06:58:52Z","published":"2023-06-28T07:59:34Z","title":"Tensorformer: Normalized Matrix Attention Transformer for High-quality\n  Point Cloud Reconstruction","summary":"  Surface reconstruction from raw point clouds has been studied for decades in\nthe computer graphics community, which is highly demanded by modeling and\nrendering applications nowadays. Classic solutions, such as Poisson surface\nreconstruction, require point normals as extra input to perform reasonable\nresults. Modern transformer-based methods can work without normals, while the\nresults are less fine-grained due to limited encoding performance in local\nfusion from discrete points. We introduce a novel normalized matrix attention\ntransformer (Tensorformer) to perform high-quality reconstruction. The proposed\nmatrix attention allows for simultaneous point-wise and channel-wise message\npassing, while the previous vector attention loses neighbor point information\nacross different channels. It brings more degree of freedom in feature learning\nand thus facilitates better modeling of local geometries. Our method achieves\nstate-of-the-art on two commonly used datasets, ShapeNetCore and ABC, and\nattains 4% improvements on IOU on ShapeNet. Code can be accessed\nhttps://github.com/THHHomas/Tensorformer6.\n","authors":["Hui Tian","Zheng Qin","Renjiao Yi","Chenyang Zhu","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2306.15989v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02163v2","updated":"2023-08-28T06:03:39Z","published":"2023-04-04T23:41:20Z","title":"GINA-3D: Learning to Generate Implicit Neural Assets in the Wild","summary":"  Modeling the 3D world from sensor data for simulation is a scalable way of\ndeveloping testing and validation environments for robotic learning problems\nsuch as autonomous driving. However, manually creating or re-creating\nreal-world-like environments is difficult, expensive, and not scalable. Recent\ngenerative model techniques have shown promising progress to address such\nchallenges by learning 3D assets using only plentiful 2D images -- but still\nsuffer limitations as they leverage either human-curated image datasets or\nrenderings from manually-created synthetic 3D environments. In this paper, we\nintroduce GINA-3D, a generative model that uses real-world driving data from\ncamera and LiDAR sensors to create realistic 3D implicit neural assets of\ndiverse vehicles and pedestrians. Compared to the existing image datasets, the\nreal-world driving setting poses new challenges due to occlusions,\nlighting-variations and long-tail distributions. GINA-3D tackles these\nchallenges by decoupling representation learning and generative modeling into\ntwo stages with a learned tri-plane latent structure, inspired by recent\nadvances in generative modeling of images. To evaluate our approach, we\nconstruct a large-scale object-centric dataset containing over 1.2M images of\nvehicles and pedestrians from the Waymo Open Dataset, and a new set of 80K\nimages of long-tail instances such as construction equipment, garbage trucks,\nand cable cars. We compare our model with existing approaches and demonstrate\nthat it achieves state-of-the-art performance in quality and diversity for both\ngenerated images and geometries.\n","authors":["Bokui Shen","Xinchen Yan","Charles R. Qi","Mahyar Najibi","Boyang Deng","Leonidas Guibas","Yin Zhou","Dragomir Anguelov"],"pdf_url":"https://arxiv.org/pdf/2304.02163v2.pdf","comment":"Accepted by CVPR 2023; Our WOD-ObjectAsset can be accessed through\n  waymo.com/open"},{"id":"http://arxiv.org/abs/2212.04636v3","updated":"2023-08-28T02:51:25Z","published":"2022-12-09T02:25:20Z","title":"Ego-Body Pose Estimation via Ego-Head Pose Estimation","summary":"  Estimating 3D human motion from an egocentric video sequence plays a critical\nrole in human behavior understanding and has various applications in VR/AR.\nHowever, naively learning a mapping between egocentric videos and human motions\nis challenging, because the user's body is often unobserved by the front-facing\ncamera placed on the head of the user. In addition, collecting large-scale,\nhigh-quality datasets with paired egocentric videos and 3D human motions\nrequires accurate motion capture devices, which often limit the variety of\nscenes in the videos to lab-like environments. To eliminate the need for paired\negocentric video and human motions, we propose a new method, Ego-Body Pose\nEstimation via Ego-Head Pose Estimation (EgoEgo), which decomposes the problem\ninto two stages, connected by the head motion as an intermediate\nrepresentation. EgoEgo first integrates SLAM and a learning approach to\nestimate accurate head motion. Subsequently, leveraging the estimated head pose\nas input, EgoEgo utilizes conditional diffusion to generate multiple plausible\nfull-body motions. This disentanglement of head and body pose eliminates the\nneed for training datasets with paired egocentric videos and 3D human motion,\nenabling us to leverage large-scale egocentric video datasets and motion\ncapture datasets separately. Moreover, for systematic benchmarking, we develop\na synthetic dataset, AMASS-Replica-Ego-Syn (ARES), with paired egocentric\nvideos and human motion. On both ARES and real data, our EgoEgo model performs\nsignificantly better than the current state-of-the-art methods.\n","authors":["Jiaman Li","C. Karen Liu","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2212.04636v3.pdf","comment":"CVPR 2023 (Award Candidate)"},{"id":"http://arxiv.org/abs/2308.14244v1","updated":"2023-08-28T01:19:33Z","published":"2023-08-28T01:19:33Z","title":"HoloFusion: Towards Photo-realistic 3D Generative Modeling","summary":"  Diffusion-based image generators can now produce high-quality and diverse\nsamples, but their success has yet to fully translate to 3D generation:\nexisting diffusion methods can either generate low-resolution but 3D consistent\noutputs, or detailed 2D views of 3D objects but with potential structural\ndefects and lacking view consistency or realism. We present HoloFusion, a\nmethod that combines the best of these approaches to produce high-fidelity,\nplausible, and diverse 3D samples while learning from a collection of\nmulti-view 2D images only. The method first generates coarse 3D samples using a\nvariant of the recently proposed HoloDiffusion generator. Then, it\nindependently renders and upsamples a large number of views of the coarse 3D\nmodel, super-resolves them to add detail, and distills those into a single,\nhigh-fidelity implicit 3D representation, which also ensures view consistency\nof the final renders. The super-resolution network is trained as an integral\npart of HoloFusion, end-to-end, and the final distillation uses a new sampling\nscheme to capture the space of super-resolved signals. We compare our method\nagainst existing baselines, including DreamFusion, Get3D, EG3D, and\nHoloDiffusion, and achieve, to the best of our knowledge, the most realistic\nresults on the challenging CO3Dv2 dataset.\n","authors":["Animesh Karnewar","Niloy J. Mitra","Andrea Vedaldi","David Novotny"],"pdf_url":"https://arxiv.org/pdf/2308.14244v1.pdf","comment":"ICCV 2023 conference; project page at:\n  https://holodiffusion.github.io/holofusion"}]},"2023-08-27T00:00:00Z":{"Graphics":[{"id":"http://arxiv.org/abs/2112.14540v3","updated":"2023-08-27T04:14:25Z","published":"2021-12-29T13:34:48Z","title":"Res2NetFuse: A Fusion Method for Infrared and Visible Images","summary":"  This paper presents a novel Res2Net-based fusion framework for infrared and\nvisible images. The proposed fusion model has three parts: an encoder, a fusion\nlayer and a decoder, respectively. The Res2Net-based encoder is used to extract\nmulti-scale features of source images, the paper introducing a new training\nstrategy for training a Res2Net-based encoder that uses only a single image.\nThen, a new fusion strategy is developed based on the attention model. Finally,\nthe fused image is reconstructed by the decoder. The proposed approach is also\nanalyzed in detail. Experiments show that our method achieves state-of-the-art\nfusion performance in objective and subjective assessment by comparing with the\nexisting methods.\n","authors":["Xu Song","Xiao-Jun Wu","Hui Li","Jun Sun","Vasile Palade"],"pdf_url":"https://arxiv.org/pdf/2112.14540v3.pdf","comment":"There are some errors that need to be corrected"},{"id":"http://arxiv.org/abs/2308.12508v2","updated":"2023-08-27T02:07:26Z","published":"2023-08-24T02:28:18Z","title":"FFEINR: Flow Feature-Enhanced Implicit Neural Representation for\n  Spatio-temporal Super-Resolution","summary":"  Large-scale numerical simulations are capable of generating data up to\nterabytes or even petabytes. As a promising method of data reduction,\nsuper-resolution (SR) has been widely studied in the scientific visualization\ncommunity. However, most of them are based on deep convolutional neural\nnetworks (CNNs) or generative adversarial networks (GANs) and the scale factor\nneeds to be determined before constructing the network. As a result, a single\ntraining session only supports a fixed factor and has poor generalization\nability. To address these problems, this paper proposes a Feature-Enhanced\nImplicit Neural Representation (FFEINR) for spatio-temporal super-resolution of\nflow field data. It can take full advantage of the implicit neural\nrepresentation in terms of model structure and sampling resolution. The neural\nrepresentation is based on a fully connected network with periodic activation\nfunctions, which enables us to obtain lightweight models. The learned\ncontinuous representation can decode the low-resolution flow field input data\nto arbitrary spatial and temporal resolutions, allowing for flexible\nupsampling. The training process of FFEINR is facilitated by introducing\nfeature enhancements for the input layer, which complements the contextual\ninformation of the flow field. To demonstrate the effectiveness of the proposed\nmethod, a series of experiments are conducted on different datasets by setting\ndifferent hyperparameters. The results show that FFEINR achieves significantly\nbetter results than the trilinear interpolation method.\n","authors":["Chenyue Jiao","Chongke Bi","Lu Yang"],"pdf_url":"https://arxiv.org/pdf/2308.12508v2.pdf","comment":"This paper has been accepted and published by ChinaVis\n  2023(2023.7.21-24)"},{"id":"http://arxiv.org/abs/2307.15131v2","updated":"2023-08-27T01:53:45Z","published":"2023-07-27T18:08:19Z","title":"Seal-3D: Interactive Pixel-Level Editing for Neural Radiance Fields","summary":"  With the popularity of implicit neural representations, or neural radiance\nfields (NeRF), there is a pressing need for editing methods to interact with\nthe implicit 3D models for tasks like post-processing reconstructed scenes and\n3D content creation. While previous works have explored NeRF editing from\nvarious perspectives, they are restricted in editing flexibility, quality, and\nspeed, failing to offer direct editing response and instant preview. The key\nchallenge is to conceive a locally editable neural representation that can\ndirectly reflect the editing instructions and update instantly. To bridge the\ngap, we propose a new interactive editing method and system for implicit\nrepresentations, called Seal-3D, which allows users to edit NeRF models in a\npixel-level and free manner with a wide range of NeRF-like backbone and preview\nthe editing effects instantly. To achieve the effects, the challenges are\naddressed by our proposed proxy function mapping the editing instructions to\nthe original space of NeRF models in the teacher model and a two-stage training\nstrategy for the student model with local pretraining and global finetuning. A\nNeRF editing system is built to showcase various editing types. Our system can\nachieve compelling editing effects with an interactive speed of about 1 second.\n","authors":["Xiangyu Wang","Jingsen Zhu","Qi Ye","Yuchi Huo","Yunlong Ran","Zhihua Zhong","Jiming Chen"],"pdf_url":"https://arxiv.org/pdf/2307.15131v2.pdf","comment":"Accepted by ICCV2023. Project Page:\n  https://windingwind.github.io/seal-3d/ Code:\n  https://github.com/windingwind/seal-3d/"}]},"2023-08-26T00:00:00Z":{"Graphics":[{"id":"http://arxiv.org/abs/2308.13934v1","updated":"2023-08-26T18:20:38Z","published":"2023-08-26T18:20:38Z","title":"Patch-Grid: An Efficient and Feature-Preserving Neural Implicit Surface\n  Representation","summary":"  Neural implicit representations are known to be more compact for depicting 3D\nshapes than traditional discrete representations. However, the neural\nrepresentations tend to round sharp corners or edges and struggle to represent\nsurfaces with open boundaries. Moreover, they are slow to train. We present a\nunified neural implicit representation, called Patch-Grid, that fits to complex\nshapes efficiently, preserves sharp features, and effectively models surfaces\nwith open boundaries and thin geometric features. Our superior efficiency comes\nfrom embedding each surface patch into a local latent volume and decoding it\nusing a shared MLP decoder, which is pretrained on various local surface\ngeometries. With this pretrained decoder fixed, fitting novel shapes and local\nshape updates can be done efficiently. The faithful preservation of sharp\nfeatures is enabled by adopting a novel merge grid to perform local\nconstructive solid geometry (CSG) combinations of surface patches in the cells\nof an adaptive Octree, yielding better robustness than using a global CSG\nconstruction as proposed in the literature. Experiments show that our\nPatch-Grid method faithfully captures shapes with complex sharp features, open\nboundaries and thin structures, and outperforms existing learning-based methods\nin both efficiency and quality for surface fitting and local shape updates.\n","authors":["Guying Lin","Lei Yang","Congyi Zhang","Hao Pan","Yuhan Ping","Guodong Wei","Taku Komura","John Keyser","Wenping Wang"],"pdf_url":"https://arxiv.org/pdf/2308.13934v1.pdf","comment":null}]},"2023-08-25T00:00:00Z":{"Graphics":[{"id":"http://arxiv.org/abs/2308.11951v2","updated":"2023-08-25T18:52:03Z","published":"2023-08-23T06:49:07Z","title":"Pose Modulated Avatars from Video","summary":"  It is now possible to reconstruct dynamic human motion and shape from a\nsparse set of cameras using Neural Radiance Fields (NeRF) driven by an\nunderlying skeleton. However, a challenge remains to model the deformation of\ncloth and skin in relation to skeleton pose. Unlike existing avatar models that\nare learned implicitly or rely on a proxy surface, our approach is motivated by\nthe observation that different poses necessitate unique frequency assignments.\nNeglecting this distinction yields noisy artifacts in smooth areas or blurs\nfine-grained texture and shape details in sharp regions. We develop a\ntwo-branch neural network that is adaptive and explicit in the frequency\ndomain. The first branch is a graph neural network that models correlations\namong body parts locally, taking skeleton pose as input. The second branch\ncombines these correlation features to a set of global frequencies and then\nmodulates the feature encoding. Our experiments demonstrate that our network\noutperforms state-of-the-art methods in terms of preserving details and\ngeneralization capabilities.\n","authors":["Chunjin Song","Bastian Wandt","Helge Rhodin"],"pdf_url":"https://arxiv.org/pdf/2308.11951v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13487v1","updated":"2023-08-25T16:51:20Z","published":"2023-08-25T16:51:20Z","title":"Multi-Focus Querying of the Human Genome Information on Desktop and in\n  Virtual Reality: an Evaluation","summary":"  The human genome is incredibly information-rich, consisting of approximately\n25,000 protein-coding genes spread out over 3.2 billion nucleotide base pairs\ncontained within 24 unique chromosomes. The genome is important in maintaining\nspatial context, which assists in understanding gene interactions and\nrelationships. However, existing methods of genome visualization that utilize\nspatial awareness are inefficient and prone to limitations in presenting gene\ninformation and spatial context. This study proposed an innovative approach to\ngenome visualization and exploration utilizing virtual reality. To determine\nthe optimal placement of gene information and evaluate its essentiality in a VR\nenvironment, we implemented and conducted a user study with three different\ninteraction methods. Two interaction methods were developed in virtual reality\nto determine if gene information is better suited to be embedded within the\nchromosome ideogram or separate from the ideogram. The final ideogram\ninteraction method was performed on a desktop and served as a benchmark to\nevaluate the potential benefits associated with the use of VR. Our study\nfindings reveal a preference for VR, despite longer task completion times. In\naddition, the placement of gene information within the visualization had a\nnotable impact on the ability of a user to complete tasks. Specifically, gene\ninformation embedded within the chromosome ideogram was better suited for\nsingle target identification and summarization tasks, while separating gene\ninformation from the ideogram better supported region comparison tasks.\n","authors":["Gunnar Reiske","Sungwon In","Yalong Yang"],"pdf_url":"https://arxiv.org/pdf/2308.13487v1.pdf","comment":"To be published at ISMAR 2023 conference"},{"id":"http://arxiv.org/abs/2308.13404v1","updated":"2023-08-25T14:27:03Z","published":"2023-08-25T14:27:03Z","title":"Relighting Neural Radiance Fields with Shadow and Highlight Hints","summary":"  This paper presents a novel neural implicit radiance representation for free\nviewpoint relighting from a small set of unstructured photographs of an object\nlit by a moving point light source different from the view position. We express\nthe shape as a signed distance function modeled by a multi layer perceptron. In\ncontrast to prior relightable implicit neural representations, we do not\ndisentangle the different reflectance components, but model both the local and\nglobal reflectance at each point by a second multi layer perceptron that, in\naddition, to density features, the current position, the normal (from the\nsigned distace function), view direction, and light position, also takes shadow\nand highlight hints to aid the network in modeling the corresponding high\nfrequency light transport effects. These hints are provided as a suggestion,\nand we leave it up to the network to decide how to incorporate these in the\nfinal relit result. We demonstrate and validate our neural implicit\nrepresentation on synthetic and real scenes exhibiting a wide variety of\nshapes, material properties, and global illumination light transport.\n","authors":["Chong Zeng","Guojun Chen","Yue Dong","Pieter Peers","Hongzhi Wu","Xin Tong"],"pdf_url":"https://arxiv.org/pdf/2308.13404v1.pdf","comment":"Accepted to SIGGRAPH 2023. Author's version. Project page:\n  https://nrhints.github.io/"},{"id":"http://arxiv.org/abs/2308.00474v3","updated":"2023-08-25T12:05:35Z","published":"2023-08-01T11:55:52Z","title":"Simulating the Geometric Growth of the Marine Sponge Crella Incrustans","summary":"  Simulating marine sponge growth helps marine biologists analyze, measure, and\npredict the effects that the marine environment has on marine sponges, and vice\nversa. This paper describes a way to simulate and grow geometric models of the\nmarine sponge Crella incrustans while considering environmental factors\nincluding fluid flow and nutrients. The simulation improves upon prior work by\nchanging the skeletal architecture of the sponge in the growth model to better\nsuit the structure of Crella incrustans. The change in skeletal architecture\nand other simulation parameters are then evaluated qualitatively against photos\nof a real-life Crella incrustans sponge. The results support the hypothesis\nthat changing the skeletal architecture from radiate accretive to Halichondrid\nproduces a sponge model which is closer in resemblance to Crella incrustans\nthan the prior work.\n","authors":["Joshua O'Hagan","Andrew Chalmers","Taehyun Rhee"],"pdf_url":"https://arxiv.org/pdf/2308.00474v3.pdf","comment":"5 pages, 5 figures, IEEE VIS 2023, short paper, 9 supplementary\n  figures, 1 supplementary table"},{"id":"http://arxiv.org/abs/2203.01923v5","updated":"2023-08-25T07:30:32Z","published":"2022-03-03T18:56:08Z","title":"Recovering 3D Human Mesh from Monocular Images: A Survey","summary":"  Estimating human pose and shape from monocular images is a long-standing\nproblem in computer vision. Since the release of statistical body models, 3D\nhuman mesh recovery has been drawing broader attention. With the same goal of\nobtaining well-aligned and physically plausible mesh results, two paradigms\nhave been developed to overcome challenges in the 2D-to-3D lifting process: i)\nan optimization-based paradigm, where different data terms and regularization\nterms are exploited as optimization objectives; and ii) a regression-based\nparadigm, where deep learning techniques are embraced to solve the problem in\nan end-to-end fashion. Meanwhile, continuous efforts are devoted to improving\nthe quality of 3D mesh labels for a wide range of datasets. Though remarkable\nprogress has been achieved in the past decade, the task is still challenging\ndue to flexible body motions, diverse appearances, complex environments, and\ninsufficient in-the-wild annotations. To the best of our knowledge, this is the\nfirst survey that focuses on the task of monocular 3D human mesh recovery. We\nstart with the introduction of body models and then elaborate recovery\nframeworks and training objectives by providing in-depth analyses of their\nstrengths and weaknesses. We also summarize datasets, evaluation metrics, and\nbenchmark results. Open issues and future directions are discussed in the end,\nhoping to motivate researchers and facilitate their research in this area. A\nregularly updated project page can be found at\nhttps://github.com/tinatiansjz/hmr-survey.\n","authors":["Yating Tian","Hongwen Zhang","Yebin Liu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2203.01923v5.pdf","comment":"Published in IEEE TPAMI, Survey on monocular 3D human mesh recovery,\n  Project page: https://github.com/tinatiansjz/hmr-survey"}]},"2023-08-24T00:00:00Z":{"Graphics":[{"id":"http://arxiv.org/abs/2212.02501v4","updated":"2023-08-24T22:14:53Z","published":"2022-12-05T18:59:57Z","title":"SceneRF: Self-Supervised Monocular 3D Scene Reconstruction with Radiance\n  Fields","summary":"  3D reconstruction from a single 2D image was extensively covered in the\nliterature but relies on depth supervision at training time, which limits its\napplicability. To relax the dependence to depth we propose SceneRF, a\nself-supervised monocular scene reconstruction method using only posed image\nsequences for training. Fueled by the recent progress in neural radiance fields\n(NeRF) we optimize a radiance field though with explicit depth optimization and\na novel probabilistic sampling strategy to efficiently handle large scenes. At\ninference, a single input image suffices to hallucinate novel depth views which\nare fused together to obtain 3D scene reconstruction. Thorough experiments\ndemonstrate that we outperform all baselines for novel depth views synthesis\nand scene reconstruction, on indoor BundleFusion and outdoor SemanticKITTI.\nCode is available at https://astra-vision.github.io/SceneRF .\n","authors":["Anh-Quan Cao","Raoul de Charette"],"pdf_url":"https://arxiv.org/pdf/2212.02501v4.pdf","comment":"ICCV 2023. Project page: https://astra-vision.github.io/SceneRF"},{"id":"http://arxiv.org/abs/2308.12970v1","updated":"2023-08-24T17:59:54Z","published":"2023-08-24T17:59:54Z","title":"NeuralClothSim: Neural Deformation Fields Meet the Kirchhoff-Love Thin\n  Shell Theory","summary":"  Cloth simulation is an extensively studied problem, with a plethora of\nsolutions available in computer graphics literature. Existing cloth simulators\nproduce realistic cloth deformations that obey different types of boundary\nconditions. Nevertheless, their operational principle remains limited in\nseveral ways: They operate on explicit surface representations with a fixed\nspatial resolution, perform a series of discretised updates (which bounds their\ntemporal resolution), and require comparably large amounts of storage.\nMoreover, back-propagating gradients through the existing solvers is often not\nstraightforward, which poses additional challenges when integrating them into\nmodern neural architectures. In response to the limitations mentioned above,\nthis paper takes a fundamentally different perspective on physically-plausible\ncloth simulation and re-thinks this long-standing problem: We propose\nNeuralClothSim, i.e., a new cloth simulation approach using thin shells, in\nwhich surface evolution is encoded in neural network weights. Our\nmemory-efficient and differentiable solver operates on a new continuous\ncoordinate-based representation of dynamic surfaces, i.e., neural deformation\nfields (NDFs); it supervises NDF evolution with the rules of the non-linear\nKirchhoff-Love shell theory. NDFs are adaptive in the sense that they 1)\nallocate their capacity to the deformation details as the latter arise during\nthe cloth evolution and 2) allow surface state queries at arbitrary spatial and\ntemporal resolutions without retraining. We show how to train our\nNeuralClothSim solver while imposing hard boundary conditions and demonstrate\nmultiple applications, such as material interpolation and simulation editing.\nThe experimental results highlight the effectiveness of our formulation and its\npotential impact.\n","authors":["Navami Kairanda","Marc Habermann","Christian Theobalt","Vladislav Golyanik"],"pdf_url":"https://arxiv.org/pdf/2308.12970v1.pdf","comment":"27 pages, 22 figures and 3 tables; project page:\n  https://4dqv.mpi-inf.mpg.de/NeuralClothSim/"},{"id":"http://arxiv.org/abs/2308.12964v1","updated":"2023-08-24T17:59:01Z","published":"2023-08-24T17:59:01Z","title":"Dense Text-to-Image Generation with Attention Modulation","summary":"  Existing text-to-image diffusion models struggle to synthesize realistic\nimages given dense captions, where each text prompt provides a detailed\ndescription for a specific image region. To address this, we propose\nDenseDiffusion, a training-free method that adapts a pre-trained text-to-image\nmodel to handle such dense captions while offering control over the scene\nlayout. We first analyze the relationship between generated images' layouts and\nthe pre-trained model's intermediate attention maps. Next, we develop an\nattention modulation method that guides objects to appear in specific regions\naccording to layout guidance. Without requiring additional fine-tuning or\ndatasets, we improve image generation performance given dense captions\nregarding both automatic and human evaluation scores. In addition, we achieve\nsimilar-quality visual results with models specifically trained with layout\nconditions.\n","authors":["Yunji Kim","Jiyoung Lee","Jin-Hwa Kim","Jung-Woo Ha","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2308.12964v1.pdf","comment":"Accepted by ICCV2023. Code and data are available at\n  https://github.com/naver-ai/DenseDiffusion"},{"id":"http://arxiv.org/abs/2308.08741v2","updated":"2023-08-24T15:43:17Z","published":"2023-08-17T02:33:16Z","title":"MIPS-Fusion: Multi-Implicit-Submaps for Scalable and Robust Online\n  Neural RGB-D Reconstruction","summary":"  We introduce MIPS-Fusion, a robust and scalable online RGB-D reconstruction\nmethod based on a novel neural implicit representation --\nmulti-implicit-submap. Different from existing neural RGB-D reconstruction\nmethods lacking either flexibility with a single neural map or scalability due\nto extra storage of feature grids, we propose a pure neural representation\ntackling both difficulties with a divide-and-conquer design. In our method,\nneural submaps are incrementally allocated alongside the scanning trajectory\nand efficiently learned with local neural bundle adjustments. The submaps can\nbe refined individually in a back-end optimization and optimized jointly to\nrealize submap-level loop closure. Meanwhile, we propose a hybrid tracking\napproach combining randomized and gradient-based pose optimizations. For the\nfirst time, randomized optimization is made possible in neural tracking with\nseveral key designs to the learning process, enabling efficient and robust\ntracking even under fast camera motions. The extensive evaluation demonstrates\nthat our method attains higher reconstruction quality than the state of the\narts for large-scale scenes and under fast camera motions.\n","authors":["Yijie Tang","Jiazhao Zhang","Zhinan Yu","He Wang","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2308.08741v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12158v2","updated":"2023-08-24T15:10:25Z","published":"2023-08-23T14:18:56Z","title":"A Visualization System for Hexahedral Mesh Quality Study","summary":"  In this paper, we introduce a new 3D hex mesh visual analysis system that\nemphasizes poor-quality areas with an aggregated glyph, highlights overlapping\nelements, and provides detailed boundary error inspection in three forms. By\nsupporting multi-level analysis through multiple views, our system effectively\nevaluates various mesh models and compares the performance of mesh generation\nand optimization algorithms for hexahedral meshes.\n","authors":["Lei Si","Guoning Chen"],"pdf_url":"https://arxiv.org/pdf/2308.12158v2.pdf","comment":"Accepted by IEEE VIS 2023 Short Papers and will be published on IEEE\n  Xplore. Paper contains 4 pages, and 1 reference page. Supplemental includes 4\n  pages"},{"id":"http://arxiv.org/abs/2308.11909v2","updated":"2023-08-24T13:05:46Z","published":"2023-08-23T04:29:40Z","title":"Edge-aware Hard Clustering Graph Pooling for Brain Imaging Data","summary":"  Graph Convolutional Networks (GCNs) can capture non-Euclidean spatial\ndependence between different brain regions, and the graph pooling operator in\nGCNs is key to enhancing the representation learning capability and acquiring\nabnormal brain maps. However, the majority of existing research designs graph\npooling operators only from the perspective of nodes while disregarding the\noriginal edge features, in a way that not only confines graph pooling\napplication scenarios, but also diminishes its ability to capture critical\nsubstructures. In this study, a clustering graph pooling method that first\nsupports multidimensional edge features, called Edge-aware hard clustering\ngraph pooling (EHCPool), is developed. EHCPool proposes the first\n'Edge-to-node' score evaluation criterion based on edge features to assess node\nfeature significance. To more effectively capture the critical subgraphs, a\nnovel Iteration n-top strategy is further designed to adaptively learn sparse\nhard clustering assignments for graphs. Subsequently, an innovative N-E\nAggregation strategy is presented to aggregate node and edge feature\ninformation in each independent subgraph. The proposed model was evaluated on\nmulti-site brain imaging public datasets and yielded state-of-the-art\nperformance. We believe this method is the first deep learning tool with the\npotential to probe different types of abnormal functional brain networks from\ndata-driven perspective.\n","authors":["Cheng Zhu","Jiayi Zhu","Lijuan Zhang","Xi Wu","Shuqi Yang","Ping Liang","Honghan Chen","Ying Tan"],"pdf_url":"https://arxiv.org/pdf/2308.11909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12751v1","updated":"2023-08-24T12:56:39Z","published":"2023-08-24T12:56:39Z","title":"Motion In-Betweening with Phase Manifolds","summary":"  This paper introduces a novel data-driven motion in-betweening system to\nreach target poses of characters by making use of phases variables learned by a\nPeriodic Autoencoder. Our approach utilizes a mixture-of-experts neural network\nmodel, in which the phases cluster movements in both space and time with\ndifferent expert weights. Each generated set of weights then produces a\nsequence of poses in an autoregressive manner between the current and target\nstate of the character. In addition, to satisfy poses which are manually\nmodified by the animators or where certain end effectors serve as constraints\nto be reached by the animation, a learned bi-directional control scheme is\nimplemented to satisfy such constraints. The results demonstrate that using\nphases for motion in-betweening tasks sharpen the interpolated movements, and\nfurthermore stabilizes the learning process. Moreover, using phases for motion\nin-betweening tasks can also synthesize more challenging movements beyond\nlocomotion behaviors. Additionally, style control is enabled between given\ntarget keyframes. Our proposed framework can compete with popular\nstate-of-the-art methods for motion in-betweening in terms of motion quality\nand generalization, especially in the existence of long transition durations.\nOur framework contributes to faster prototyping workflows for creating animated\ncharacter sequences, which is of enormous interest for the game and film\nindustry.\n","authors":["Paul Starke","Sebastian Starke","Taku Komura","Frank Steinicke"],"pdf_url":"https://arxiv.org/pdf/2308.12751v1.pdf","comment":"17 pages, 11 figures, conference"},{"id":"http://arxiv.org/abs/2305.00391v2","updated":"2023-08-24T11:17:37Z","published":"2023-04-30T05:25:39Z","title":"Alternately denoising and reconstructing unoriented point sets","summary":"  We propose a new strategy to bridge point cloud denoising and surface\nreconstruction by alternately updating the denoised point clouds and the\nreconstructed surfaces. In Poisson surface reconstruction, the implicit\nfunction is generated by a set of smooth basis functions centered at the\noctnodes. When the octree depth is properly selected, the reconstructed surface\nis a good smooth approximation of the noisy point set. Our method projects the\nnoisy points onto the surface and alternately reconstructs and projects the\npoint set. We use the iterative Poisson surface reconstruction (iPSR) to\nsupport unoriented surface reconstruction. Our method iteratively performs iPSR\nand acts as an outer loop of iPSR. Considering that the octree depth\nsignificantly affects the reconstruction results, we propose an adaptive depth\nselection strategy to ensure an appropriate depth choice. To manage the\noversmoothing phenomenon near the sharp features, we propose a\n$\\lambda$-projection method, which means to project the noisy points onto the\nsurface with an individual control coefficient $\\lambda_{i}$ for each point.\nThe coefficients are determined through a Voronoi-based feature detection\nmethod. Experimental results show that our method achieves high performance in\npoint cloud denoising and unoriented surface reconstruction within different\nnoise scales, and exhibits well-rounded performance in various types of inputs.\nThe source code is available\nat~\\url{https://github.com/Submanifold/AlterUpdate}.\n","authors":["Dong Xiao","Zuoqiang Shi","Bin Wang"],"pdf_url":"https://arxiv.org/pdf/2305.00391v2.pdf","comment":"Accepted by Computers & Graphics from CAD/Graphics 2023"},{"id":"http://arxiv.org/abs/2308.10517v2","updated":"2023-08-24T10:35:24Z","published":"2023-08-21T07:10:43Z","title":"Patternshop: Editing Point Patterns by Image Manipulation","summary":"  Point patterns are characterized by their density and correlation. While\nspatial variation of density is well-understood, analysis and synthesis of\nspatially-varying correlation is an open challenge. No tools are available to\nintuitively edit such point patterns, primarily due to the lack of a compact\nrepresentation for spatially varying correlation. We propose a low-dimensional\nperceptual embedding for point correlations. This embedding can map point\npatterns to common three-channel raster images, enabling manipulation with\noff-the-shelf image editing software. To synthesize back point patterns, we\npropose a novel edge-aware objective that carefully handles sharp variations in\ndensity and correlation. The resulting framework allows intuitive and\nbackward-compatible manipulation of point patterns, such as recoloring,\nrelighting to even texture synthesis that have not been available to 2D point\npattern design before. Effectiveness of our approach is tested in several user\nexperiments.\n","authors":["Xingchang Huang","Tobias Ritschel","Hans-Peter Seidel","Pooran Memari","Gurprit Singh"],"pdf_url":"https://arxiv.org/pdf/2308.10517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12646v1","updated":"2023-08-24T08:42:06Z","published":"2023-08-24T08:42:06Z","title":"The GENEA Challenge 2023: A large scale evaluation of gesture generation\n  models in monadic and dyadic settings","summary":"  This paper reports on the GENEA Challenge 2023, in which participating teams\nbuilt speech-driven gesture-generation systems using the same speech and motion\ndataset, followed by a joint evaluation. This year's challenge provided data on\nboth sides of a dyadic interaction, allowing teams to generate full-body motion\nfor an agent given its speech (text and audio) and the speech and motion of the\ninterlocutor. We evaluated 12 submissions and 2 baselines together with\nheld-out motion-capture data in several large-scale user studies. The studies\nfocused on three aspects: 1) the human-likeness of the motion, 2) the\nappropriateness of the motion for the agent's own speech whilst controlling for\nthe human-likeness of the motion, and 3) the appropriateness of the motion for\nthe behaviour of the interlocutor in the interaction, using a setup that\ncontrols for both the human-likeness of the motion and the agent's own speech.\nWe found a large span in human-likeness between challenge submissions, with a\nfew systems rated close to human mocap. Appropriateness seems far from being\nsolved, with most submissions performing in a narrow range slightly above\nchance, far behind natural motion. The effect of the interlocutor is even more\nsubtle, with submitted systems at best performing barely above chance.\nInterestingly, a dyadic system being highly appropriate for agent speech does\nnot necessarily imply high appropriateness for the interlocutor. Additional\nmaterial is available via the project website at\nhttps://svito-zar.github.io/GENEAchallenge2023/ .\n","authors":["Taras Kucherenko","Rajmund Nagy","Youngwoo Yoon","Jieyeon Woo","Teodor Nikolov","Mihail Tsakov","Gustav Eje Henter"],"pdf_url":"https://arxiv.org/pdf/2308.12646v1.pdf","comment":"The first three authors made equal contributions. Accepted for\n  publication at the ACM International Conference on Multimodal Interaction\n  (ICMI)"},{"id":"http://arxiv.org/abs/2212.01735v4","updated":"2023-08-24T04:39:38Z","published":"2022-12-04T03:45:08Z","title":"Neural Fourier Filter Bank","summary":"  We present a novel method to provide efficient and highly detailed\nreconstructions. Inspired by wavelets, we learn a neural field that decompose\nthe signal both spatially and frequency-wise. We follow the recent grid-based\nparadigm for spatial decomposition, but unlike existing work, encourage\nspecific frequencies to be stored in each grid via Fourier features encodings.\nWe then apply a multi-layer perceptron with sine activations, taking these\nFourier encoded features in at appropriate layers so that higher-frequency\ncomponents are accumulated on top of lower-frequency components sequentially,\nwhich we sum up to form the final output. We demonstrate that our method\noutperforms the state of the art regarding model compactness and convergence\nspeed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural\nradiance fields. Our code is available at https://github.com/ubc-vision/NFFB.\n","authors":["Zhijie Wu","Yuhe Jin","Kwang Moo Yi"],"pdf_url":"https://arxiv.org/pdf/2212.01735v4.pdf","comment":null}]},"2023-08-23T00:00:00Z":{"Graphics":[{"id":"http://arxiv.org/abs/2308.12452v1","updated":"2023-08-23T22:22:20Z","published":"2023-08-23T22:22:20Z","title":"ARF-Plus: Controlling Perceptual Factors in Artistic Radiance Fields for\n  3D Scene Stylization","summary":"  The radiance fields style transfer is an emerging field that has recently\ngained popularity as a means of 3D scene stylization, thanks to the outstanding\nperformance of neural radiance fields in 3D reconstruction and view synthesis.\nWe highlight a research gap in radiance fields style transfer, the lack of\nsufficient perceptual controllability, motivated by the existing concept in the\n2D image style transfer. In this paper, we present ARF-Plus, a 3D neural style\ntransfer framework offering manageable control over perceptual factors, to\nsystematically explore the perceptual controllability in 3D scene stylization.\nFour distinct types of controls - color preservation control, (style pattern)\nscale control, spatial (selective stylization area) control, and depth\nenhancement control - are proposed and integrated into this framework. Results\nfrom real-world datasets, both quantitative and qualitative, show that the four\ntypes of controls in our ARF-Plus framework successfully accomplish their\ncorresponding perceptual controls when stylizing 3D scenes. These techniques\nwork well for individual style inputs as well as for the simultaneous\napplication of multiple styles within a scene. This unlocks a realm of\nlimitless possibilities, allowing customized modifications of stylization\neffects and flexible merging of the strengths of different styles, ultimately\nenabling the creation of novel and eye-catching stylistic effects on 3D scenes.\n","authors":["Wenzhao Li","Tianhao Wu","Fangcheng Zhong","Cengiz Oztireli"],"pdf_url":"https://arxiv.org/pdf/2308.12452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.05669v2","updated":"2023-08-23T20:52:27Z","published":"2023-04-12T07:46:05Z","title":"Factorized Inverse Path Tracing for Efficient and Accurate\n  Material-Lighting Estimation","summary":"  Inverse path tracing has recently been applied to joint material and lighting\nestimation, given geometry and multi-view HDR observations of an indoor scene.\nHowever, it has two major limitations: path tracing is expensive to compute,\nand ambiguities exist between reflection and emission. Our Factorized Inverse\nPath Tracing (FIPT) addresses these challenges by using a factored light\ntransport formulation and finds emitters driven by rendering errors. Our\nalgorithm enables accurate material and lighting optimization faster than\nprevious work, and is more effective at resolving ambiguities. The exhaustive\nexperiments on synthetic scenes show that our method (1) outperforms\nstate-of-the-art indoor inverse rendering and relighting methods particularly\nin the presence of complex illumination effects; (2) speeds up inverse path\ntracing optimization to less than an hour. We further demonstrate robustness to\nnoisy inputs through material and lighting estimates that allow plausible\nrelighting in a real scene. The source code is available at:\nhttps://github.com/lwwu2/fipt\n","authors":["Liwen Wu","Rui Zhu","Mustafa B. Yaldiz","Yinhao Zhu","Hong Cai","Janarbek Matai","Fatih Porikli","Tzu-Mao Li","Manmohan Chandraker","Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2304.05669v2.pdf","comment":"Updated experiment results; modified real-world sections"},{"id":"http://arxiv.org/abs/2308.13551v1","updated":"2023-08-23T15:54:42Z","published":"2023-08-23T15:54:42Z","title":"Dance with You: The Diversity Controllable Dancer Generation via\n  Diffusion Models","summary":"  Recently, digital humans for interpersonal interaction in virtual\nenvironments have gained significant attention. In this paper, we introduce a\nnovel multi-dancer synthesis task called partner dancer generation, which\ninvolves synthesizing virtual human dancers capable of performing dance with\nusers. The task aims to control the pose diversity between the lead dancer and\nthe partner dancer. The core of this task is to ensure the controllable\ndiversity of the generated partner dancer while maintaining temporal\ncoordination with the lead dancer. This scenario varies from earlier research\nin generating dance motions driven by music, as our emphasis is on\nautomatically designing partner dancer postures according to pre-defined\ndiversity, the pose of lead dancer, as well as the accompanying tunes. To\nachieve this objective, we propose a three-stage framework called\nDance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to\ncollect a wide range of basic dance poses as references for motion generation.\nThen, we introduce a hyper-parameter that coordinates the similarity between\ndancers by masking poses to prevent the generation of sequences that are\nover-diverse or consistent. To avoid the rigidity of movements, we design a\nDance Pre-generated stage to pre-generate these masked poses instead of filling\nthem with zeros. After that, a Dance Motion Transfer stage is adopted with\nleader sequences and music, in which a multi-conditional sampling formula is\nrewritten to transfer the pre-generated poses into a sequence with a partner\nstyle. In practice, to address the lack of multi-person datasets, we introduce\nAIST-M, a new dataset for partner dancer generation, which is publicly\navailiable. Comprehensive evaluations on our AIST-M dataset demonstrate that\nthe proposed DanY can synthesize satisfactory partner dancer results with\ncontrollable diversity.\n","authors":["Siyue Yao","Mingjie Sun","Bingliang Li","Fengyu Yang","Junle Wang","Ruimao Zhang"],"pdf_url":"https://arxiv.org/pdf/2308.13551v1.pdf","comment":"Accepted by ACM MM"},{"id":"http://arxiv.org/abs/2303.11225v2","updated":"2023-08-23T11:46:57Z","published":"2023-03-20T16:07:02Z","title":"HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and\n  Dynamic Details","summary":"  3D Morphable Models (3DMMs) demonstrate great potential for reconstructing\nfaithful and animatable 3D facial surfaces from a single image. The facial\nsurface is influenced by the coarse shape, as well as the static detail (e,g.,\nperson-specific appearance) and dynamic detail (e.g., expression-driven\nwrinkles). Previous work struggles to decouple the static and dynamic details\nthrough image-level supervision, leading to reconstructions that are not\nrealistic. In this paper, we aim at high-fidelity 3D face reconstruction and\npropose HiFace to explicitly model the static and dynamic details.\nSpecifically, the static detail is modeled as the linear combination of a\ndisplacement basis, while the dynamic detail is modeled as the linear\ninterpolation of two displacement maps with polarized expressions. We exploit\nseveral loss functions to jointly learn the coarse shape and fine details with\nboth synthetic and real-world datasets, which enable HiFace to reconstruct\nhigh-fidelity 3D shapes with animatable details. Extensive quantitative and\nqualitative experiments demonstrate that HiFace presents state-of-the-art\nreconstruction quality and faithfully recovers both the static and dynamic\ndetails. Our project page can be found at https://project-hiface.github.io.\n","authors":["Zenghao Chai","Tianke Zhang","Tianyu He","Xu Tan","Tadas Baltrušaitis","HsiangTao Wu","Runnan Li","Sheng Zhao","Chun Yuan","Jiang Bian"],"pdf_url":"https://arxiv.org/pdf/2303.11225v2.pdf","comment":"Accepted to ICCV 2023, camera-ready version; Project page:\n  https://project-hiface.github.io/"},{"id":"http://arxiv.org/abs/2308.11974v1","updated":"2023-08-23T07:46:44Z","published":"2023-08-23T07:46:44Z","title":"Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields","summary":"  Text-driven localized editing of 3D objects is particularly difficult as\nlocally mixing the original 3D object with the intended new object and style\neffects without distorting the object's form is not a straightforward process.\nTo address this issue, we propose a novel NeRF-based model, Blending-NeRF,\nwhich consists of two NeRF networks: pretrained NeRF and editable NeRF.\nAdditionally, we introduce new blending operations that allow Blending-NeRF to\nproperly edit target regions which are localized by text. By using a pretrained\nvision-language aligned model, CLIP, we guide Blending-NeRF to add new objects\nwith varying colors and densities, modify textures, and remove parts of the\noriginal object. Our extensive experiments demonstrate that Blending-NeRF\nproduces naturally and locally edited 3D objects from various text prompts.\n","authors":["Hyeonseop Song","Seokhun Choi","Hoseok Do","Chul Lee","Taehyeong Kim"],"pdf_url":"https://arxiv.org/pdf/2308.11974v1.pdf","comment":"Accepted to ICCV 2023. The first two authors contributed equally to\n  this work"}]}}